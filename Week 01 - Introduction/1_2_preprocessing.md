<h1><center>Text preprocessing</center></h1>

## Main pipeline
* Character level:
    * Tokenization: splitting text into words
    * Splitting the text into sentences
* Word level (morphology):
    * Part-of-speech tagging
    * Resolving morphological ambiguity
    * Normalization (stemming or lemmatization)
* Sentence level (syntax):
    * Nominal and verb groups recognition
    * Semantic roles detection
    * Constituency and dependency trees
* Semantic level (semantics and discourse):
    * Coreference resolution
    * Finding synonyms and antonyms
    * Analysis of the argument-based relations

## Main problems
* Ambiguity
    * Lexical ambiguity: *–æ—Ä–≥–∞–Ω, –ø–∞—Ä–∏—Ç—å, —Ä–æ–∂–∫–∏, –∞—Ç–ª–∞—Å*
    * Morphological ambiguity: *–•—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ–Ω–µ–≥ –≤ –±–∞–Ω–∫–µ. –ß—Ç–æ –¥–µ–ª–∞—é—Ç –±–µ–ª–∫–∏ –≤ –∫–ª–µ—Ç–∫–µ?*
    * Syntactic ambiguity: *–ú—É–∂—É –∏–∑–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è. –ï–≥–æ —É–¥–∏–≤–∏–ª –ø—Ä–æ—Å—Ç–æ–π —Å–æ–ª–¥–∞—Ç. –≠—Ç–∏ —Ç–∏–ø—ã —Å—Ç–∞–ª–∏ –µ—Å—Ç—å –≤ —Ü–µ—Ö–µ.*
* Neologisms: *–ø–µ—á–µ–Ω—å–∫–∏, –∑–∞–∏–Ω—Å—Ç–∞–≥—Ä–∞–º–º–∏—Ç—å, —Ä–µ–ø–æ—Å—Ç–Ω—É—Ç—å, —Ä–∞—Å—à–∞—Ä–∏—Ç—å, –±–∏—Ç–∫–æ–∏–Ω—ã*
* Different : *–†–æ—Å—Å–∏—è, –†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è, –†–§*
* Non-standard spelling (including spelling errors and typos)

<img src="https://i.postimg.cc/pTzqjFkL/pipeline.png" alt="pipeline.png" style="width: 400px;"/>

### NLP-libraries

NLP Python libraries:
* Natural Language Toolkit (NLTK)
* Apache OpenNLP
* Stanford NLP suite
* Gate NLP library
* Spacy
* Yargy
* DeepPavlov
* CLTK (for ancient languages)
* and many others

The oldest and most famous is NLTK. NLTK does not have only various tools for text processing, but also various data ‚Äî text corpora, pre-trained sentiment models and POS tagging models, stopwords lists for different languages, etc.

* [Book on NLTK](https://www.nltk.org/book/) from the authors of the library and [tutorials](https://github.com/hb20007/hands-on-nltk-tutorial) on solving NLP task using NLTK.
* [Spacy documnetation](https://spacy.io/)
* [Yargy documentation](https://yargy.readthedocs.io/)
* [DeepPavlop documentation](http://docs.deeppavlov.ai/)

## Text preprocessing

1. **Tokenization** is the very first step in text processing.
2. **Normalization** ‚Äî mapping to the same lowercase form, removing punctuation, correcting typos, etc.
3.
    * **Stemming** ‚Äî reducing the words to their word stem or root form. The objective of stemming is to reduce related words to the same stem even if the stem is not a dictionary word. For example, connection, connected, connecting word reduce to a common word ‚Äúconnect‚Äù.
    * **Lemmatization** ‚Äî unlike stemming, lemmatization reduces words to their base (dictionary) form, reducing the inflected words properly and ensuring that the root word belongs to the language.
4. **Deleting stopwords** ‚Äî the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. The list depends on the task!

**Important!** You don't always need all the stages, it all depends on the task!

## Tokenization

#### How many words are there in the following sentence?

*–ù–∞ –¥–≤–æ—Ä–µ —Ç—Ä–∞–≤–∞, –Ω–∞ —Ç—Ä–∞–≤–µ –¥—Ä–æ–≤–∞, –Ω–µ —Ä—É–±–∏ –¥—Ä–æ–≤–∞ –Ω–∞ —Ç—Ä–∞–≤–µ –¥–≤–æ—Ä–∞.*

* 12 tokens: –ù–∞, –¥–≤–æ—Ä–µ, —Ç—Ä–∞–≤–∞, –Ω–∞, —Ç—Ä–∞–≤–µ, –¥—Ä–æ–≤–∞, –Ω–µ, —Ä—É–±–∏, –¥—Ä–æ–≤–∞, –Ω–∞, —Ç—Ä–∞–≤–µ, –¥–≤–æ—Ä–∞
* 8-9 word forms: –ù–∞/–Ω–∞, –¥–≤–æ—Ä–µ, —Ç—Ä–∞–≤–∞, —Ç—Ä–∞–≤–µ, –¥—Ä–æ–≤–∞, –Ω–µ, —Ä—É–±–∏, –¥–≤–æ—Ä–∞
* 6 lemmas: –Ω–∞, –Ω–µ, –¥–≤–æ—Ä, —Ç—Ä–∞–≤–∞, –¥—Ä–æ–≤–∞, —Ä—É–±–∏—Ç—å


### Tokens and word forms

**Word form** ‚Äì a unique word from the text

**Token** ‚Äì a word form and its position in the text

The volume of the corpus is measured in tokens, the volume of the dictionary is measured in word forms or lexemes.


### Notation
$N$ = number of tokens

$V$ = dictionary (all wordforms)

$|V|$ = number of wordforms in the dictionary

### Token ‚â† word


```python
# the most obvious tokenization approach: split text by space

text = '''
–ü—Ä–æ–¥–∞—ë—Ç—Å—è LADA 4x4. –ü–¢–° 01.12.2018, –∫—É–ø–ª–µ–Ω–∞ 20 —è–Ω–≤–∞—Ä—è 19 –≥–æ–¥–∞, 10 000 –∫–º –ø—Ä–æ–±–µ–≥–∞. 
–ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è –ø–æ–ª–Ω–∞—è. –ù–æ–≤–∞—è –≤ —Å–∞–ª–æ–Ω–µ 750 000, –æ—Ç–¥–∞–º –∑–∞ 650 000. 
–í–æ–∑–º–æ–∂–µ–Ω –æ–±–º–µ–Ω –Ω–∞ –í–ê–ó-2110 –∏–ª–∏ –í–ê–ó 2109 —Å –≤–∞—à–µ–π –¥–æ–ø–ª–∞—Ç–æ–π. 
–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä, —É–ª. –ú–∏–∫–ª—É—Ö–æ-–ú–∞–∫–ª–∞—è, –¥. 4/5, –ø–æ–¥—å–µ–∑–¥ 1 
–¢–µ–ª. 8(999)1234567, 8 903 987-65-43, +7 (351) 111 22 33 
–ò.–ò. –ò–≤–∞–Ω–æ–≤ (–ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á) 
'''

tokens = text.split()
print(tokens)
len(tokens)
```

    ['–ü—Ä–æ–¥–∞—ë—Ç—Å—è', 'LADA', '4x4.', '–ü–¢–°', '01.12.2018,', '–∫—É–ø–ª–µ–Ω–∞', '20', '—è–Ω–≤–∞—Ä—è', '19', '–≥–æ–¥–∞,', '10', '000', '–∫–º', '–ø—Ä–æ–±–µ–≥–∞.', '–ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è', '–ø–æ–ª–Ω–∞—è.', '–ù–æ–≤–∞—è', '–≤', '—Å–∞–ª–æ–Ω–µ', '750', '000,', '–æ—Ç–¥–∞–º', '–∑–∞', '650', '000.', '–í–æ–∑–º–æ–∂–µ–Ω', '–æ–±–º–µ–Ω', '–Ω–∞', '–í–ê–ó-2110', '–∏–ª–∏', '–í–ê–ó', '2109', '—Å', '–≤–∞—à–µ–π', '–¥–æ–ø–ª–∞—Ç–æ–π.', '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä,', '—É–ª.', '–ú–∏–∫–ª—É—Ö–æ-–ú–∞–∫–ª–∞—è,', '–¥.', '4/5,', '–ø–æ–¥—å–µ–∑–¥', '1', '–¢–µ–ª.', '8(999)1234567,', '8', '903', '987-65-43,', '+7', '(351)', '111', '22', '33', '–ò.–ò.', '–ò–≤–∞–Ω–æ–≤', '(–ò–≤–∞–Ω', '–ò–≤–∞–Ω–æ–≤–∏—á)']





    56




```python
text_en = '''
328i trim. FUEL EFFICIENT 28 MPG Hwy/18 MPG City!, 
PRICED TO MOVE $600 below Kelley Blue Book! Moonroof, Leather, Dual Zone A/C, CD Player, Rear Air, 
6-SPEED STEPTRONIC AUTOMATIC TRANSMIS... PREMIUM PKG, VALUE PKG. 
======KEY FEATURES INCLUDE: Rear Air, CD Player, Dual Zone A/C. BMW 328i with Crimson Red exterior and Oyster/Black Dakota Leather interior features a Straight 6 Cylinder Engine with 230 HP at 6500 RPM*. 
======OPTION PACKAGES: PREMIUM PKG Dakota leather seat trim, universal garage door opener, auto-dimming pwr folding exterior mirrors w/2-position memory, auto-dimming rearview mirror w/compass, pwr front seats w/4-way pwr lumbar, 2-position driver seat memory, BMW Assist, Bluetooth interface, pwr tilt/slide glass moonroof, VALUE PKG iPod & USB adapter, Dakota leather seat trim, 17' x 8.0 V-spoke alloy wheels (style 285), P225/45R17 run-flat performance tires, 6-SPEED STEPTRONIC AUTOMATIC TRANSMISSION normal, sport & manual shift modes. Remote Trunk Release, Keyless Entry, Steering Wheel Controls, Child Safety Locks, Electronic Stability Control. 
======AFFORDABILITY: This 328i is priced $600 below Kelley Blue Book. 
======OUR OFFERINGS: Here at DCH Subaru of Riverside, everything we do revolves around you. 
Our various teams are trained to address your needs from the moment you walk through the door, whether you're in the market for your next vehicle or tuning up your current one. Our Riverside showroom is the place to be if you're in the market for a new Subaru or quality pre-owned vehicle from today's top automakers. 
No matter what you are in search of, we deliver customer happiness! Pricing analysis performed on 8/22/2021. 
Horsepower calculations based on trim engine configuration. 
Fuel economy calculations based on original manufacturer data for trim engine configuration. 
Please confirm the accuracy of the included equipment by calling us prior to purchase.

DCH Subaru of Riverside
| 862 Lifetime Reviews
8043 Indiana Avenue

Riverside, California 92504

(951) 428-2314
'''

tokens = text_en.split()
print(tokens)
len(tokens)
```

    ['328i', 'trim.', 'FUEL', 'EFFICIENT', '28', 'MPG', 'Hwy/18', 'MPG', 'City!,', 'PRICED', 'TO', 'MOVE', '$600', 'below', 'Kelley', 'Blue', 'Book!', 'Moonroof,', 'Leather,', 'Dual', 'Zone', 'A/C,', 'CD', 'Player,', 'Rear', 'Air,', '6-SPEED', 'STEPTRONIC', 'AUTOMATIC', 'TRANSMIS...', 'PREMIUM', 'PKG,', 'VALUE', 'PKG.', '======KEY', 'FEATURES', 'INCLUDE:', 'Rear', 'Air,', 'CD', 'Player,', 'Dual', 'Zone', 'A/C.', 'BMW', '328i', 'with', 'Crimson', 'Red', 'exterior', 'and', 'Oyster/Black', 'Dakota', 'Leather', 'interior', 'features', 'a', 'Straight', '6', 'Cylinder', 'Engine', 'with', '230', 'HP', 'at', '6500', 'RPM*.', '======OPTION', 'PACKAGES:', 'PREMIUM', 'PKG', 'Dakota', 'leather', 'seat', 'trim,', 'universal', 'garage', 'door', 'opener,', 'auto-dimming', 'pwr', 'folding', 'exterior', 'mirrors', 'w/2-position', 'memory,', 'auto-dimming', 'rearview', 'mirror', 'w/compass,', 'pwr', 'front', 'seats', 'w/4-way', 'pwr', 'lumbar,', '2-position', 'driver', 'seat', 'memory,', 'BMW', 'Assist,', 'Bluetooth', 'interface,', 'pwr', 'tilt/slide', 'glass', 'moonroof,', 'VALUE', 'PKG', 'iPod', '&', 'USB', 'adapter,', 'Dakota', 'leather', 'seat', 'trim,', "17'", 'x', '8.0', 'V-spoke', 'alloy', 'wheels', '(style', '285),', 'P225/45R17', 'run-flat', 'performance', 'tires,', '6-SPEED', 'STEPTRONIC', 'AUTOMATIC', 'TRANSMISSION', 'normal,', 'sport', '&', 'manual', 'shift', 'modes.', 'Remote', 'Trunk', 'Release,', 'Keyless', 'Entry,', 'Steering', 'Wheel', 'Controls,', 'Child', 'Safety', 'Locks,', 'Electronic', 'Stability', 'Control.', '======AFFORDABILITY:', 'This', '328i', 'is', 'priced', '$600', 'below', 'Kelley', 'Blue', 'Book.', '======OUR', 'OFFERINGS:', 'Here', 'at', 'DCH', 'Subaru', 'of', 'Riverside,', 'everything', 'we', 'do', 'revolves', 'around', 'you.', 'Our', 'various', 'teams', 'are', 'trained', 'to', 'address', 'your', 'needs', 'from', 'the', 'moment', 'you', 'walk', 'through', 'the', 'door,', 'whether', "you're", 'in', 'the', 'market', 'for', 'your', 'next', 'vehicle', 'or', 'tuning', 'up', 'your', 'current', 'one.', 'Our', 'Riverside', 'showroom', 'is', 'the', 'place', 'to', 'be', 'if', "you're", 'in', 'the', 'market', 'for', 'a', 'new', 'Subaru', 'or', 'quality', 'pre-owned', 'vehicle', 'from', "today's", 'top', 'automakers.', 'No', 'matter', 'what', 'you', 'are', 'in', 'search', 'of,', 'we', 'deliver', 'customer', 'happiness!', 'Pricing', 'analysis', 'performed', 'on', '8/22/2021.', 'Horsepower', 'calculations', 'based', 'on', 'trim', 'engine', 'configuration.', 'Fuel', 'economy', 'calculations', 'based', 'on', 'original', 'manufacturer', 'data', 'for', 'trim', 'engine', 'configuration.', 'Please', 'confirm', 'the', 'accuracy', 'of', 'the', 'included', 'equipment', 'by', 'calling', 'us', 'prior', 'to', 'purchase.', 'DCH', 'Subaru', 'of', 'Riverside', '|', '862', 'Lifetime', 'Reviews', '8043', 'Indiana', 'Avenue', 'Riverside,', 'California', '92504', '(951)', '428-2314']





    301




```python
!pip install yargy
```

    Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
    Collecting yargy
      Downloading yargy-0.15.0-py3-none-any.whl (41 kB)
    [K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41 kB 109 kB/s 
    [?25hCollecting pymorphy2
      Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)
    [K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55 kB 3.5 MB/s 
    [?25hCollecting dawg-python>=0.7.1
      Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)
    Collecting docopt>=0.6
      Downloading docopt-0.6.2.tar.gz (25 kB)
    Collecting pymorphy2-dicts-ru<3.0,>=2.4
      Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)
    [K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.2 MB 42.2 MB/s 
    [?25hBuilding wheels for collected packages: docopt
      Building wheel for docopt (setup.py) ... [?25l[?25hdone
      Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=c9bf68fad8be34eea11468a596666f075ca4cf9f4a94bb8ea986fd27d895bdad
      Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9
    Successfully built docopt
    Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, yargy
    Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 yargy-0.15.0



```python
# the most detailed tokenization
from yargy.tokenizer import MorphTokenizer

tknzr = MorphTokenizer()
tokens = [_.value for _ in tknzr(text)]
print(tokens)
len(tokens)
```

    ['\n', '–ü—Ä–æ–¥–∞—ë—Ç—Å—è', 'LADA', '4', 'x', '4', '.', '–ü–¢–°', '01', '.', '12', '.', '2018', ',', '–∫—É–ø–ª–µ–Ω–∞', '20', '—è–Ω–≤–∞—Ä—è', '19', '–≥–æ–¥–∞', ',', '10', '000', '–∫–º', '–ø—Ä–æ–±–µ–≥–∞', '.', '\n', '–ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è', '–ø–æ–ª–Ω–∞—è', '.', '–ù–æ–≤–∞—è', '–≤', '—Å–∞–ª–æ–Ω–µ', '750', '000', ',', '–æ—Ç–¥–∞–º', '–∑–∞', '650', '000', '.', '\n', '–í–æ–∑–º–æ–∂–µ–Ω', '–æ–±–º–µ–Ω', '–Ω–∞', '–í–ê–ó', '-', '2110', '–∏–ª–∏', '–í–ê–ó', '2109', '—Å', '–≤–∞—à–µ–π', '–¥–æ–ø–ª–∞—Ç–æ–π', '.', '\n', '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä', ',', '—É–ª', '.', '–ú–∏–∫–ª—É—Ö–æ', '-', '–ú–∞–∫–ª–∞—è', ',', '–¥', '.', '4', '/', '5', ',', '–ø–æ–¥—å–µ–∑–¥', '1', '\n', '–¢–µ–ª', '.', '8', '(', '999', ')', '1234567', ',', '8', '903', '987', '-', '65', '-', '43', ',', '+', '7', '(', '351', ')', '111', '22', '33', '\n', '–ò', '.', '–ò', '.', '–ò–≤–∞–Ω–æ–≤', '(', '–ò–≤–∞–Ω', '–ò–≤–∞–Ω–æ–≤–∏—á', ')', '\n']





    107




```python
!pip install --user spacy
```

    Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
    Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)
    Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)
    Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)
    Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)
    Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)
    Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)
    Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)
    Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)
    Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)
    Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)
    Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)
    Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)
    Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)
    Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)
    Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)
    Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)
    Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)
    Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)
    Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)
    Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)
    Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)
    Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)
    Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)
    Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)
    Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)
    Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)



```python
import spacy
nlp = spacy.load("en_core_web_sm")
```


```python
doc = nlp(text_en)
tokens = [token.text for token in doc]
print(tokens)
len(tokens)
```

    ['\n', '328i', 'trim', '.', 'FUEL', 'EFFICIENT', '28', 'MPG', 'Hwy/18', 'MPG', 'City', '!', ',', '\n', 'PRICED', 'TO', 'MOVE', '$', '600', 'below', 'Kelley', 'Blue', 'Book', '!', 'Moonroof', ',', 'Leather', ',', 'Dual', 'Zone', 'A', '/', 'C', ',', 'CD', 'Player', ',', 'Rear', 'Air', ',', '\n', '6', '-', 'SPEED', 'STEPTRONIC', 'AUTOMATIC', 'TRANSMIS', '...', 'PREMIUM', 'PKG', ',', 'VALUE', 'PKG', '.', '\n', '=', '=', '=', '=', '=', '=', 'KEY', 'FEATURES', 'INCLUDE', ':', 'Rear', 'Air', ',', 'CD', 'Player', ',', 'Dual', 'Zone', 'A', '/', 'C.', 'BMW', '328i', 'with', 'Crimson', 'Red', 'exterior', 'and', 'Oyster', '/', 'Black', 'Dakota', 'Leather', 'interior', 'features', 'a', 'Straight', '6', 'Cylinder', 'Engine', 'with', '230', 'HP', 'at', '6500', 'RPM', '*', '.', '\n', '=', '=', '=', '=', '=', '=', 'OPTION', 'PACKAGES', ':', 'PREMIUM', 'PKG', 'Dakota', 'leather', 'seat', 'trim', ',', 'universal', 'garage', 'door', 'opener', ',', 'auto', '-', 'dimming', 'pwr', 'folding', 'exterior', 'mirrors', 'w/2', '-', 'position', 'memory', ',', 'auto', '-', 'dimming', 'rearview', 'mirror', 'w', '/', 'compass', ',', 'pwr', 'front', 'seats', 'w/4', '-', 'way', 'pwr', 'lumbar', ',', '2', '-', 'position', 'driver', 'seat', 'memory', ',', 'BMW', 'Assist', ',', 'Bluetooth', 'interface', ',', 'pwr', 'tilt', '/', 'slide', 'glass', 'moonroof', ',', 'VALUE', 'PKG', 'iPod', '&', 'USB', 'adapter', ',', 'Dakota', 'leather', 'seat', 'trim', ',', '17', "'", 'x', '8.0', 'V', '-', 'spoke', 'alloy', 'wheels', '(', 'style', '285', ')', ',', 'P225/45R17', 'run', '-', 'flat', 'performance', 'tires', ',', '6', '-', 'SPEED', 'STEPTRONIC', 'AUTOMATIC', 'TRANSMISSION', 'normal', ',', 'sport', '&', 'manual', 'shift', 'modes', '.', 'Remote', 'Trunk', 'Release', ',', 'Keyless', 'Entry', ',', 'Steering', 'Wheel', 'Controls', ',', 'Child', 'Safety', 'Locks', ',', 'Electronic', 'Stability', 'Control', '.', '\n', '=', '=', '=', '=', '=', '=', 'AFFORDABILITY', ':', 'This', '328i', 'is', 'priced', '$', '600', 'below', 'Kelley', 'Blue', 'Book', '.', '\n', '=', '=', '=', '=', '=', '=', 'OUR', 'OFFERINGS', ':', 'Here', 'at', 'DCH', 'Subaru', 'of', 'Riverside', ',', 'everything', 'we', 'do', 'revolves', 'around', 'you', '.', '\n', 'Our', 'various', 'teams', 'are', 'trained', 'to', 'address', 'your', 'needs', 'from', 'the', 'moment', 'you', 'walk', 'through', 'the', 'door', ',', 'whether', 'you', "'re", 'in', 'the', 'market', 'for', 'your', 'next', 'vehicle', 'or', 'tuning', 'up', 'your', 'current', 'one', '.', 'Our', 'Riverside', 'showroom', 'is', 'the', 'place', 'to', 'be', 'if', 'you', "'re", 'in', 'the', 'market', 'for', 'a', 'new', 'Subaru', 'or', 'quality', 'pre', '-', 'owned', 'vehicle', 'from', 'today', "'s", 'top', 'automakers', '.', '\n', 'No', 'matter', 'what', 'you', 'are', 'in', 'search', 'of', ',', 'we', 'deliver', 'customer', 'happiness', '!', 'Pricing', 'analysis', 'performed', 'on', '8/22/2021', '.', '\n', 'Horsepower', 'calculations', 'based', 'on', 'trim', 'engine', 'configuration', '.', '\n', 'Fuel', 'economy', 'calculations', 'based', 'on', 'original', 'manufacturer', 'data', 'for', 'trim', 'engine', 'configuration', '.', '\n', 'Please', 'confirm', 'the', 'accuracy', 'of', 'the', 'included', 'equipment', 'by', 'calling', 'us', 'prior', 'to', 'purchase', '.', '\n\n', 'DCH', 'Subaru', 'of', 'Riverside', '\n', '|', '862', 'Lifetime', 'Reviews', '\n', '8043', 'Indiana', 'Avenue', '\n\n', 'Riverside', ',', 'California', '92504', '\n\n', '(', '951', ')', '428', '-', '2314', '\n']





    438




```python
!pip install --user nltk
```

    Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)
    Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)



```python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('snowball_data')
nltk.download('perluniprops')
nltk.download('universal_tagset')
nltk.download('stopwords')
nltk.download('nonbreaking_prefixes')
nltk.download('wordnet')
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!
    [nltk_data] Downloading package punkt to /root/nltk_data...
    [nltk_data]   Package punkt is already up-to-date!
    [nltk_data] Downloading package snowball_data to /root/nltk_data...
    [nltk_data]   Package snowball_data is already up-to-date!
    [nltk_data] Downloading package perluniprops to /root/nltk_data...
    [nltk_data]   Package perluniprops is already up-to-date!
    [nltk_data] Downloading package universal_tagset to /root/nltk_data...
    [nltk_data]   Package universal_tagset is already up-to-date!
    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!
    [nltk_data] Downloading package nonbreaking_prefixes to
    [nltk_data]     /root/nltk_data...
    [nltk_data]   Package nonbreaking_prefixes is already up-to-date!
    [nltk_data] Downloading package wordnet to /root/nltk_data...
    [nltk_data]   Package wordnet is already up-to-date!





    True




```python
from nltk.tokenize import word_tokenize, ToktokTokenizer

tokens = word_tokenize(text)
print(tokens)
len(tokens)
```

    ['–ü—Ä–æ–¥–∞—ë—Ç—Å—è', 'LADA', '4x4', '.', '–ü–¢–°', '01.12.2018', ',', '–∫—É–ø–ª–µ–Ω–∞', '20', '—è–Ω–≤–∞—Ä—è', '19', '–≥–æ–¥–∞', ',', '10', '000', '–∫–º', '–ø—Ä–æ–±–µ–≥–∞', '.', '–ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è', '–ø–æ–ª–Ω–∞—è', '.', '–ù–æ–≤–∞—è', '–≤', '—Å–∞–ª–æ–Ω–µ', '750', '000', ',', '–æ—Ç–¥–∞–º', '–∑–∞', '650', '000', '.', '–í–æ–∑–º–æ–∂–µ–Ω', '–æ–±–º–µ–Ω', '–Ω–∞', '–í–ê–ó-2110', '–∏–ª–∏', '–í–ê–ó', '2109', '—Å', '–≤–∞—à–µ–π', '–¥–æ–ø–ª–∞—Ç–æ–π', '.', '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä', ',', '—É–ª', '.', '–ú–∏–∫–ª—É—Ö–æ-–ú–∞–∫–ª–∞—è', ',', '–¥', '.', '4/5', ',', '–ø–æ–¥—å–µ–∑–¥', '1', '–¢–µ–ª', '.', '8', '(', '999', ')', '1234567', ',', '8', '903', '987-65-43', ',', '+7', '(', '351', ')', '111', '22', '33', '–ò.–ò', '.', '–ò–≤–∞–Ω–æ–≤', '(', '–ò–≤–∞–Ω', '–ò–≤–∞–Ω–æ–≤–∏—á', ')']





    81




```python
tokens = word_tokenize(text_en)
print(tokens)
len(tokens)
```

    ['328i', 'trim', '.', 'FUEL', 'EFFICIENT', '28', 'MPG', 'Hwy/18', 'MPG', 'City', '!', ',', 'PRICED', 'TO', 'MOVE', '$', '600', 'below', 'Kelley', 'Blue', 'Book', '!', 'Moonroof', ',', 'Leather', ',', 'Dual', 'Zone', 'A/C', ',', 'CD', 'Player', ',', 'Rear', 'Air', ',', '6-SPEED', 'STEPTRONIC', 'AUTOMATIC', 'TRANSMIS', '...', 'PREMIUM', 'PKG', ',', 'VALUE', 'PKG', '.', '======KEY', 'FEATURES', 'INCLUDE', ':', 'Rear', 'Air', ',', 'CD', 'Player', ',', 'Dual', 'Zone', 'A/C', '.', 'BMW', '328i', 'with', 'Crimson', 'Red', 'exterior', 'and', 'Oyster/Black', 'Dakota', 'Leather', 'interior', 'features', 'a', 'Straight', '6', 'Cylinder', 'Engine', 'with', '230', 'HP', 'at', '6500', 'RPM*', '.', '======OPTION', 'PACKAGES', ':', 'PREMIUM', 'PKG', 'Dakota', 'leather', 'seat', 'trim', ',', 'universal', 'garage', 'door', 'opener', ',', 'auto-dimming', 'pwr', 'folding', 'exterior', 'mirrors', 'w/2-position', 'memory', ',', 'auto-dimming', 'rearview', 'mirror', 'w/compass', ',', 'pwr', 'front', 'seats', 'w/4-way', 'pwr', 'lumbar', ',', '2-position', 'driver', 'seat', 'memory', ',', 'BMW', 'Assist', ',', 'Bluetooth', 'interface', ',', 'pwr', 'tilt/slide', 'glass', 'moonroof', ',', 'VALUE', 'PKG', 'iPod', '&', 'USB', 'adapter', ',', 'Dakota', 'leather', 'seat', 'trim', ',', '17', "'", 'x', '8.0', 'V-spoke', 'alloy', 'wheels', '(', 'style', '285', ')', ',', 'P225/45R17', 'run-flat', 'performance', 'tires', ',', '6-SPEED', 'STEPTRONIC', 'AUTOMATIC', 'TRANSMISSION', 'normal', ',', 'sport', '&', 'manual', 'shift', 'modes', '.', 'Remote', 'Trunk', 'Release', ',', 'Keyless', 'Entry', ',', 'Steering', 'Wheel', 'Controls', ',', 'Child', 'Safety', 'Locks', ',', 'Electronic', 'Stability', 'Control', '.', '======AFFORDABILITY', ':', 'This', '328i', 'is', 'priced', '$', '600', 'below', 'Kelley', 'Blue', 'Book', '.', '======OUR', 'OFFERINGS', ':', 'Here', 'at', 'DCH', 'Subaru', 'of', 'Riverside', ',', 'everything', 'we', 'do', 'revolves', 'around', 'you', '.', 'Our', 'various', 'teams', 'are', 'trained', 'to', 'address', 'your', 'needs', 'from', 'the', 'moment', 'you', 'walk', 'through', 'the', 'door', ',', 'whether', 'you', "'re", 'in', 'the', 'market', 'for', 'your', 'next', 'vehicle', 'or', 'tuning', 'up', 'your', 'current', 'one', '.', 'Our', 'Riverside', 'showroom', 'is', 'the', 'place', 'to', 'be', 'if', 'you', "'re", 'in', 'the', 'market', 'for', 'a', 'new', 'Subaru', 'or', 'quality', 'pre-owned', 'vehicle', 'from', 'today', "'s", 'top', 'automakers', '.', 'No', 'matter', 'what', 'you', 'are', 'in', 'search', 'of', ',', 'we', 'deliver', 'customer', 'happiness', '!', 'Pricing', 'analysis', 'performed', 'on', '8/22/2021', '.', 'Horsepower', 'calculations', 'based', 'on', 'trim', 'engine', 'configuration', '.', 'Fuel', 'economy', 'calculations', 'based', 'on', 'original', 'manufacturer', 'data', 'for', 'trim', 'engine', 'configuration', '.', 'Please', 'confirm', 'the', 'accuracy', 'of', 'the', 'included', 'equipment', 'by', 'calling', 'us', 'prior', 'to', 'purchase', '.', 'DCH', 'Subaru', 'of', 'Riverside', '|', '862', 'Lifetime', 'Reviews', '8043', 'Indiana', 'Avenue', 'Riverside', ',', 'California', '92504', '(', '951', ')', '428-2314']





    364




```python
tknzr = ToktokTokenizer()
tokens = tknzr.tokenize(text)
print(tokens)
len(tokens)
```

    ['–ü—Ä–æ–¥–∞—ë—Ç—Å—è', 'LADA', '4x4.', '–ü–¢–°', '01.12.2018', ',', '–∫—É–ø–ª–µ–Ω–∞', '20', '—è–Ω–≤–∞—Ä—è', '19', '–≥–æ–¥–∞', ',', '10', '000', '–∫–º', '–ø—Ä–æ–±–µ–≥–∞.', '–ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è', '–ø–æ–ª–Ω–∞—è.', '–ù–æ–≤–∞—è', '–≤', '—Å–∞–ª–æ–Ω–µ', '750', '000', ',', '–æ—Ç–¥–∞–º', '–∑–∞', '650', '000.', '–í–æ–∑–º–æ–∂–µ–Ω', '–æ–±–º–µ–Ω', '–Ω–∞', '–í–ê–ó-2110', '–∏–ª–∏', '–í–ê–ó', '2109', '—Å', '–≤–∞—à–µ–π', '–¥–æ–ø–ª–∞—Ç–æ–π.', '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä', ',', '—É–ª.', '–ú–∏–∫–ª—É—Ö–æ-–ú–∞–∫–ª–∞—è', ',', '–¥.', '4/5', ',', '–ø–æ–¥—å–µ–∑–¥', '1', '–¢–µ–ª.', '8(', '999', ')', '1234567', ',', '8', '903', '987-65-43', ',', '+7', '(', '351', ')', '111', '22', '33', '–ò.–ò.', '–ò–≤–∞–Ω–æ–≤', '(', '–ò–≤–∞–Ω', '–ò–≤–∞–Ω–æ–≤–∏—á', ')']





    71




```python
tknzr = ToktokTokenizer()
tokens = tknzr.tokenize(text_en)
print(tokens)
len(tokens)
```

    ['328i', 'trim.', 'FUEL', 'EFFICIENT', '28', 'MPG', 'Hwy/18', 'MPG', 'City', '!', ',', 'PRICED', 'TO', 'MOVE', '$', '600', 'below', 'Kelley', 'Blue', 'Book', '!', 'Moonroof', ',', 'Leather', ',', 'Dual', 'Zone', 'A/C', ',', 'CD', 'Player', ',', 'Rear', 'Air', ',', '6-SPEED', 'STEPTRONIC', 'AUTOMATIC', 'TRANSMIS', '...', 'PREMIUM', 'PKG', ',', 'VALUE', 'PKG.', '======KEY', 'FEATURES', 'INCLUDE', ':', 'Rear', 'Air', ',', 'CD', 'Player', ',', 'Dual', 'Zone', 'A/C.', 'BMW', '328i', 'with', 'Crimson', 'Red', 'exterior', 'and', 'Oyster/Black', 'Dakota', 'Leather', 'interior', 'features', 'a', 'Straight', '6', 'Cylinder', 'Engine', 'with', '230', 'HP', 'at', '6500', 'RPM*.', '======OPTION', 'PACKAGES', ':', 'PREMIUM', 'PKG', 'Dakota', 'leather', 'seat', 'trim', ',', 'universal', 'garage', 'door', 'opener', ',', 'auto-dimming', 'pwr', 'folding', 'exterior', 'mirrors', 'w/2-position', 'memory', ',', 'auto-dimming', 'rearview', 'mirror', 'w/compass', ',', 'pwr', 'front', 'seats', 'w/4-way', 'pwr', 'lumbar', ',', '2-position', 'driver', 'seat', 'memory', ',', 'BMW', 'Assist', ',', 'Bluetooth', 'interface', ',', 'pwr', 'tilt/slide', 'glass', 'moonroof', ',', 'VALUE', 'PKG', 'iPod', '&amp;', 'USB', 'adapter', ',', 'Dakota', 'leather', 'seat', 'trim', ',', '17', "'", 'x', '8.0', 'V-spoke', 'alloy', 'wheels', '(', 'style', '285', ')', ',', 'P225/45R17', 'run-flat', 'performance', 'tires', ',', '6-SPEED', 'STEPTRONIC', 'AUTOMATIC', 'TRANSMISSION', 'normal', ',', 'sport', '&amp;', 'manual', 'shift', 'modes.', 'Remote', 'Trunk', 'Release', ',', 'Keyless', 'Entry', ',', 'Steering', 'Wheel', 'Controls', ',', 'Child', 'Safety', 'Locks', ',', 'Electronic', 'Stability', 'Control.', '======AFFORDABILITY', ':', 'This', '328i', 'is', 'priced', '$', '600', 'below', 'Kelley', 'Blue', 'Book.', '======OUR', 'OFFERINGS', ':', 'Here', 'at', 'DCH', 'Subaru', 'of', 'Riverside', ',', 'everything', 'we', 'do', 'revolves', 'around', 'you.', 'Our', 'various', 'teams', 'are', 'trained', 'to', 'address', 'your', 'needs', 'from', 'the', 'moment', 'you', 'walk', 'through', 'the', 'door', ',', 'whether', 'you', "'", 're', 'in', 'the', 'market', 'for', 'your', 'next', 'vehicle', 'or', 'tuning', 'up', 'your', 'current', 'one.', 'Our', 'Riverside', 'showroom', 'is', 'the', 'place', 'to', 'be', 'if', 'you', "'", 're', 'in', 'the', 'market', 'for', 'a', 'new', 'Subaru', 'or', 'quality', 'pre-owned', 'vehicle', 'from', 'today', "'", 's', 'top', 'automakers.', 'No', 'matter', 'what', 'you', 'are', 'in', 'search', 'of', ',', 'we', 'deliver', 'customer', 'happiness', '!', 'Pricing', 'analysis', 'performed', 'on', '8/22/2021.', 'Horsepower', 'calculations', 'based', 'on', 'trim', 'engine', 'configuration.', 'Fuel', 'economy', 'calculations', 'based', 'on', 'original', 'manufacturer', 'data', 'for', 'trim', 'engine', 'configuration.', 'Please', 'confirm', 'the', 'accuracy', 'of', 'the', 'included', 'equipment', 'by', 'calling', 'us', 'prior', 'to', 'purchase.', 'DCH', 'Subaru', 'of', 'Riverside', '&#124;', '862', 'Lifetime', 'Reviews', '8043', 'Indiana', 'Avenue', 'Riverside', ',', 'California', '92504', '(', '951', ')', '428-2314']





    353




```python
# tokenizer that specifies on tweets
from nltk.tokenize import TweetTokenizer

tknzr = TweetTokenizer()
tweet = "@remy This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--"
tknzr.tokenize(tweet)
```




    ['@remy',
     'This',
     'is',
     'a',
     'cooool',
     '#dummysmiley',
     ':',
     ':-)',
     ':-P',
     '<3',
     'and',
     'some',
     'arrows',
     '<',
     '>',
     '->',
     '<--']




```python
word_tokenize(tweet)
```




    ['@',
     'remy',
     'This',
     'is',
     'a',
     'cooool',
     '#',
     'dummysmiley',
     ':',
     ':',
     '-',
     ')',
     ':',
     '-P',
     '<',
     '3',
     'and',
     'some',
     'arrows',
     '<',
     '>',
     '-',
     '>',
     '<',
     '--']




```python
# tokenizer based on regexp
from nltk.tokenize import RegexpTokenizer

s = "Good muffins cost $3.88 in New York.  Please buy me two of them. \n\nThanks."
tknzr = RegexpTokenizer('\w+|\$[\d\.]+|\S+')
tknzr.tokenize(s)
```




    ['Good',
     'muffins',
     'cost',
     '$3.88',
     'in',
     'New',
     'York',
     '.',
     'Please',
     'buy',
     'me',
     'two',
     'of',
     'them',
     '.',
     'Thanks',
     '.']




```python

```

[A-z–ê-—è0-9-] = \w

\S+

Actually, `nltk` possesses a wide range of tokenizers:


```python
from nltk import tokenize
dir(tokenize)[:16]
```




    ['BlanklineTokenizer',
     'LineTokenizer',
     'MWETokenizer',
     'PunktSentenceTokenizer',
     'RegexpTokenizer',
     'ReppTokenizer',
     'SExprTokenizer',
     'SpaceTokenizer',
     'StanfordSegmenter',
     'TabTokenizer',
     'TextTilingTokenizer',
     'ToktokTokenizer',
     'TreebankWordTokenizer',
     'TweetTokenizer',
     'WhitespaceTokenizer',
     'WordPunctTokenizer']



They are able to provide the beginning and the end indices of each token:


```python
wh_tok = tokenize.WhitespaceTokenizer()
list(wh_tok.span_tokenize("don't stop me"))
```




    [(0, 5), (6, 10), (11, 13)]



Some tokenizers are very specific:


```python
tokenize.TreebankWordTokenizer().tokenize("don't stop me")
```




    ['do', "n't", 'stop', 'me']



## Sentence splitting

Sentence splitting mostly relies on punctuation marks. "? ", "! "are usually unambiguous, thea problems arise with ". " Possible solution: develop a binary classifier for sentence segmentation. For each dot "." task is to fefine whether it is the end of the sentence or not.


```python
from nltk.tokenize import sent_tokenize

sents = sent_tokenize(text)
print(len(sents))
sents
```

    10





    ['\n–ü—Ä–æ–¥–∞—ë—Ç—Å—è LADA 4x4.',
     '–ü–¢–° 01.12.2018, –∫—É–ø–ª–µ–Ω–∞ 20 —è–Ω–≤–∞—Ä—è 19 –≥–æ–¥–∞, 10 000 –∫–º –ø—Ä–æ–±–µ–≥–∞.',
     '–ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è –ø–æ–ª–Ω–∞—è.',
     '–ù–æ–≤–∞—è –≤ —Å–∞–ª–æ–Ω–µ 750 000, –æ—Ç–¥–∞–º –∑–∞ 650 000.',
     '–í–æ–∑–º–æ–∂–µ–Ω –æ–±–º–µ–Ω –Ω–∞ –í–ê–ó-2110 –∏–ª–∏ –í–ê–ó 2109 —Å –≤–∞—à–µ–π –¥–æ–ø–ª–∞—Ç–æ–π.',
     '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä, —É–ª.',
     '–ú–∏–∫–ª—É—Ö–æ-–ú–∞–∫–ª–∞—è, –¥.',
     '4/5, –ø–æ–¥—å–µ–∑–¥ 1 \n–¢–µ–ª.',
     '8(999)1234567, 8 903 987-65-43, +7 (351) 111 22 33 \n–ò.–ò.',
     '–ò–≤–∞–Ω–æ–≤ (–ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á)']




```python
sents = sent_tokenize(text_en)
print(len(sents))
sents
```

    18





    ['\n328i trim.',
     'FUEL EFFICIENT 28 MPG Hwy/18 MPG City!, \nPRICED TO MOVE $600 below Kelley Blue Book!',
     'Moonroof, Leather, Dual Zone A/C, CD Player, Rear Air, \n6-SPEED STEPTRONIC AUTOMATIC TRANSMIS...',
     'PREMIUM PKG, VALUE PKG.',
     '======KEY FEATURES INCLUDE: Rear Air, CD Player, Dual Zone A/C.',
     'BMW 328i with Crimson Red exterior and Oyster/Black Dakota Leather interior features a Straight 6 Cylinder Engine with 230 HP at 6500 RPM*.',
     "======OPTION PACKAGES: PREMIUM PKG Dakota leather seat trim, universal garage door opener, auto-dimming pwr folding exterior mirrors w/2-position memory, auto-dimming rearview mirror w/compass, pwr front seats w/4-way pwr lumbar, 2-position driver seat memory, BMW Assist, Bluetooth interface, pwr tilt/slide glass moonroof, VALUE PKG iPod & USB adapter, Dakota leather seat trim, 17' x 8.0 V-spoke alloy wheels (style 285), P225/45R17 run-flat performance tires, 6-SPEED STEPTRONIC AUTOMATIC TRANSMISSION normal, sport & manual shift modes.",
     'Remote Trunk Release, Keyless Entry, Steering Wheel Controls, Child Safety Locks, Electronic Stability Control.',
     '======AFFORDABILITY: This 328i is priced $600 below Kelley Blue Book.',
     '======OUR OFFERINGS: Here at DCH Subaru of Riverside, everything we do revolves around you.',
     "Our various teams are trained to address your needs from the moment you walk through the door, whether you're in the market for your next vehicle or tuning up your current one.",
     "Our Riverside showroom is the place to be if you're in the market for a new Subaru or quality pre-owned vehicle from today's top automakers.",
     'No matter what you are in search of, we deliver customer happiness!',
     'Pricing analysis performed on 8/22/2021.',
     'Horsepower calculations based on trim engine configuration.',
     'Fuel economy calculations based on original manufacturer data for trim engine configuration.',
     'Please confirm the accuracy of the included equipment by calling us prior to purchase.',
     'DCH Subaru of Riverside\n| 862 Lifetime Reviews\n8043 Indiana Avenue\n\nRiverside, California 92504\n\n(951) 428-2314']




```python
!pip install rusenttokenize
```

    Requirement already satisfied: rusenttokenize in /root/.local/lib/python3.7/site-packages (0.0.5)



```python
from ru_sent_tokenize import ru_sent_tokenize
sents = ru_sent_tokenize(text)

print(len(sents))
sents
```


```python

```

## Normalization

### Delete punctuation


```python
text
```




    '\n–ü—Ä–æ–¥–∞—ë—Ç—Å—è LADA 4x4. –ü–¢–° 01.12.2018, –∫—É–ø–ª–µ–Ω–∞ 20 —è–Ω–≤–∞—Ä—è 19 –≥–æ–¥–∞, 10 000 –∫–º –ø—Ä–æ–±–µ–≥–∞. \n–ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è –ø–æ–ª–Ω–∞—è. –ù–æ–≤–∞—è –≤ —Å–∞–ª–æ–Ω–µ 750 000, –æ—Ç–¥–∞–º –∑–∞ 650 000. \n–í–æ–∑–º–æ–∂–µ–Ω –æ–±–º–µ–Ω –Ω–∞ –í–ê–ó-2110 –∏–ª–∏ –í–ê–ó 2109 —Å –≤–∞—à–µ–π –¥–æ–ø–ª–∞—Ç–æ–π. \n–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä, —É–ª. –ú–∏–∫–ª—É—Ö–æ-–ú–∞–∫–ª–∞—è, –¥. 4/5, –ø–æ–¥—å–µ–∑–¥ 1 \n–¢–µ–ª. 8(999)1234567, 8 903 987-65-43, +7 (351) 111 22 33 \n–ò.–ò. –ò–≤–∞–Ω–æ–≤ (–ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á) \n'




```python
# First option
import re

# set of punctuation marks which depends on the task and texts
punct = '[!"#$%&()*\+,-\./:;<=>?@\[\]^_`{|}~‚Äû‚Äú¬´¬ª‚Ä†*\‚Äî/\-‚Äò‚Äô]'
clean_text = re.sub(punct, r' ', text)
print(clean_text.split())

# Another point
clean_words = [w.strip(punct) for w in word_tokenize(text)]
print(clean_words)

clean_words == clean_text
```

    ['–ü—Ä–æ–¥–∞—ë—Ç—Å—è', 'LADA', '4x4', '–ü–¢–°', '01', '12', '2018', '–∫—É–ø–ª–µ–Ω–∞', '20', '—è–Ω–≤–∞—Ä—è', '19', '–≥–æ–¥–∞', '10', '000', '–∫–º', '–ø—Ä–æ–±–µ–≥–∞', '–ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è', '–ø–æ–ª–Ω–∞—è', '–ù–æ–≤–∞—è', '–≤', '—Å–∞–ª–æ–Ω–µ', '750', '000', '–æ—Ç–¥–∞–º', '–∑–∞', '650', '000', '–í–æ–∑–º–æ–∂–µ–Ω', '–æ–±–º–µ–Ω', '–Ω–∞', '–í–ê–ó', '2110', '–∏–ª–∏', '–í–ê–ó', '2109', '—Å', '–≤–∞—à–µ–π', '–¥–æ–ø–ª–∞—Ç–æ–π', '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä', '—É–ª', '–ú–∏–∫–ª—É—Ö–æ', '–ú–∞–∫–ª–∞—è', '–¥', '4', '5', '–ø–æ–¥—å–µ–∑–¥', '1', '–¢–µ–ª', '8', '999', '1234567', '8', '903', '987', '65', '43', '7', '351', '111', '22', '33', '–ò', '–ò', '–ò–≤–∞–Ω–æ–≤', '–ò–≤–∞–Ω', '–ò–≤–∞–Ω–æ–≤–∏—á']
    ['–ü—Ä–æ–¥–∞—ë—Ç—Å—è', 'LADA', '4x4', '', '–ü–¢–°', '01.12.2018', '', '–∫—É–ø–ª–µ–Ω–∞', '20', '—è–Ω–≤–∞—Ä—è', '19', '–≥–æ–¥–∞', '', '10', '000', '–∫–º', '–ø—Ä–æ–±–µ–≥–∞', '', '–ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è', '–ø–æ–ª–Ω–∞—è', '', '–ù–æ–≤–∞—è', '–≤', '—Å–∞–ª–æ–Ω–µ', '750', '000', '', '–æ—Ç–¥–∞–º', '–∑–∞', '650', '000', '', '–í–æ–∑–º–æ–∂–µ–Ω', '–æ–±–º–µ–Ω', '–Ω–∞', '–í–ê–ó-2110', '–∏–ª–∏', '–í–ê–ó', '2109', '—Å', '–≤–∞—à–µ–π', '–¥–æ–ø–ª–∞—Ç–æ–π', '', '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä', '', '—É–ª', '', '–ú–∏–∫–ª—É—Ö–æ-–ú–∞–∫–ª–∞—è', '', '–¥', '', '4/5', '', '–ø–æ–¥—å–µ–∑–¥', '1', '–¢–µ–ª', '', '8', '', '999', '', '1234567', '', '8', '903', '987-65-43', '', '7', '', '351', '', '111', '22', '33', '–ò.–ò', '', '–ò–≤–∞–Ω–æ–≤', '', '–ò–≤–∞–Ω', '–ò–≤–∞–Ω–æ–≤–∏—á', '']





    False




```python
'____jlgherg.'.strip(punctuation)
```




    'jlgherg'




```python
from string import punctuation
```


```python
punctuation
```




    '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'




```python
clean_words_en = [w.strip(punctuation) for w in word_tokenize(text_en)]
print(clean_words_en)
```

    ['328i', 'trim', '', 'FUEL', 'EFFICIENT', '28', 'MPG', 'Hwy/18', 'MPG', 'City', '', '', 'PRICED', 'TO', 'MOVE', '', '600', 'below', 'Kelley', 'Blue', 'Book', '', 'Moonroof', '', 'Leather', '', 'Dual', 'Zone', 'A/C', '', 'CD', 'Player', '', 'Rear', 'Air', '', '6-SPEED', 'STEPTRONIC', 'AUTOMATIC', 'TRANSMIS', '', 'PREMIUM', 'PKG', '', 'VALUE', 'PKG', '', 'KEY', 'FEATURES', 'INCLUDE', '', 'Rear', 'Air', '', 'CD', 'Player', '', 'Dual', 'Zone', 'A/C', '', 'BMW', '328i', 'with', 'Crimson', 'Red', 'exterior', 'and', 'Oyster/Black', 'Dakota', 'Leather', 'interior', 'features', 'a', 'Straight', '6', 'Cylinder', 'Engine', 'with', '230', 'HP', 'at', '6500', 'RPM', '', 'OPTION', 'PACKAGES', '', 'PREMIUM', 'PKG', 'Dakota', 'leather', 'seat', 'trim', '', 'universal', 'garage', 'door', 'opener', '', 'auto-dimming', 'pwr', 'folding', 'exterior', 'mirrors', 'w/2-position', 'memory', '', 'auto-dimming', 'rearview', 'mirror', 'w/compass', '', 'pwr', 'front', 'seats', 'w/4-way', 'pwr', 'lumbar', '', '2-position', 'driver', 'seat', 'memory', '', 'BMW', 'Assist', '', 'Bluetooth', 'interface', '', 'pwr', 'tilt/slide', 'glass', 'moonroof', '', 'VALUE', 'PKG', 'iPod', '', 'USB', 'adapter', '', 'Dakota', 'leather', 'seat', 'trim', '', '17', '', 'x', '8.0', 'V-spoke', 'alloy', 'wheels', '', 'style', '285', '', '', 'P225/45R17', 'run-flat', 'performance', 'tires', '', '6-SPEED', 'STEPTRONIC', 'AUTOMATIC', 'TRANSMISSION', 'normal', '', 'sport', '', 'manual', 'shift', 'modes', '', 'Remote', 'Trunk', 'Release', '', 'Keyless', 'Entry', '', 'Steering', 'Wheel', 'Controls', '', 'Child', 'Safety', 'Locks', '', 'Electronic', 'Stability', 'Control', '', 'AFFORDABILITY', '', 'This', '328i', 'is', 'priced', '', '600', 'below', 'Kelley', 'Blue', 'Book', '', 'OUR', 'OFFERINGS', '', 'Here', 'at', 'DCH', 'Subaru', 'of', 'Riverside', '', 'everything', 'we', 'do', 'revolves', 'around', 'you', '', 'Our', 'various', 'teams', 'are', 'trained', 'to', 'address', 'your', 'needs', 'from', 'the', 'moment', 'you', 'walk', 'through', 'the', 'door', '', 'whether', 'you', 're', 'in', 'the', 'market', 'for', 'your', 'next', 'vehicle', 'or', 'tuning', 'up', 'your', 'current', 'one', '', 'Our', 'Riverside', 'showroom', 'is', 'the', 'place', 'to', 'be', 'if', 'you', 're', 'in', 'the', 'market', 'for', 'a', 'new', 'Subaru', 'or', 'quality', 'pre-owned', 'vehicle', 'from', 'today', 's', 'top', 'automakers', '', 'No', 'matter', 'what', 'you', 'are', 'in', 'search', 'of', '', 'we', 'deliver', 'customer', 'happiness', '', 'Pricing', 'analysis', 'performed', 'on', '8/22/2021', '', 'Horsepower', 'calculations', 'based', 'on', 'trim', 'engine', 'configuration', '', 'Fuel', 'economy', 'calculations', 'based', 'on', 'original', 'manufacturer', 'data', 'for', 'trim', 'engine', 'configuration', '', 'Please', 'confirm', 'the', 'accuracy', 'of', 'the', 'included', 'equipment', 'by', 'calling', 'us', 'prior', 'to', 'purchase', '', 'DCH', 'Subaru', 'of', 'Riverside', '', '862', 'Lifetime', 'Reviews', '8043', 'Indiana', 'Avenue', 'Riverside', '', 'California', '92504', '', '951', '', '428-2314']


### Convert to lower/upper case


```python
clean_words = [w.lower() for w in clean_words if w != '']
print(clean_words)
```

    ['–ø—Ä–æ–¥–∞—ë—Ç—Å—è', 'lada', '4x4', '–ø—Ç—Å', '01.12.2018', '–∫—É–ø–ª–µ–Ω–∞', '20', '—è–Ω–≤–∞—Ä—è', '19', '–≥–æ–¥–∞', '10', '000', '–∫–º', '–ø—Ä–æ–±–µ–≥–∞', '–∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è', '–ø–æ–ª–Ω–∞—è', '–Ω–æ–≤–∞—è', '–≤', '—Å–∞–ª–æ–Ω–µ', '750', '000', '–æ—Ç–¥–∞–º', '–∑–∞', '650', '000', '–≤–æ–∑–º–æ–∂–µ–Ω', '–æ–±–º–µ–Ω', '–Ω–∞', '–≤–∞–∑-2110', '–∏–ª–∏', '–≤–∞–∑', '2109', '—Å', '–≤–∞—à–µ–π', '–¥–æ–ø–ª–∞—Ç–æ–π', '–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä', '—É–ª', '–º–∏–∫–ª—É—Ö–æ-–º–∞–∫–ª–∞—è', '–¥', '4/5', '–ø–æ–¥—å–µ–∑–¥', '1', '—Ç–µ–ª', '8', '999', '1234567', '8', '903', '987-65-43', '7', '351', '111', '22', '33', '–∏.–∏', '–∏–≤–∞–Ω–æ–≤', '–∏–≤–∞–Ω', '–∏–≤–∞–Ω–æ–≤–∏—á']



```python
clean_words_en = [w.lower() for w in clean_words_en if w != '']
print(clean_words_en)
```

    ['328i', 'trim', 'fuel', 'efficient', '28', 'mpg', 'hwy/18', 'mpg', 'city', 'priced', 'to', 'move', '600', 'below', 'kelley', 'blue', 'book', 'moonroof', 'leather', 'dual', 'zone', 'a/c', 'cd', 'player', 'rear', 'air', '6-speed', 'steptronic', 'automatic', 'transmis', 'premium', 'pkg', 'value', 'pkg', 'key', 'features', 'include', 'rear', 'air', 'cd', 'player', 'dual', 'zone', 'a/c', 'bmw', '328i', 'with', 'crimson', 'red', 'exterior', 'and', 'oyster/black', 'dakota', 'leather', 'interior', 'features', 'a', 'straight', '6', 'cylinder', 'engine', 'with', '230', 'hp', 'at', '6500', 'rpm', 'option', 'packages', 'premium', 'pkg', 'dakota', 'leather', 'seat', 'trim', 'universal', 'garage', 'door', 'opener', 'auto-dimming', 'pwr', 'folding', 'exterior', 'mirrors', 'w/2-position', 'memory', 'auto-dimming', 'rearview', 'mirror', 'w/compass', 'pwr', 'front', 'seats', 'w/4-way', 'pwr', 'lumbar', '2-position', 'driver', 'seat', 'memory', 'bmw', 'assist', 'bluetooth', 'interface', 'pwr', 'tilt/slide', 'glass', 'moonroof', 'value', 'pkg', 'ipod', 'usb', 'adapter', 'dakota', 'leather', 'seat', 'trim', '17', 'x', '8.0', 'v-spoke', 'alloy', 'wheels', 'style', '285', 'p225/45r17', 'run-flat', 'performance', 'tires', '6-speed', 'steptronic', 'automatic', 'transmission', 'normal', 'sport', 'manual', 'shift', 'modes', 'remote', 'trunk', 'release', 'keyless', 'entry', 'steering', 'wheel', 'controls', 'child', 'safety', 'locks', 'electronic', 'stability', 'control', 'affordability', 'this', '328i', 'is', 'priced', '600', 'below', 'kelley', 'blue', 'book', 'our', 'offerings', 'here', 'at', 'dch', 'subaru', 'of', 'riverside', 'everything', 'we', 'do', 'revolves', 'around', 'you', 'our', 'various', 'teams', 'are', 'trained', 'to', 'address', 'your', 'needs', 'from', 'the', 'moment', 'you', 'walk', 'through', 'the', 'door', 'whether', 'you', 're', 'in', 'the', 'market', 'for', 'your', 'next', 'vehicle', 'or', 'tuning', 'up', 'your', 'current', 'one', 'our', 'riverside', 'showroom', 'is', 'the', 'place', 'to', 'be', 'if', 'you', 're', 'in', 'the', 'market', 'for', 'a', 'new', 'subaru', 'or', 'quality', 'pre-owned', 'vehicle', 'from', 'today', 's', 'top', 'automakers', 'no', 'matter', 'what', 'you', 'are', 'in', 'search', 'of', 'we', 'deliver', 'customer', 'happiness', 'pricing', 'analysis', 'performed', 'on', '8/22/2021', 'horsepower', 'calculations', 'based', 'on', 'trim', 'engine', 'configuration', 'fuel', 'economy', 'calculations', 'based', 'on', 'original', 'manufacturer', 'data', 'for', 'trim', 'engine', 'configuration', 'please', 'confirm', 'the', 'accuracy', 'of', 'the', 'included', 'equipment', 'by', 'calling', 'us', 'prior', 'to', 'purchase', 'dch', 'subaru', 'of', 'riverside', '862', 'lifetime', 'reviews', '8043', 'indiana', 'avenue', 'riverside', 'california', '92504', '951', '428-2314']



```python
# how to convert to upper case?
# your code here for text_en
```

### Stopwords

**Stop words** ‚Äî are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document. They make up the top of the frequency list in any language. The set of stop words is not universal, it will depend on your task!

NLTK has ready-made stop word lists for many languages.


```python
from nltk.corpus import stopwords

# language list
stopwords.fileids()
```




    ['arabic',
     'azerbaijani',
     'bengali',
     'danish',
     'dutch',
     'english',
     'finnish',
     'french',
     'german',
     'greek',
     'hungarian',
     'indonesian',
     'italian',
     'kazakh',
     'nepali',
     'norwegian',
     'portuguese',
     'romanian',
     'russian',
     'slovene',
     'spanish',
     'swedish',
     'tajik',
     'turkish']




```python
sw = stopwords.words('russian')
print(sw)
```

    ['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É']



```python
print([w if w not in sw else print(w) for w in clean_words])
```

    –≤
    –∑–∞
    –Ω–∞
    –∏–ª–∏
    —Å
    ['–ø—Ä–æ–¥–∞—ë—Ç—Å—è', 'lada', '4x4', '–ø—Ç—Å', '01.12.2018', '–∫—É–ø–ª–µ–Ω–∞', '20', '—è–Ω–≤–∞—Ä—è', '19', '–≥–æ–¥–∞', '10', '000', '–∫–º', '–ø—Ä–æ–±–µ–≥–∞', '–∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è', '–ø–æ–ª–Ω–∞—è', '–Ω–æ–≤–∞—è', None, '—Å–∞–ª–æ–Ω–µ', '750', '000', '–æ—Ç–¥–∞–º', None, '650', '000', '–≤–æ–∑–º–æ–∂–µ–Ω', '–æ–±–º–µ–Ω', None, '–≤–∞–∑-2110', None, '–≤–∞–∑', '2109', None, '–≤–∞—à–µ–π', '–¥–æ–ø–ª–∞—Ç–æ–π', '–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä', '—É–ª', '–º–∏–∫–ª—É—Ö–æ-–º–∞–∫–ª–∞—è', '–¥', '4/5', '–ø–æ–¥—å–µ–∑–¥', '1', '—Ç–µ–ª', '8', '999', '1234567', '8', '903', '987-65-43', '7', '351', '111', '22', '33', '–∏.–∏', '–∏–≤–∞–Ω–æ–≤', '–∏–≤–∞–Ω', '–∏–≤–∞–Ω–æ–≤–∏—á']



```python
sw = stopwords.words('english')
print(sw)
```

    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]



```python
nltk_clean_words_en = [w for w in clean_words_en if w not in sw]
print(nltk_clean_words_en)
```

    ['328i', 'trim', 'fuel', 'efficient', '28', 'mpg', 'hwy/18', 'mpg', 'city', 'priced', 'move', '600', 'kelley', 'blue', 'book', 'moonroof', 'leather', 'dual', 'zone', 'a/c', 'cd', 'player', 'rear', 'air', '6-speed', 'steptronic', 'automatic', 'transmis', 'premium', 'pkg', 'value', 'pkg', 'key', 'features', 'include', 'rear', 'air', 'cd', 'player', 'dual', 'zone', 'a/c', 'bmw', '328i', 'crimson', 'red', 'exterior', 'oyster/black', 'dakota', 'leather', 'interior', 'features', 'straight', '6', 'cylinder', 'engine', '230', 'hp', '6500', 'rpm', 'option', 'packages', 'premium', 'pkg', 'dakota', 'leather', 'seat', 'trim', 'universal', 'garage', 'door', 'opener', 'auto-dimming', 'pwr', 'folding', 'exterior', 'mirrors', 'w/2-position', 'memory', 'auto-dimming', 'rearview', 'mirror', 'w/compass', 'pwr', 'front', 'seats', 'w/4-way', 'pwr', 'lumbar', '2-position', 'driver', 'seat', 'memory', 'bmw', 'assist', 'bluetooth', 'interface', 'pwr', 'tilt/slide', 'glass', 'moonroof', 'value', 'pkg', 'ipod', 'usb', 'adapter', 'dakota', 'leather', 'seat', 'trim', '17', 'x', '8.0', 'v-spoke', 'alloy', 'wheels', 'style', '285', 'p225/45r17', 'run-flat', 'performance', 'tires', '6-speed', 'steptronic', 'automatic', 'transmission', 'normal', 'sport', 'manual', 'shift', 'modes', 'remote', 'trunk', 'release', 'keyless', 'entry', 'steering', 'wheel', 'controls', 'child', 'safety', 'locks', 'electronic', 'stability', 'control', 'affordability', '328i', 'priced', '600', 'kelley', 'blue', 'book', 'offerings', 'dch', 'subaru', 'riverside', 'everything', 'revolves', 'around', 'various', 'teams', 'trained', 'address', 'needs', 'moment', 'walk', 'door', 'whether', 'market', 'next', 'vehicle', 'tuning', 'current', 'one', 'riverside', 'showroom', 'place', 'market', 'new', 'subaru', 'quality', 'pre-owned', 'vehicle', 'today', 'top', 'automakers', 'matter', 'search', 'deliver', 'customer', 'happiness', 'pricing', 'analysis', 'performed', '8/22/2021', 'horsepower', 'calculations', 'based', 'trim', 'engine', 'configuration', 'fuel', 'economy', 'calculations', 'based', 'original', 'manufacturer', 'data', 'trim', 'engine', 'configuration', 'please', 'confirm', 'accuracy', 'included', 'equipment', 'calling', 'us', 'prior', 'purchase', 'dch', 'subaru', 'riverside', '862', 'lifetime', 'reviews', '8043', 'indiana', 'avenue', 'riverside', 'california', '92504', '951', '428-2314']



```python
clean_text_en = re.sub(punct, r'', text_en.lower())

spacy_clean_words_en = [token.text for token in nlp(clean_text_en) if not token.is_stop and token.text.isalnum()]
print(spacy_clean_words_en)
```

    ['328i', 'trim', 'fuel', 'efficient', '28', 'mpg', 'hwy18', 'mpg', 'city', 'priced', '600', 'kelley', 'blue', 'book', 'moonroof', 'leather', 'dual', 'zone', 'ac', 'cd', 'player', 'rear', 'air', '6speed', 'steptronic', 'automatic', 'transmis', 'premium', 'pkg', 'value', 'pkg', 'key', 'features', 'include', 'rear', 'air', 'cd', 'player', 'dual', 'zone', 'ac', 'bmw', '328i', 'crimson', 'red', 'exterior', 'oysterblack', 'dakota', 'leather', 'interior', 'features', 'straight', '6', 'cylinder', 'engine', '230', 'hp', '6500', 'rpm', 'option', 'packages', 'premium', 'pkg', 'dakota', 'leather', 'seat', 'trim', 'universal', 'garage', 'door', 'opener', 'autodimming', 'pwr', 'folding', 'exterior', 'mirrors', 'w2position', 'memory', 'autodimming', 'rearview', 'mirror', 'wcompass', 'pwr', 'seats', 'w4way', 'pwr', 'lumbar', '2position', 'driver', 'seat', 'memory', 'bmw', 'assist', 'bluetooth', 'interface', 'pwr', 'tiltslide', 'glass', 'moonroof', 'value', 'pkg', 'ipod', 'usb', 'adapter', 'dakota', 'leather', 'seat', 'trim', '17', 'x', '80', 'vspoke', 'alloy', 'wheels', 'style', '285', 'p22545r17', 'runflat', 'performance', 'tires', '6speed', 'steptronic', 'automatic', 'transmission', 'normal', 'sport', 'manual', 'shift', 'modes', 'remote', 'trunk', 'release', 'keyless', 'entry', 'steering', 'wheel', 'controls', 'child', 'safety', 'locks', 'electronic', 'stability', 'control', 'affordability', '328i', 'priced', '600', 'kelley', 'blue', 'book', 'offerings', 'dch', 'subaru', 'riverside', 'revolves', 'teams', 'trained', 'address', 'needs', 'moment', 'walk', 'door', 'market', 'vehicle', 'tuning', 'current', 'riverside', 'showroom', 'place', 'market', 'new', 'subaru', 'quality', 'preowned', 'vehicle', 'today', 'automakers', 'matter', 'search', 'deliver', 'customer', 'happiness', 'pricing', 'analysis', 'performed', '8222021', 'horsepower', 'calculations', 'based', 'trim', 'engine', 'configuration', 'fuel', 'economy', 'calculations', 'based', 'original', 'manufacturer', 'data', 'trim', 'engine', 'configuration', 'confirm', 'accuracy', 'included', 'equipment', 'calling', 'prior', 'purchase', 'dch', 'subaru', 'riverside', '862', 'lifetime', 'reviews', '8043', 'indiana', 'avenue', 'riverside', 'california', '92504', '951', '4282314']



```python
len(nltk_clean_words_en)==len(spacy_clean_words_en), len(nltk_clean_words_en), len(spacy_clean_words_en)
```




    (False, 234, 223)



## Stemming

**Stemming** reduces words to their stem (root) by removing endings and suffixes. The remaining part is called stem. However, it should not necessarily match with the morphological basis of the word. The problem of stemming is that the same stems may  be obtained from the words with different roots and vice versa.

* 1st type of error: –±–µ–ª—ã–π, –±–µ–ª–∫–∞, –±–µ–ª—å–µ $\implies$  –±–µ–ª

* 2nd type of error: —Ç—Ä—É–¥–Ω–æ—Å—Ç—å, —Ç—Ä—É–¥–Ω—ã–π $\implies$  —Ç—Ä—É–¥–Ω–æ—Å—Ç, —Ç—Ä—É–¥

* 3rd type of error: –±—ã—Å—Ç—Ä—ã–π, –±—ã—Å—Ç—Ä–µ–µ $\implies$  –±—ã—Å—Ç, –ø–æ–±—ã—Å—Ç—Ä–µ–µ $\implies$  –ø–æ–±—ã—Å—Ç

The simplest algorithm is the Porter algorithm. It consists of 5 loops with commands, on each loop there is an operation to delete / replace the suffix. Probabilistic extensions of the algorithm are possible.

### Snowball stemmer
An improved version of Porter's stemmer; unlike Porter's stemmer, it can work with multiple languages.


```python
from nltk.stem.snowball import SnowballStemmer

SnowballStemmer.languages  
```




    ('arabic',
     'danish',
     'dutch',
     'english',
     'finnish',
     'french',
     'german',
     'hungarian',
     'italian',
     'norwegian',
     'porter',
     'portuguese',
     'romanian',
     'russian',
     'spanish',
     'swedish')




```python
poem = '''
–ü–æ –º–æ—Ä—è–º, –∏–≥—Ä–∞—è, –Ω–æ—Å–∏—Ç—Å—è
—Å –º–∏–Ω–æ–Ω–æ—Å—Ü–µ–º –º–∏–Ω–æ–Ω–æ—Å–∏—Ü–∞.
–õ—å–Ω–µ—Ç, –∫–∞–∫ –±—É–¥—Ç–æ –∫ –º–µ–¥—É –æ—Å–æ—á–∫–∞,
–∫ –º–∏–Ω–æ–Ω–æ—Å—Ü—É –º–∏–Ω–æ–Ω–æ—Å–æ—á–∫–∞.
–ò –∫–æ–Ω—Ü–∞ –± –Ω–µ –¥–æ–≤–µ–ª–æ—Å—å –µ–º—É,
–±–ª–∞–≥–æ–¥—É—à—å—é –º–∏–Ω–æ–Ω–æ—Å—å–µ–º—É.
–í–¥—Ä—É–≥ –ø—Ä–æ–∂–µ–∫—Ç–æ—Ä, –≤–∑–¥–µ–≤ –Ω–∞ –Ω–æ—Å –æ—á–∫–∏,
–≤–ø–∏–ª—Å—è –≤ —Å–ø–∏–Ω—É –º–∏–Ω–æ–Ω–æ—Å–æ—á–∫–∏.
–ö–∞–∫ –≤–∑—Ä–µ–≤–µ—Ç –º–µ–¥–Ω–æ–≥–æ–ª–æ—Å–∏–Ω–∞:
–†-—Ä-—Ä-–∞—Å—Ç–∞–∫–∞—è –º–∏–Ω–æ–Ω–æ—Å–∏–Ω–∞!
'''

words = [w.strip(punct).lower() for w in word_tokenize(poem)]
words = [w for w in words if w not in sw and w != '']
```


```python
snowball = SnowballStemmer("russian")

for w in words:
    print("%s: %s" % (w, snowball.stem(w)))
```

    –ø–æ: –ø–æ
    –º–æ—Ä—è–º: –º–æ—Ä
    –∏–≥—Ä–∞—è: –∏–≥—Ä
    –Ω–æ—Å–∏—Ç—Å—è: –Ω–æ—Å
    —Å: —Å
    –º–∏–Ω–æ–Ω–æ—Å—Ü–µ–º: –º–∏–Ω–æ–Ω–æ—Å—Ü
    –º–∏–Ω–æ–Ω–æ—Å–∏—Ü–∞: –º–∏–Ω–æ–Ω–æ—Å–∏—Ü
    –ª—å–Ω–µ—Ç: –ª—å–Ω–µ—Ç
    –∫–∞–∫: –∫–∞–∫
    –±—É–¥—Ç–æ: –±—É–¥—Ç
    –∫: –∫
    –º–µ–¥—É: –º–µ–¥
    –æ—Å–æ—á–∫–∞: –æ—Å–æ—á–∫
    –∫: –∫
    –º–∏–Ω–æ–Ω–æ—Å—Ü—É: –º–∏–Ω–æ–Ω–æ—Å—Ü
    –º–∏–Ω–æ–Ω–æ—Å–æ—á–∫–∞: –º–∏–Ω–æ–Ω–æ—Å–æ—á–∫
    –∏: –∏
    –∫–æ–Ω—Ü–∞: –∫–æ–Ω—Ü
    –±: –±
    –Ω–µ: –Ω–µ
    –¥–æ–≤–µ–ª–æ—Å—å: –¥–æ–≤–µ–ª
    –µ–º—É: –µ–º
    –±–ª–∞–≥–æ–¥—É—à—å—é: –±–ª–∞–≥–æ–¥—É—à
    –º–∏–Ω–æ–Ω–æ—Å—å–µ–º—É: –º–∏–Ω–æ–Ω–æ—Å
    –≤–¥—Ä—É–≥: –≤–¥—Ä—É–≥
    –ø—Ä–æ–∂–µ–∫—Ç–æ—Ä: –ø—Ä–æ–∂–µ–∫—Ç–æ—Ä
    –≤–∑–¥–µ–≤: –≤–∑–¥–µ–≤
    –Ω–∞: –Ω–∞
    –Ω–æ—Å: –Ω–æ—Å
    –æ—á–∫–∏: –æ—á–∫
    –≤–ø–∏–ª—Å—è: –≤–ø–∏–ª
    –≤: –≤
    —Å–ø–∏–Ω—É: —Å–ø–∏–Ω
    –º–∏–Ω–æ–Ω–æ—Å–æ—á–∫–∏: –º–∏–Ω–æ–Ω–æ—Å–æ—á–∫
    –∫–∞–∫: –∫–∞–∫
    –≤–∑—Ä–µ–≤–µ—Ç: –≤–∑—Ä–µ–≤–µ—Ç
    –º–µ–¥–Ω–æ–≥–æ–ª–æ—Å–∏–Ω–∞: –º–µ–¥–Ω–æ–≥–æ–ª–æ—Å–∏–Ω
    —Ä-—Ä-—Ä-–∞—Å—Ç–∞–∫–∞—è: —Ä-—Ä-—Ä-–∞—Å—Ç–∞–∫
    –º–∏–Ω–æ–Ω–æ—Å–∏–Ω–∞: –º–∏–Ω–æ–Ω–æ—Å–∏–Ω



```python
snowball = SnowballStemmer("english")
```


```python
poem_en = '''
Twinkle, twinkle, little star,
How I wonder what you are.
Up above the world so high,
Like a diamond in the sky.
Twinkle, twinkle, little star,
How I wonder what you are!

When the blazing sun is gone,
When he nothing shines upon,
Then you show your little light,
Twinkle, twinkle, all the night.
Twinkle, twinkle, little star,
How I wonder what you are!
'''

words = [w.strip(punct).lower() for w in word_tokenize(poem_en)]
words = [w for w in words if w not in sw and w != '']
for w in words:
    print("%s: %s" % (w, snowball.stem(w)))
```

    twinkle: twinkl
    twinkle: twinkl
    little: littl
    star: star
    wonder: wonder
    world: world
    high: high
    like: like
    diamond: diamond
    sky: sky
    twinkle: twinkl
    twinkle: twinkl
    little: littl
    star: star
    wonder: wonder
    blazing: blaze
    sun: sun
    gone: gone
    nothing: noth
    shines: shine
    upon: upon
    show: show
    little: littl
    light: light
    twinkle: twinkl
    twinkle: twinkl
    night: night
    twinkle: twinkl
    twinkle: twinkl
    little: littl
    star: star
    wonder: wonder


## Morphological analysis

The are the following tasks of morphological analysis:

* Morphological segmentation ‚Äî converting word to its normal form (lemma), extracting the basis (stem) and grammatical characteristics of the word
* Word form generation ‚Äî word form generation according to its lemma and given grammatical characteristics

Morphological analysis is not exactly the strong suit of NLTK. You should better use `Spacy` for european languages and `pymorphy2` and `pymystem3` for Russian.

## Lemmatization

**Lemmatization** ‚Äî the process of converting a word form to a lemma, (a normal, dictionary form). This is a more complex task than stemming, but it also gives much more meaningful results, especially for languages with rich morphology.

* –∫–æ—à–∫–µ, –∫–æ—à–∫—É, –∫–æ—à–∫–∞–º, –∫–æ—à–∫–æ–π $\implies$ –∫–æ—à–∫–∞
* –±–µ–∂–∞–ª, –±–µ–∂–∏—Ç, –±–µ–≥—É $\implies$  –±–µ–∂–∞—Ç—å
* –±–µ–ª–æ–º—É, –±–µ–ª—ã–º, –±–µ–ª—ã–º–∏ $\implies$ –±–µ–ª—ã–π

## POS-tagging

**–ß–∞—Å—Ç–µ—Ä–µ—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞**, –∏–ª–∏ **POS-tagging** _(part of speech tagging)_ ‚Äî  the process of marking up a word in a text (corpus) as corresponding to a particular part of speech (tags), based on both its definition and its context.

–î–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤–æ–∑–º–æ–∂–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–±–æ—Ä–æ–≤ (—Ç.–µ. –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö –ª–µ–º–º, –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ –∏ —Ç.–ø.). –¢–µ–≥–≥–µ—Ä –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç  –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã, —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –∏—Ö –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–¥–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π. –í—ã–±–æ—Ä –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–±–æ—Ä–∞ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è **—Å–Ω—è—Ç–∏–µ–º –æ–º–æ–Ω–∏–º–∏–∏**, –∏–ª–∏ **–¥–∏–∑–∞–º–±–∏–≥—É–∞—Ü–∏–µ–π**.

Part-of-speech tagging is harder than just having a list of words and their parts of speech, because some word forms can represent more than one part of speech at different times. It means that in natural languages a large percentage of word forms are ambiguous. For example, even "dogs", which is usually thought of as just a plural noun, can also be a verb:

    He reads books \<plural noun\>
    He books \<3rd person singular verb\> tickets.

Correct grammatical tagging which chooses one of possible options is called **morphological disambiguation**.

### Tag sets

There are many sets of grammatical tags, or tagsets:
* –ù–ö–†–Ø
* Mystem
* UPenn
* OpenCorpora (is used in pymorphy2)
* Universal Dependencies
* ...

There is a [library](https://github.com/kmike/russian-tagsets) to convert tags from one system to another for the Russian language, `russian-tagsets`. However, you may loose important information when converting POS-tags from one system to another.

At the moment, the standard is **Universal Dependencies**. You can read more about the project [here](http://universaldependencies.org/), and [here](http://universaldependencies.org/u/pos/) about tags. Here below you can see the list of the main UD tags:

* ADJ: adjective
* ADP: adposition
* ADV: adverb
* AUX: auxiliary
* CCONJ: coordinating conjunction
* DET: determiner
* INTJ: interjection
* NOUN: noun
* NUM: numeral
* PART: particle
* PRON: pronoun
* PROPN: proper noun
* PUNCT: punctuation
* SCONJ: subordinating conjunction
* SYM: symbol
* VERB: verb
* X: other

### pymystem3

**pymystem3** ‚Äî a wrapper for "an excellent morphological analyzer" for Russian language Yandex Mystem 3.1 released in June 2014. You can download it separately and use it from the console. An outstanding advantage of Mystem is that the system relies on the word context which is quite helpful when resolving ambiguity. 

* [Mystem documentation](https://tech.yandex.ru/mystem/doc/index-docpage/)
* [pymystem3 documentation](http://pythonhosted.org/pymystem3/)

Initialize Mystem with the default parameters. BTW, parameters are as follows:
* mystem_bin - path to `mystem`, if if there are several of them
* grammar_info - analyze grammatical information or lemmas only (grammatical information is included by default)
* disambiguation - morphological disambiguation (included by default)
* entire_input - save all characters from the input to the output (e.g. different types of space characters, \everything is included by default)

Mystem methods accept string as input and tokenizes it inside. You may provide tokens separately, however, then the context would not be taken into consideration.


```python
! pip install --user pymystem3
```

    Requirement already satisfied: pymystem3 in /usr/local/lib/python3.7/dist-packages (0.2.0)
    Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pymystem3) (2.23.0)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (2.10)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (1.24.3)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (3.0.4)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (2021.5.30)



```python
from pymystem3 import Mystem

m = Mystem()
lemmas = m.lemmatize(' '.join(words))
print(lemmas)
```


```python
from pymystem3 import Mystem

m = Mystem()
lemmas = m.stem(' '.join(words))
print(lemmas)
```


```python
parsed = m.analyze(poem)
parsed[:10]
```




    [{'text': '\n'},
     {'analysis': [{'gr': 'PR=', 'lex': '–ø–æ'}], 'text': '–ü–æ'},
     {'text': ' '},
     {'analysis': [{'gr': 'S,—Å—Ä–µ–¥,–Ω–µ–æ–¥=–¥–∞—Ç,–º–Ω', 'lex': '–º–æ—Ä–µ'}], 'text': '–º–æ—Ä—è–º'},
     {'text': ', '},
     {'analysis': [{'gr': 'V,–Ω–µ—Å–æ–≤,–ø–µ=–Ω–µ–ø—Ä–æ—à,–¥–µ–µ–ø—Ä', 'lex': '–∏–≥—Ä–∞—Ç—å'}],
      'text': '–∏–≥—Ä–∞—è'},
     {'text': ', '},
     {'analysis': [{'gr': 'V,–Ω–µ—Å–æ–≤,–Ω–ø=–Ω–µ–ø—Ä–æ—à,–µ–¥,–∏–∑—ä—è–≤,3-–ª', 'lex': '–Ω–æ—Å–∏—Ç—å—Å—è'}],
      'text': '–Ω–æ—Å–∏—Ç—Å—è'},
     {'text': '\n'},
     {'analysis': [{'gr': 'PR=', 'lex': '—Å'}], 'text': '—Å'}]




```python
# how to get pos of the word

for word in parsed[:20]:
    if 'analysis' in word:
        gr = word['analysis'][0]['gr']
        pos = gr.split('=')[0].split(',')[0]
        print(word['text'], pos)
```

    –ü–æ PR
    –º–æ—Ä—è–º S
    –∏–≥—Ä–∞—è V
    –Ω–æ—Å–∏—Ç—Å—è V
    —Å PR
    –º–∏–Ω–æ–Ω–æ—Å—Ü–µ–º S
    –º–∏–Ω–æ–Ω–æ—Å–∏—Ü–∞ S
    –õ—å–Ω–µ—Ç V
    –∫–∞–∫ ADVPRO



```python
spacy_poem_en = nlp(poem_en)
```


```python
for token in spacy_poem_en:
    if token.pos_ != "SPACE":
        print(token.text, token.lemma_, token.pos_)
```

    Twinkle Twinkle PROPN
    , , PUNCT
    twinkle twinkle NOUN
    , , PUNCT
    little little ADJ
    star star NOUN
    , , PUNCT
    How how ADV
    I -PRON- PRON
    wonder wonder VERB
    what what PRON
    you -PRON- PRON
    are be AUX
    . . PUNCT
    Up up ADP
    above above ADP
    the the DET
    world world NOUN
    so so ADV
    high high ADV
    , , PUNCT
    Like like SCONJ
    a a DET
    diamond diamond NOUN
    in in ADP
    the the DET
    sky sky NOUN
    . . PUNCT
    Twinkle Twinkle PROPN
    , , PUNCT
    twinkle twinkle NOUN
    , , PUNCT
    little little ADJ
    star star NOUN
    , , PUNCT
    How how ADV
    I -PRON- PRON
    wonder wonder VERB
    what what PRON
    you -PRON- PRON
    are be AUX
    ! ! PUNCT
    When when ADV
    the the DET
    blazing blaze VERB
    sun sun NOUN
    is be AUX
    gone go VERB
    , , PUNCT
    When when ADV
    he -PRON- PRON
    nothing nothing PRON
    shines shine VERB
    upon upon SCONJ
    , , PUNCT
    Then then ADV
    you -PRON- PRON
    show show VERB
    your -PRON- DET
    little little ADJ
    light light NOUN
    , , PUNCT
    Twinkle Twinkle PROPN
    , , PUNCT
    twinkle twinkle NOUN
    , , PUNCT
    all all DET
    the the DET
    night night NOUN
    . . PUNCT
    Twinkle Twinkle PROPN
    , , PUNCT
    twinkle twinkle NOUN
    , , PUNCT
    little little ADJ
    star star NOUN
    , , PUNCT
    How how ADV
    I -PRON- PRON
    wonder wonder VERB
    what what PRON
    you -PRON- PRON
    are be AUX
    ! ! PUNCT


###  pymorphy2

**pymorphy2** ‚Äî is a full-fledged morphological analyzer, written entirely in Python. Unlike Mystem, it does not take into account the context, which means that the question of disambiguation should be resolved by our means. It also knows how to put words in the correct form (conjugate and incline).

[pymorphy2 documentation](https://pymorphy2.readthedocs.io/en/latest/)


```python
# ! pip install --user pymorphy2
```


```python
from pymorphy2 import MorphAnalyzer

morph = MorphAnalyzer()
p = morph.parse('—Å—Ç–∞–ª–∏')
p
```




    [Parse(word='—Å—Ç–∞–ª–∏', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='—Å—Ç–∞—Ç—å', score=0.975342, methods_stack=((DictionaryAnalyzer(), '—Å—Ç–∞–ª–∏', 945, 4),)),
     Parse(word='—Å—Ç–∞–ª–∏', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='—Å—Ç–∞–ª—å', score=0.010958, methods_stack=((DictionaryAnalyzer(), '—Å—Ç–∞–ª–∏', 13, 1),)),
     Parse(word='—Å—Ç–∞–ª–∏', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='—Å—Ç–∞–ª—å', score=0.005479, methods_stack=((DictionaryAnalyzer(), '—Å—Ç–∞–ª–∏', 13, 6),)),
     Parse(word='—Å—Ç–∞–ª–∏', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='—Å—Ç–∞–ª—å', score=0.002739, methods_stack=((DictionaryAnalyzer(), '—Å—Ç–∞–ª–∏', 13, 2),)),
     Parse(word='—Å—Ç–∞–ª–∏', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='—Å—Ç–∞–ª—å', score=0.002739, methods_stack=((DictionaryAnalyzer(), '—Å—Ç–∞–ª–∏', 13, 5),)),
     Parse(word='—Å—Ç–∞–ª–∏', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='—Å—Ç–∞–ª—å', score=0.002739, methods_stack=((DictionaryAnalyzer(), '—Å—Ç–∞–ª–∏', 13, 9),))]




```python
first = p[0]  # –ø–µ—Ä–≤—ã–π —Ä–∞–∑–±–æ—Ä
print('Word:', first.word)
print('Tag:', first.tag)
print('Lemma:', first.normal_form)
print('Proba:', first.score)
```

    Word: —Å—Ç–∞–ª–∏
    Tag: VERB,perf,intr plur,past,indc
    Lemma: —Å—Ç–∞—Ç—å
    Proba: 0.975342


You can get more detailed information from each tag. If the grammeme is in parsing, its value will be returned, if it is not, it will be returned

[list of grammems](https://pymorphy2.readthedocs.io/en/latest/user/grammemes.html)


```python
print(first.normalized)        # –ª–µ–º–º–∞
print(first.tag.POS)           # Part of Speech, —á–∞—Å—Ç—å —Ä–µ—á–∏
print(first.tag.animacy)       # –æ–¥—É—à–µ–≤–ª–µ–Ω–Ω–æ—Å—Ç—å
print(first.tag.aspect)        # –≤–∏–¥: —Å–æ–≤–µ—Ä—à–µ–Ω–Ω—ã–π –∏–ª–∏ –Ω–µ—Å–æ–≤–µ—Ä—à–µ–Ω–Ω—ã–π
print(first.tag.case)          # –ø–∞–¥–µ–∂
print(first.tag.gender)        # —Ä–æ–¥ (–º—É–∂—Å–∫–æ–π, –∂–µ–Ω—Å–∫–∏–π, —Å—Ä–µ–¥–Ω–∏–π)
print(first.tag.involvement)   # –≤–∫–ª—é—á–µ–Ω–Ω–æ—Å—Ç—å –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –≤ –¥–µ–π—Å—Ç–≤–∏–µ
print(first.tag.mood)          # –Ω–∞–∫–ª–æ–Ω–µ–Ω–∏–µ (–ø–æ–≤–µ–ª–∏—Ç–µ–ª—å–Ω–æ–µ, –∏–∑—ä—è–≤–∏—Ç–µ–ª—å–Ω–æ–µ)
print(first.tag.number)        # —á–∏—Å–ª–æ (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ, –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ)
print(first.tag.person)        # –ª–∏—Ü–æ (1, 2, 3)
print(first.tag.tense)         # –≤—Ä–µ–º—è (–Ω–∞—Å—Ç–æ—è—â–µ–µ, –ø—Ä–æ—à–µ–¥—à–µ–µ, –±—É–¥—É—â–µ–µ)
print(first.tag.transitivity)  # –ø–µ—Ä–µ—Ö–æ–¥–Ω–æ—Å—Ç—å (–ø–µ—Ä–µ—Ö–æ–¥–Ω—ã–π, –Ω–µ–ø–µ—Ä–µ—Ö–æ–¥–Ω—ã–π)
print(first.tag.voice)         # –∑–∞–ª–æ–≥ (–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–π, —Å—Ç—Ä–∞–¥–∞—Ç–µ–ª—å–Ω—ã–π)
```

    Parse(word='—Å—Ç–∞—Ç—å', tag=OpencorporaTag('INFN,perf,intr'), normal_form='—Å—Ç–∞—Ç—å', score=1.0, methods_stack=((<DictionaryAnalyzer>, '—Å—Ç–∞—Ç—å', 904, 0),))
    VERB
    None
    perf
    None
    None
    None
    indc
    plur
    None
    past
    intr
    None



```python
print(first.normalized)      
print(first.tag.POS)
print(first.tag.aspect)
print(first.tag.case)
```

    Parse(word='—Å—Ç–∞—Ç—å', tag=OpencorporaTag('INFN,perf,intr'), normal_form='—Å—Ç–∞—Ç—å', score=1.0, methods_stack=((<DictionaryAnalyzer>, '—Å—Ç–∞—Ç—å', 904, 0),))
    VERB
    perf
    None


### mystem vs. pymorphy

1) Both of them can work with out-of-vocabulary words (OOV).

2) *Speed*. Mystem runs incredibly slow under Windows on large texts, but still very fast if you run it from the console on linux / mac os.

3) *Disambiguation*. Mystem is able to disambiguate words by context (although it does not always succeed), pymorphy2 takes one word as input and, accordingly, does not know how to disambiguate by context at all


```python
homonym1 = '–ó–∞ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —è –ø—Ä–æ—Å–ª—É—à–∞–ª –±–æ–ª—å—à–µ —Å–æ—Ä–æ–∫–∞ –∫—É—Ä—Å–æ–≤.'
homonym2 = '–°–æ—Ä–æ–∫–∞ —Å–≤–æ—Ä–æ–≤–∞–ª–∞ –±–ª–µ—Å—Ç—è—â–µ–µ —É–∫—Ä–∞—à–µ–Ω–∏–µ —Å–æ —Å—Ç–æ–ª–∞.'
mystem_analyzer = Mystem() # initialize object with default parameters

print(mystem_analyzer.analyze(homonym1)[-5])
print(mystem_analyzer.analyze(homonym2)[0])
```

    {'text': '—Å–æ—Ä–æ–∫–∞', 'analysis': [{'lex': '—Å–æ—Ä–æ–∫', 'gr': 'NUM=(–ø—Ä|–¥–∞—Ç|—Ä–æ–¥|—Ç–≤–æ—Ä)'}]}
    {'text': '–°–æ—Ä–æ–∫–∞', 'analysis': [{'lex': '—Å–æ—Ä–æ–∫–∞', 'gr': 'S,–∂–µ–Ω,–æ–¥=–∏–º,–µ–¥'}]}



```python
p = morph.parse('—Å–æ—Ä–æ–∫–∞')
```


```python
p
```




    [Parse(word='—Å–æ—Ä–æ–∫–∞', tag=OpencorporaTag('NUMR loct'), normal_form='—Å–æ—Ä–æ–∫', score=0.285714, methods_stack=((<DictionaryAnalyzer>, '—Å–æ—Ä–æ–∫–∞', 2802, 5),)),
     Parse(word='—Å–æ—Ä–æ–∫–∞', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='—Å–æ—Ä–æ–∫–∞', score=0.142857, methods_stack=((<DictionaryAnalyzer>, '—Å–æ—Ä–æ–∫–∞', 43, 0),)),
     Parse(word='—Å–æ—Ä–æ–∫–∞', tag=OpencorporaTag('NOUN,anim,femn sing,nomn'), normal_form='—Å–æ—Ä–æ–∫–∞', score=0.142857, methods_stack=((<DictionaryAnalyzer>, '—Å–æ—Ä–æ–∫–∞', 403, 0),)),
     Parse(word='—Å–æ—Ä–æ–∫–∞', tag=OpencorporaTag('NUMR gent'), normal_form='—Å–æ—Ä–æ–∫', score=0.142857, methods_stack=((<DictionaryAnalyzer>, '—Å–æ—Ä–æ–∫–∞', 2802, 1),)),
     Parse(word='—Å–æ—Ä–æ–∫–∞', tag=OpencorporaTag('NUMR datv'), normal_form='—Å–æ—Ä–æ–∫', score=0.142857, methods_stack=((<DictionaryAnalyzer>, '—Å–æ—Ä–æ–∫–∞', 2802, 2),)),
     Parse(word='—Å–æ—Ä–æ–∫–∞', tag=OpencorporaTag('NUMR ablt'), normal_form='—Å–æ—Ä–æ–∫', score=0.142857, methods_stack=((<DictionaryAnalyzer>, '—Å–æ—Ä–æ–∫–∞', 2802, 4),))]



### Joining it all together:

Let us make a standard data preprocessing pipeline of texts from the Lenta.ru website


```python
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1IMtWrTrs54SVzHlTgtBWqcQA990CCrQh' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1IMtWrTrs54SVzHlTgtBWqcQA990CCrQh" -O lenta-ru-news-full.csv && rm -rf /tmp/cookies.txt
```

    --2021-10-26 19:36:26--  https://docs.google.com/uc?export=download&confirm=94CX&id=1IMtWrTrs54SVzHlTgtBWqcQA990CCrQh
    Resolving docs.google.com (docs.google.com)... 74.125.134.113, 74.125.134.139, 74.125.134.100, ...
    Connecting to docs.google.com (docs.google.com)|74.125.134.113|:443... connected.
    HTTP request sent, awaiting response... 302 Moved Temporarily
    Location: https://doc-0c-bc-docs.googleusercontent.com/docs/securesc/82i407l4bglp5lqo6aftb2tgvf5s9g97/c4i97c0alcigkiodq3ec7ffqfjufgeqn/1635276975000/14013181690233399305/11514772734847751932Z/1IMtWrTrs54SVzHlTgtBWqcQA990CCrQh?e=download [following]
    --2021-10-26 19:36:26--  https://doc-0c-bc-docs.googleusercontent.com/docs/securesc/82i407l4bglp5lqo6aftb2tgvf5s9g97/c4i97c0alcigkiodq3ec7ffqfjufgeqn/1635276975000/14013181690233399305/11514772734847751932Z/1IMtWrTrs54SVzHlTgtBWqcQA990CCrQh?e=download
    Resolving doc-0c-bc-docs.googleusercontent.com (doc-0c-bc-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84
    Connecting to doc-0c-bc-docs.googleusercontent.com (doc-0c-bc-docs.googleusercontent.com)|172.217.204.132|:443... connected.
    HTTP request sent, awaiting response... 302 Found
    Location: https://docs.google.com/nonceSigner?nonce=uq550be0h2if8&continue=https://doc-0c-bc-docs.googleusercontent.com/docs/securesc/82i407l4bglp5lqo6aftb2tgvf5s9g97/c4i97c0alcigkiodq3ec7ffqfjufgeqn/1635276975000/14013181690233399305/11514772734847751932Z/1IMtWrTrs54SVzHlTgtBWqcQA990CCrQh?e%3Ddownload&hash=ab67k9r9b1sp1rsogml1udeavvave01f [following]
    --2021-10-26 19:36:26--  https://docs.google.com/nonceSigner?nonce=uq550be0h2if8&continue=https://doc-0c-bc-docs.googleusercontent.com/docs/securesc/82i407l4bglp5lqo6aftb2tgvf5s9g97/c4i97c0alcigkiodq3ec7ffqfjufgeqn/1635276975000/14013181690233399305/11514772734847751932Z/1IMtWrTrs54SVzHlTgtBWqcQA990CCrQh?e%3Ddownload&hash=ab67k9r9b1sp1rsogml1udeavvave01f
    Connecting to docs.google.com (docs.google.com)|74.125.134.113|:443... connected.
    HTTP request sent, awaiting response... 302 Found
    Location: https://doc-0c-bc-docs.googleusercontent.com/docs/securesc/82i407l4bglp5lqo6aftb2tgvf5s9g97/c4i97c0alcigkiodq3ec7ffqfjufgeqn/1635276975000/14013181690233399305/11514772734847751932Z/1IMtWrTrs54SVzHlTgtBWqcQA990CCrQh?e=download&nonce=uq550be0h2if8&user=11514772734847751932Z&hash=5rn3pfspspiui2ob0f9li8tqfcotjd7j [following]
    --2021-10-26 19:36:26--  https://doc-0c-bc-docs.googleusercontent.com/docs/securesc/82i407l4bglp5lqo6aftb2tgvf5s9g97/c4i97c0alcigkiodq3ec7ffqfjufgeqn/1635276975000/14013181690233399305/11514772734847751932Z/1IMtWrTrs54SVzHlTgtBWqcQA990CCrQh?e=download&nonce=uq550be0h2if8&user=11514772734847751932Z&hash=5rn3pfspspiui2ob0f9li8tqfcotjd7j
    Connecting to doc-0c-bc-docs.googleusercontent.com (doc-0c-bc-docs.googleusercontent.com)|172.217.204.132|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 2084746431 (1.9G) [text/csv]
    Saving to: ‚Äòlenta-ru-news-full.csv‚Äô
    
    lenta-ru-news-full. 100%[===================>]   1.94G  78.1MB/s    in 21s     
    
    2021-10-26 19:36:48 (93.2 MB/s) - ‚Äòlenta-ru-news-full.csv‚Äô saved [2084746431/2084746431]
    



```python
import pandas as pd
pd.set_option('display.max_columns', None)  
pd.set_option('display.expand_frame_repr', False)
pd.set_option('max_colwidth', 800)

data = pd.read_csv('./lenta-ru-news-full.csv', usecols=['text'])
data.sample(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>214459</th>
      <td>–ß–∏—Å–ª–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π —Å–∞–π—Ç–æ–≤ –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö –≥–∞–∑–µ—Ç –≤ 2007 –≥–æ–¥—É —Ä–µ–∑–∫–æ –≤–æ–∑—Ä–æ—Å–ª–æ, —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É—é—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, –æ–±–Ω–∞—Ä–æ–¥–æ–≤–∞–Ω–Ω—ã–µ 24 —è–Ω–≤–∞—Ä—è –ì–∞–∑–µ—Ç–Ω–æ–π –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–µ–π –ê–º–µ—Ä–∏–∫–∏ (Newspaper Association of America), —Å–æ–æ–±—â–∞–µ—Ç Reuters. –í 2007 –≥–æ–¥—É –≤ —Å—Ä–µ–¥–Ω–µ–º –∑–∞ –º–µ—Å—è—Ü –Ω–∞ —Å–∞–π—Ç—ã –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö –≥–∞–∑–µ—Ç –∑–∞—Ö–æ–¥–∏–ª–∏ –æ–∫–æ–ª–æ 60 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π - –Ω–∞ —à–µ—Å—Ç—å –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –±–æ–ª—å—à–µ, —á–µ–º –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –≥–æ–¥—É. –ü—Ä–∏ —ç—Ç–æ–º –≤ —á–µ—Ç–≤–µ—Ä—Ç–æ–º –∫–≤–∞—Ä—Ç–∞–ª–µ 2007 –≥–æ–¥–∞ –∞—É–¥–∏—Ç–æ—Ä–∏—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–≤–µ—Ä—Å–∏–π –≥–∞–∑–µ—Ç, –∏–∑–¥–∞—é—â–∏—Ö—Å—è –≤ –°–®–ê, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –ø—Ä–∏—Ä–æ—Å—Ç –Ω–∞ 9 –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –≤ –º–µ—Å—è—Ü. –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–≤–µ—Ä—Å–∏–∏ –≥–∞–∑–µ—Ç –ø–æ—Å–µ—â–∞–ª–∏ –≤ 2007 –≥–æ–¥—É –Ω–µ –º–µ–Ω–µ–µ 39 –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –æ—Ç –≤—Å–µ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏ –°–µ—Ç–∏. –ü—Ä–∏ —ç—Ç–æ–º —Å—Ä–µ–¥–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ–≤–æ–¥–∏–ª –Ω–∞ —Å–∞–π—Ç–∞—Ö –≥–∞–∑–µ—Ç –Ω–µ –º–µ–Ω–µ–µ 44 –º–∏–Ω—É—Ç –≤ –º–µ—Å—è—Ü. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ –ì–∞–∑–µ—Ç–Ω–æ–π –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–µ–π –ê–º–µ—Ä–∏–∫–∏, —É—á–∏—Ç—ã–≤–∞—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –≤—ã—Ö–æ–¥—è—â–∏—Ö –≤ ...</td>
    </tr>
    <tr>
      <th>664833</th>
      <td>–ù–∞ –õ–æ–Ω–¥–æ–Ω—Å–∫–æ–º –º–æ—Å—Ç—É –º–∏–∫—Ä–æ–∞–≤—Ç–æ–±—É—Å –Ω–∞–µ—Ö–∞–ª –Ω–∞ –ø–µ—à–µ—Ö–æ–¥–æ–≤, –ø—Ä–æ—Ö–æ–¥–∏—Ç —ç–≤–∞–∫—É–∞—Ü–∏—è –ª—é–¥–µ–π, —Å–æ–æ–±—â–∞–µ—Ç –≥–∞–∑–µ—Ç–∞ The Sun. –û—á–µ–≤–∏–¥—Ü—ã —Ç–∞–∫–∂–µ —Å–æ–æ–±—â–∞—é—Ç, —á—Ç–æ –≤ —Ä–∞–π–æ–Ω–µ –º–æ—Å—Ç–∞ —Å–ª—ã—à–Ω—ã –≤—ã—Å—Ç—Ä–µ–ª—ã, –∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª—é–¥–µ–π –ø–æ–ª—É—á–∏–ª–∏ –Ω–æ–∂–µ–≤—ã–µ —Ä–∞–Ω–µ–Ω–∏—è. –ü–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ BBC News, –Ω–∞ –º–µ—Å—Ç–µ –ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è —Ä–∞–±–æ—Ç–∞—é—Ç –≤–æ–æ—Ä—É–∂–µ–Ω–Ω—ã–µ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ –ø–æ–ª–∏—Ü–∏–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è, –∫–∞–∫ –ø–∏—à–µ—Ç The Telegraph, –ø–æ—Å—Ç—Ä–∞–¥–∞–ª–∏ 15-20 —á–µ–ª–æ–≤–µ–∫. 22 –º–∞—Ä—Ç–∞ –≤ –õ–æ–Ω–¥–æ–Ω–µ —É—Ä–æ–∂–µ–Ω–µ—Ü —é–≥–æ-–≤–æ—Å—Ç–æ–∫–∞ –ê–Ω–≥–ª–∏–∏ –≠–¥—Ä–∏–∞–Ω –†–∞—Å—Å–µ–ª –ê–¥–∂–∞–æ, —Å–º–µ–Ω–∏–≤—à–∏–π –∏–º—è –Ω–∞ –•–∞–ª–∏–¥ –ú–∞—Å—É–¥, –Ω–∞—Ö–æ–¥—è—Å—å –∑–∞ —Ä—É–ª–µ–º –∞–≤—Ç–æ–º–æ–±–∏–ª—è, –Ω–∞–ø—Ä–∞–≤–∏–ª –µ–≥–æ –Ω–∞ –ø–µ—à–µ—Ö–æ–¥–æ–≤ –Ω–∞ –í–µ—Å—Ç–º–∏–Ω—Å—Ç–µ—Ä—Å–∫–æ–º –º–æ—Å—Ç—É, —Å–±–∏–ª –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–µ–ª–æ–≤–µ–∫ –∏ –¥–æ–µ—Ö–∞–ª –¥–æ –∑–¥–∞–Ω–∏—è –ø–∞—Ä–ª–∞–º–µ–Ω—Ç–∞. –¢–∞–º –º–∞—à–∏–Ω—É –æ—Å—Ç–∞–Ω–æ–≤–∏–ª –ø–æ–ª–∏—Ü–µ–π—Å–∫–∏–π. –ü—Ä–µ—Å—Ç—É–ø–Ω–∏–∫ –∑–∞—Ä–µ–∑–∞–ª –µ–≥–æ –Ω–æ–∂–æ–º, –ø–æ—Å–ª–µ —á–µ–≥–æ –±—ã–ª –∑–∞—Å—Ç—Ä–µ–ª–µ–Ω –¥—Ä—É–≥–∏–º —Å—Ç—Ä–∞–∂–µ–º –ø–æ—Ä—è–¥–∫–∞. –ü–æ–≥–∏–±–ª–∏ –ø—è—Ç—å —á–µ–ª–æ–≤–µ–∫.</td>
    </tr>
    <tr>
      <th>110006</th>
      <td>–ê–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∞—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è Altnet –Ω–∞–ø—Ä–∞–≤–∏–ª–∞ –ø–∏—Å—å–º–∞ –≤–ª–∞–¥–µ–ª—å—Ü–∞–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–∞–∑–∏—Ä—É—é—â–∏—Ö—Å—è –≤ –°–®–ê —Ñ–∞–π–ª–æ–æ–±–º–µ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ–º –æ —Ç–æ–º, —á—Ç–æ –æ–Ω–∏ –Ω–µ–∑–∞–∫–æ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≤ —Å–≤–æ–µ–π —Ä–∞–±–æ—Ç–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é, –ø–∞—Ç–µ–Ω—Ç –Ω–∞ –∫–æ—Ç–æ—Ä—É—é –µ–π –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç. –ö–∞–∫ —Å–æ–æ–±—â–∞–µ—Ç –≥–∞–∑–µ—Ç–∞ Washington Post, Altnet –≤–ª–∞–¥–µ–µ—Ç –ø–∞—Ç–µ–Ω—Ç–æ–º –Ω–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é "—Ö—ç—à–∏–Ω–≥–∞" (hashing), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—É—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç —Å–Ω–∞–±–∂–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ –∫—Ä–∞—Ç–∫–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ–º ("—Ö—ç—à–µ–º"), —Å –∫–æ—Ç–æ—Ä—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å–µ—Ç–µ–π –º–æ–≥—É—Ç –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ —Ç–æ–º –∏–ª–∏ –∏–Ω–æ–º —Ñ–∞–π–ª–µ. –°–µ—Ç–∏ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ö—ç—à–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—É–±–ª–∏—á–Ω—ã–º–∏ –∫–∞—Ç–∞–ª–æ–≥–∞–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –ö–∞–∫ –∏–∑–≤–µ—Å—Ç–Ω–æ, —Ñ–∞–π–ª–æ–æ–±–º–µ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω—ã –¥–ª—è –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ –æ–±–º–µ–Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏ –º–µ–∂–¥—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç–∞, –∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–µ–¥–∏–∞–ø—Ä–æ–¥—É–∫—Ü–∏–∏, —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ–º–æ–π –ø–æ —Ç–∞–∫–∏–º —Å–µ—Ç—è–º, —è–≤–ª—è–µ—Ç—Å—è –Ω–µ–ª–∏—Ü–µ–Ω–∑–∏–æ–Ω–Ω–æ–π. ...</td>
    </tr>
    <tr>
      <th>774383</th>
      <td>–ú—ç—Ä –ú–æ—Å–∫–≤—ã –°–µ—Ä–≥–µ–π –°–æ–±—è–Ω–∏–Ω –∑–∞—è–≤–∏–ª, —á—Ç–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä—ã –Ω–µ—Å–∞–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –≤ —Ü–µ–Ω—Ç—Ä–µ –ú–æ—Å–∫–≤—ã –ø—ã—Ç–∞–ª–∏—Å—å –≤—Ç—è–Ω—É—Ç—å —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –≤ –º–∞—Å—Å–æ–≤—ã–µ –±–µ—Å–ø–æ—Ä—è–¥–∫–∏. –û–± —ç—Ç–æ–º –æ–Ω –∑–∞—è–≤–∏–ª –≤ —ç—Ñ–∏—Ä–µ —Ç–µ–ª–µ–∫–∞–Ω–∞–ª–∞ ¬´–†–æ—Å—Å–∏—è-1¬ª, —Å–æ–æ–±—â–∞–µ—Ç –≤ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ, 4 –∞–≤–≥—É—Å—Ç–∞, —Ä–∞–¥–∏–æ—Å—Ç–∞–Ω—Ü–∏—è ¬´–ì–æ–≤–æ—Ä–∏—Ç –ú–æ—Å–∫–≤–∞¬ª. –ü–æ –µ–≥–æ —Å–ª–æ–≤–∞–º, –Ω–µ –≤—Å–µ –ø—Ä–∏—à–µ–¥—à–∏–µ –Ω–∞ –∞–∫—Ü–∏—é –≥—Ä–∞–∂–¥–∞–Ω–µ –æ–∂–∏–¥–∞–ª–∏ ¬´—Ç–∞–∫–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è —Å–æ–±—ã—Ç–∏–π¬ª. ¬´–≠—Ç–æ –≤—Å–µ –Ω–µ –¥–ª—è –±–ª–∞–≥–∞ –ª—é–¥–µ–π, –∞ —Ä–∞–¥–∏ —á—å–∏—Ö-—Ç–æ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö, —É–∑–∫–æ–∫–æ—Ä—ã—Å—Ç–Ω—ã—Ö —Ü–µ–ª–µ–π‚Ä¶¬ª ‚Äî –≤—ã—Å–∫–∞–∑–∞–ª—Å—è –≥—Ä–∞–¥–æ–Ω–∞—á–∞–ª—å–Ω–∏–∫. –û–Ω –¥–æ–±–∞–≤–∏–ª, —á—Ç–æ, –ø–æ –µ–≥–æ –¥–∞–Ω–Ω—ã–º, –º–Ω–æ–≥–∏–µ —Å–æ–±—Ä–∞–≤—à–∏–µ—Å—è –Ω–µ –∏–º–µ–ª–∏ ¬´–Ω–∏–∫–∞–∫–æ–≥–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –∫ –ú–æ—Å–∫–≤–µ¬ª –∏ –∫ –≤—ã–±–æ—Ä–∞–º –≤ –ú–æ—Å–≥–æ—Ä–¥—É–º—É. ¬´–Ø —É–≤–µ—Ä–µ–Ω, —á—Ç–æ –º–æ—Å–∫–≤–∏—á–∏ —ç—Ç–æ —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞—é—Ç¬ª, ‚Äî –∑–∞–∫–ª—é—á–∏–ª –°–æ–±—è–Ω–∏–Ω. –ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–π –º–∏—Ç–∏–Ω–≥ –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É –Ω–µ –¥–æ–ø—É—â–µ–Ω–Ω—ã—Ö –Ω–∞ –≤—ã–±–æ—Ä—ã –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ –ú–æ—Å–≥–æ—Ä–¥—É–º—É –Ω–∞—á–∞–ª—Å—è –≤ –ú–æ—Å–∫–≤–µ –¥–Ω–µ–º –≤ —Å—É–±–±–æ—Ç—É, 3 –∞–≤–≥—É—Å—Ç–∞. –ü–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ú–í...</td>
    </tr>
    <tr>
      <th>518138</th>
      <td>–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ –°–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–º–∏—Ç–µ—Ç–∞ –†–æ—Å—Å–∏–∏ –∑–∞–¥–µ—Ä–∂–∞–ª–∏ –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ –≥–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞ –∫–æ–º–ø–∞–Ω–∏–∏ ¬´–≠–∫—Å–ø–æ-—Ç—É—Ä¬ª –ò–≥–æ—Ä—è –†—é—Ä–∏–∫–æ–≤–∞, —Å–æ–æ–±—â–∞–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –≤–µ–¥–æ–º—Å—Ç–≤–∞ –í–ª–∞–¥–∏–º–∏—Ä –ú–∞—Ä–∫–∏–Ω. –ü–æ –¥–∞–Ω–Ω—ã–º —Å–ª–µ–¥—Å—Ç–≤–∏—è, –∑–∞–¥–µ—Ä–∂–∞–Ω–Ω—ã–π –∑–Ω–∞–ª, —á—Ç–æ –µ–≥–æ –∫–æ–º–ø–∞–Ω–∏—é –∏—Å–∫–ª—é—á–∞—Ç –∏–∑ –ï–¥–∏–Ω–æ–≥–æ —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–µ–µ—Å—Ç—Ä–∞ –≤ –∏—é–Ω–µ 2014¬†–≥–æ–¥–∞. –û–¥–Ω–∞–∫–æ –æ–Ω –¥–∞–ª —É–∫–∞–∑–∞–Ω–∏–µ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ä–∞–±–æ—Ç—É –∏ –æ—Ñ–æ—Ä–º–ª—è—Ç—å –ø—É—Ç–µ–≤–∫–∏. ¬´–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –µ–≥–æ –¥–µ–π—Å—Ç–≤–∏–π –±—ã–ª–∏ –ø–æ—Ö–∏—â–µ–Ω—ã –¥–µ–Ω–µ–∂–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ –≥—Ä–∞–∂–¥–∞–Ω –Ω–∞ –æ–±—â—É—é —Å—É–º–º—É –±–æ–ª–µ–µ –æ–¥–Ω–æ–≥–æ –º–∏–ª–ª–∏–æ–Ω–∞ —Ä—É–±–ª–µ–π¬ª, ‚Äî –ø–æ–¥—á–µ—Ä–∫–Ω—É–ª –ú–∞—Ä–∫–∏–Ω. –í –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è —Ä–µ—à–∞–µ—Ç—Å—è –≤–æ–ø—Ä–æ—Å –æ –µ–≥–æ –∞—Ä–µ—Å—Ç–µ –∏ –ø—Ä–µ–¥—ä—è–≤–ª–µ–Ω–∏–∏ –æ–±–≤–∏–Ω–µ–Ω–∏—è. –ü–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–æ–∫ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç—É—Ä—Ñ–∏—Ä–º –≤ –ú–æ—Å–∫–≤–µ –∏ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤–æ–∑–±—É–∂–¥–µ–Ω–æ —à–µ—Å—Ç—å —É–≥–æ–ª–æ–≤–Ω—ã—Ö –¥–µ–ª –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∫–æ–º–ø–∞–Ω–∏–π, —Å–æ–æ–±—â–∞–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –ì–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–π –ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä—ã...</td>
    </tr>
  </tbody>
</table>
</div>




```python
# !pip install pymorphy2
```


```python
import pymorphy2
import re
```


```python
m = pymorphy2.MorphAnalyzer()

# delete non-alphabetic characters
regex = re.compile("[–ê-–Ø–∞-—èA-z]+")

def words_only(text, regex=regex):
    try:
        return regex.findall(text.lower())
    except:
        return []
```


```python
print(data.text[0])
```

    –ë–æ–∏ —É –°–æ–ø–æ—Ü–∫–∏–Ω–∞ –∏ –î—Ä—É—Å–∫–µ–Ω–∏–∫ –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ–º –≥–µ—Ä–º–∞–Ω—Ü–µ–≤. –ù–µ–ø—Ä–∏—è—Ç–µ–ª—å, –ø—Ä–∏–±–ª–∏–∑–∏–≤—à–∏—Å—å —Å —Å–µ–≤–µ—Ä–∞ –∫ –û—Å–æ–≤—Ü—É –Ω–∞—á–∞–ª –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏–π—Å–∫—É—é –±–æ—Ä—å–±—É —Å –∫—Ä–µ–ø–æ—Å—Ç—å—é. –í –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏–π—Å–∫–æ–º –±–æ—é –ø—Ä–∏–Ω–∏–º–∞—é—Ç —É—á–∞—Å—Ç–∏–µ —Ç—è–∂–µ–ª—ã–µ –∫–∞–ª–∏–±—Ä—ã. –° —Ä–∞–Ω–Ω–µ–≥–æ —É—Ç—Ä–∞ 14 —Å–µ–Ω—Ç—è–±—Ä—è –æ–≥–æ–Ω—å –¥–æ—Å—Ç–∏–≥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è. –ü–æ–ø—ã—Ç–∫–∞ –≥–µ—Ä–º–∞–Ω—Å–∫–æ–π –ø–µ—Ö–æ—Ç—ã –ø—Ä–æ–±–∏—Ç—å—Å—è –±–ª–∏–∂–µ –∫ –∫—Ä–µ–ø–æ—Å—Ç–∏ –æ—Ç—Ä–∞–∂–µ–Ω–∞. –í –ì–∞–ª–∏—Ü–∏–∏ –º—ã –∑–∞–Ω—è–ª–∏ –î–µ–º–±–∏—Ü—É. –ë–æ–ª—å—à–∞—è –∫–æ–ª–æ–Ω–Ω–∞, –æ—Ç—Å—Ç—É–ø–∞–≤—à–∞—è –ø–æ —à–æ—Å—Å–µ –æ—Ç –ü–µ—Ä–µ–º—ã—à–ª—è –∫ –°–∞–Ω–æ–∫—É, –æ–±—Å—Ç—Ä–µ–ª–∏–≤–∞–ª–∞—Å—å —Å –≤—ã—Å–æ—Ç –Ω–∞—à–µ–π –±–∞—Ç–∞—Ä–µ–µ–π –∏ –±–µ–∂–∞–ª–∞, –±—Ä–æ—Å–∏–≤ –ø–∞—Ä–∫–∏, –æ–±–æ–∑ –∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏. –í—ã–ª–∞–∑–∫–∏ –≥–∞—Ä–Ω–∏–∑–æ–Ω–∞ –ü–µ—Ä–µ–º—ã—à–ª—è –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑—É—Å–ø–µ—à–Ω—ã–º–∏. –ü—Ä–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—é—â–µ–º—Å—è –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–∏ –∞–≤—Å—Ç—Ä–∏–π—Ü–µ–≤ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç—Å—è –ø–æ–ª–Ω–æ–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –∏—Ö —á–∞—Å—Ç–µ–π, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç—Å—è –Ω–æ–≤—ã–µ –ø–∞—Ä—Ç–∏–∏ –ø–ª–µ–Ω–Ω—ã—Ö, –æ—Ä—É–¥–∏—è –∏ –ø—Ä–æ—á–∞—è –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å. –ù–∞ –ø–µ—Ä–µ–≤–∞–ª–µ –£–∂–æ–∫ –º—ã —Ä–∞–∑–±–∏–ª–∏ –Ω–µ–ø—Ä–∏—è—Ç–µ–ª—å—Å–∫–∏–π –æ—Ç—Ä—è–¥, –≤–∑—è–ª–∏ –µ–≥–æ –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏—é –∏ –º–Ω–æ–≥–æ –ø–ª–µ–Ω–Ω—ã—Ö –∏, –ø—Ä–æ–¥–æ–ª–∂–∞—è –ø—Ä–µ—Å–ª–µ–¥–æ–≤–∞—Ç—å, –≤—Å—Ç—É–ø–∏–ª–∏ –≤ –ø—Ä–µ–¥–µ–ª—ã –í–µ–Ω–≥—Ä–∏–∏. 
    ¬´–†—É—Å—Å–∫–∏–π –∏–Ω–≤–∞–ª–∏–¥¬ª, 16 —Å–µ–Ω—Ç—è–±—Ä—è 1914 –≥–æ–¥–∞.



```python
print(*words_only(data.text[0]))
```

    –±–æ–∏ —É —Å–æ–ø–æ—Ü–∫–∏–Ω–∞ –∏ –¥—Ä—É—Å–∫–µ–Ω–∏–∫ –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ–º –≥–µ—Ä–º–∞–Ω—Ü–µ–≤ –Ω–µ–ø—Ä–∏—è—Ç–µ–ª—å –ø—Ä–∏–±–ª–∏–∑–∏–≤—à–∏—Å—å —Å —Å–µ–≤–µ—Ä–∞ –∫ –æ—Å–æ–≤—Ü—É –Ω–∞—á–∞–ª –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏–π—Å–∫—É—é –±–æ—Ä—å–±—É —Å –∫—Ä–µ–ø–æ—Å—Ç—å—é –≤ –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏–π—Å–∫–æ–º –±–æ—é –ø—Ä–∏–Ω–∏–º–∞—é—Ç —É—á–∞—Å—Ç–∏–µ —Ç—è–∂–µ–ª—ã–µ –∫–∞–ª–∏–±—Ä—ã —Å —Ä–∞–Ω–Ω–µ–≥–æ —É—Ç—Ä–∞ —Å–µ–Ω—Ç—è–±—Ä—è –æ–≥–æ–Ω—å –¥–æ—Å—Ç–∏–≥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –ø–æ–ø—ã—Ç–∫–∞ –≥–µ—Ä–º–∞–Ω—Å–∫–æ–π –ø–µ—Ö–æ—Ç—ã –ø—Ä–æ–±–∏—Ç—å—Å—è –±–ª–∏–∂–µ –∫ –∫—Ä–µ–ø–æ—Å—Ç–∏ –æ—Ç—Ä–∞–∂–µ–Ω–∞ –≤ –≥–∞–ª–∏—Ü–∏–∏ –º—ã –∑–∞–Ω—è–ª–∏ –¥–µ–º–±–∏—Ü—É –±–æ–ª—å—à–∞—è –∫–æ–ª–æ–Ω–Ω–∞ –æ—Ç—Å—Ç—É–ø–∞–≤—à–∞—è –ø–æ —à–æ—Å—Å–µ –æ—Ç –ø–µ—Ä–µ–º—ã—à–ª—è –∫ —Å–∞–Ω–æ–∫—É –æ–±—Å—Ç—Ä–µ–ª–∏–≤–∞–ª–∞—Å—å —Å –≤—ã—Å–æ—Ç –Ω–∞—à–µ–π –±–∞—Ç–∞—Ä–µ–µ–π –∏ –±–µ–∂–∞–ª–∞ –±—Ä–æ—Å–∏–≤ –ø–∞—Ä–∫–∏ –æ–±–æ–∑ –∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ –≤—ã–ª–∞–∑–∫–∏ –≥–∞—Ä–Ω–∏–∑–æ–Ω–∞ –ø–µ—Ä–µ–º—ã—à–ª—è –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑—É—Å–ø–µ—à–Ω—ã–º–∏ –ø—Ä–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—é—â–µ–º—Å—è –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–∏ –∞–≤—Å—Ç—Ä–∏–π—Ü–µ–≤ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç—Å—è –ø–æ–ª–Ω–æ–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –∏—Ö —á–∞—Å—Ç–µ–π –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç—Å—è –Ω–æ–≤—ã–µ –ø–∞—Ä—Ç–∏–∏ –ø–ª–µ–Ω–Ω—ã—Ö –æ—Ä—É–¥–∏—è –∏ –ø—Ä–æ—á–∞—è –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –Ω–∞ –ø–µ—Ä–µ–≤–∞–ª–µ —É–∂–æ–∫ –º—ã —Ä–∞–∑–±–∏–ª–∏ –Ω–µ–ø—Ä–∏—è—Ç–µ–ª—å—Å–∫–∏–π –æ—Ç—Ä—è–¥ –≤–∑—è–ª–∏ –µ–≥–æ –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏—é –∏ –º–Ω–æ–≥–æ –ø–ª–µ–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–¥–æ–ª–∂–∞—è –ø—Ä–µ—Å–ª–µ–¥–æ–≤–∞—Ç—å –≤—Å—Ç—É–ø–∏–ª–∏ –≤ –ø—Ä–µ–¥–µ–ª—ã –≤–µ–Ω–≥—Ä–∏–∏ —Ä—É—Å—Å–∫–∏–π –∏–Ω–≤–∞–ª–∏–¥ —Å–µ–Ω—Ç—è–±—Ä—è –≥–æ–¥–∞


`@lru_cache` method creates a cache of the specified size for the `lemmatize` function, which allows you to speed up text lemmatization in general (which is very useful, since lemmatization is a resource-intensive process).


```python
import functools
from nltk.corpus import stopwords
```


```python
@functools.lru_cache(maxsize=128)
def lemmatize_word(token, pymorphy=m):
    return pymorphy.parse(token)[0].normal_form

def lemmatize_text(text):
    return [lemmatize_word(w) for w in text]
```


```python
tokens = words_only(data.text[0])

print(lemmatize_text(tokens))
```

    ['–±–æ–π', '—É', '—Å–æ–ø–æ—Ü–∫–∏–Ω–∞', '–∏', '–¥—Ä—É—Å–∫–µ–Ω–∏–∫', '–∑–∞–∫–æ–Ω—á–∏—Ç—å—Å—è', '–æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ', '–≥–µ—Ä–º–∞–Ω–µ—Ü', '–Ω–µ–ø—Ä–∏—è—Ç–µ–ª—å', '–ø—Ä–∏–±–ª–∏–∑–∏—Ç—å—Å—è', '—Å', '—Å–µ–≤–µ—Ä', '–∫', '–æ—Å–æ–≤—Ü–∞', '–Ω–∞—á–∞—Ç—å', '–∞—Ä—Ç–∏–ª–ª–µ—Ä–∏–π—Å–∫–∏–π', '–±–æ—Ä—å–±–∞', '—Å', '–∫—Ä–µ–ø–æ—Å—Ç—å', '–≤', '–∞—Ä—Ç–∏–ª–ª–µ—Ä–∏–π—Å–∫–∏–π', '–±–æ–π', '–ø—Ä–∏–Ω–∏–º–∞—Ç—å', '—É—á–∞—Å—Ç–∏–µ', '—Ç—è–∂—ë–ª—ã–π', '–∫–∞–ª–∏–±—Ä', '—Å', '—Ä–∞–Ω–Ω–∏–π', '—É—Ç—Ä–æ', '—Å–µ–Ω—Ç—è–±—Ä—å', '–æ–≥–æ–Ω—å', '–¥–æ—Å—Ç–∏–≥–Ω—É—Ç—å', '–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π', '–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ', '–ø–æ–ø—ã—Ç–∫–∞', '–≥–µ—Ä–º–∞–Ω—Å–∫–∏–π', '–ø–µ—Ö–æ—Ç–∞', '–ø—Ä–æ–±–∏—Ç—å—Å—è', '–±–ª–∏–∑–∫–∏–π', '–∫', '–∫—Ä–µ–ø–æ—Å—Ç—å', '–æ—Ç—Ä–∞–∑–∏—Ç—å', '–≤', '–≥–∞–ª–∏—Ü–∏—è', '–º—ã', '–∑–∞–Ω—è—Ç—å', '–¥–µ–º–±–∏—Ü–∞', '–±–æ–ª—å—à–∏–π', '–∫–æ–ª–æ–Ω–Ω–∞', '–æ—Ç—Å—Ç—É–ø–∞—Ç—å', '–ø–æ', '—à–æ—Å—Å–µ', '–æ—Ç', '–ø–µ—Ä–µ–º—ã—à–ª—å', '–∫', '—Å–∞–Ω–æ–∫', '–æ–±—Å—Ç—Ä–µ–ª–∏–≤–∞—Ç—å—Å—è', '—Å', '–≤—ã—Å–æ—Ç–∞', '–Ω–∞—à', '–±–∞—Ç–∞—Ä–µ—è', '–∏', '–±–µ–∂–∞—Ç—å', '–±—Ä–æ—Å–∏—Ç—å', '–ø–∞—Ä–∫–∞', '–æ–±–æ–∑', '–∏', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', '–≤—ã–ª–∞–∑–∫–∞', '–≥–∞—Ä–Ω–∏–∑–æ–Ω', '–ø–µ—Ä–µ–º—ã—à–ª—å', '–æ—Å—Ç–∞–≤–∞—Ç—å—Å—è', '–±–µ–∑—É—Å–ø–µ—à–Ω—ã–π', '–ø—Ä–∏', '–ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å—Å—è', '–æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ', '–∞–≤—Å—Ç—Ä–∏–µ—Ü', '–æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å—Å—è', '–ø–æ–ª–Ω—ã–π', '–ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ', '–æ–Ω–∏', '—á–∞—Å—Ç—å', '–∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å—Å—è', '–Ω–æ–≤—ã–π', '–ø–∞—Ä—Ç–∏—è', '–ø–ª–µ–Ω–Ω—ã–π', '–æ—Ä—É–¥–∏–µ', '–∏', '–ø—Ä–æ—á–∏–π', '–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–π', '—á–∞—Å—Ç—å', '–Ω–∞', '–ø–µ—Ä–µ–≤–∞–ª', '—É–∂–æ–∫', '–º—ã', '—Ä–∞–∑–±–∏—Ç—å', '–Ω–µ–ø—Ä–∏—è—Ç–µ–ª—å—Å–∫–∏–π', '–æ—Ç—Ä—è–¥', '–≤–∑—è—Ç—å', '–æ–Ω', '–∞—Ä—Ç–∏–ª–ª–µ—Ä–∏—è', '–∏', '–º–Ω–æ–≥–æ', '–ø–ª–µ–Ω–Ω—ã–π', '–∏', '–ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å', '–ø—Ä–µ—Å–ª–µ–¥–æ–≤–∞—Ç—å', '–≤—Å—Ç—É–ø–∏—Ç—å', '–≤', '–ø—Ä–µ–¥–µ–ª', '–≤–µ–Ω–≥—Ä–∏—è', '—Ä—É—Å—Å–∫–∏–π', '–∏–Ω–≤–∞–ª–∏–¥', '—Å–µ–Ω—Ç—è–±—Ä—å', '–≥–æ–¥']



```python
mystopwords = stopwords.words('russian') 

def remove_stopwords(lemmas, stopwords = mystopwords):
    return [w for w in lemmas if not w in stopwords]
```


```python
lemmas = lemmatize_text(tokens)

print(*remove_stopwords(lemmas))
```

    –±–æ–π —Å–æ–ø–æ—Ü–∫–∏–Ω–∞ –¥—Ä—É—Å–∫–µ–Ω–∏–∫ –∑–∞–∫–æ–Ω—á–∏—Ç—å—Å—è –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ –≥–µ—Ä–º–∞–Ω–µ—Ü –Ω–µ–ø—Ä–∏—è—Ç–µ–ª—å –ø—Ä–∏–±–ª–∏–∑–∏—Ç—å—Å—è —Å–µ–≤–µ—Ä –æ—Å–æ–≤—Ü–∞ –Ω–∞—á–∞—Ç—å –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏–π—Å–∫–∏–π –±–æ—Ä—å–±–∞ –∫—Ä–µ–ø–æ—Å—Ç—å –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏–π—Å–∫–∏–π –±–æ–π –ø—Ä–∏–Ω–∏–º–∞—Ç—å —É—á–∞—Å—Ç–∏–µ —Ç—è–∂—ë–ª—ã–π –∫–∞–ª–∏–±—Ä —Ä–∞–Ω–Ω–∏–π —É—Ç—Ä–æ —Å–µ–Ω—Ç—è–±—Ä—å –æ–≥–æ–Ω—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ –ø–æ–ø—ã—Ç–∫–∞ –≥–µ—Ä–º–∞–Ω—Å–∫–∏–π –ø–µ—Ö–æ—Ç–∞ –ø—Ä–æ–±–∏—Ç—å—Å—è –±–ª–∏–∑–∫–∏–π –∫—Ä–µ–ø–æ—Å—Ç—å –æ—Ç—Ä–∞–∑–∏—Ç—å –≥–∞–ª–∏—Ü–∏—è –∑–∞–Ω—è—Ç—å –¥–µ–º–±–∏—Ü–∞ –±–æ–ª—å—à–∏–π –∫–æ–ª–æ–Ω–Ω–∞ –æ—Ç—Å—Ç—É–ø–∞—Ç—å —à–æ—Å—Å–µ –ø–µ—Ä–µ–º—ã—à–ª—å —Å–∞–Ω–æ–∫ –æ–±—Å—Ç—Ä–µ–ª–∏–≤–∞—Ç—å—Å—è –≤—ã—Å–æ—Ç–∞ –Ω–∞—à –±–∞—Ç–∞—Ä–µ—è –±–µ–∂–∞—Ç—å –±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–∫–∞ –æ–±–æ–∑ –∞–≤—Ç–æ–º–æ–±–∏–ª—å –≤—ã–ª–∞–∑–∫–∞ –≥–∞—Ä–Ω–∏–∑–æ–Ω –ø–µ—Ä–µ–º—ã—à–ª—å –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –±–µ–∑—É—Å–ø–µ—à–Ω—ã–π –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å—Å—è –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ –∞–≤—Å—Ç—Ä–∏–µ—Ü –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å—Å—è –ø–æ–ª–Ω—ã–π –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ —á–∞—Å—Ç—å –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å—Å—è –Ω–æ–≤—ã–π –ø–∞—Ä—Ç–∏—è –ø–ª–µ–Ω–Ω—ã–π –æ—Ä—É–¥–∏–µ –ø—Ä–æ—á–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–π —á–∞—Å—Ç—å –ø–µ—Ä–µ–≤–∞–ª —É–∂–æ–∫ —Ä–∞–∑–±–∏—Ç—å –Ω–µ–ø—Ä–∏—è—Ç–µ–ª—å—Å–∫–∏–π –æ—Ç—Ä—è–¥ –≤–∑—è—Ç—å –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏—è –ø–ª–µ–Ω–Ω—ã–π –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ø—Ä–µ—Å–ª–µ–¥–æ–≤–∞—Ç—å –≤—Å—Ç—É–ø–∏—Ç—å –ø—Ä–µ–¥–µ–ª –≤–µ–Ω–≥—Ä–∏—è —Ä—É—Å—Å–∫–∏–π –∏–Ω–≤–∞–ª–∏–¥ —Å–µ–Ω—Ç—è–±—Ä—å –≥–æ–¥



```python
def remove_stopwords(lemmas, stopwords = mystopwords):
    return [w for w in lemmas if not w in stopwords and len(w) > 3]
```


```python
print(*remove_stopwords(lemmas))
```

    —Å–æ–ø–æ—Ü–∫–∏–Ω–∞ –¥—Ä—É—Å–∫–µ–Ω–∏–∫ –∑–∞–∫–æ–Ω—á–∏—Ç—å—Å—è –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ –≥–µ—Ä–º–∞–Ω–µ—Ü –Ω–µ–ø—Ä–∏—è—Ç–µ–ª—å –ø—Ä–∏–±–ª–∏–∑–∏—Ç—å—Å—è —Å–µ–≤–µ—Ä –æ—Å–æ–≤—Ü–∞ –Ω–∞—á–∞—Ç—å –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏–π—Å–∫–∏–π –±–æ—Ä—å–±–∞ –∫—Ä–µ–ø–æ—Å—Ç—å –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏–π—Å–∫–∏–π –ø—Ä–∏–Ω–∏–º–∞—Ç—å —É—á–∞—Å—Ç–∏–µ —Ç—è–∂—ë–ª—ã–π –∫–∞–ª–∏–±—Ä —Ä–∞–Ω–Ω–∏–π —É—Ç—Ä–æ —Å–µ–Ω—Ç—è–±—Ä—å –æ–≥–æ–Ω—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ –ø–æ–ø—ã—Ç–∫–∞ –≥–µ—Ä–º–∞–Ω—Å–∫–∏–π –ø–µ—Ö–æ—Ç–∞ –ø—Ä–æ–±–∏—Ç—å—Å—è –±–ª–∏–∑–∫–∏–π –∫—Ä–µ–ø–æ—Å—Ç—å –æ—Ç—Ä–∞–∑–∏—Ç—å –≥–∞–ª–∏—Ü–∏—è –∑–∞–Ω—è—Ç—å –¥–µ–º–±–∏—Ü–∞ –±–æ–ª—å—à–∏–π –∫–æ–ª–æ–Ω–Ω–∞ –æ—Ç—Å—Ç—É–ø–∞—Ç—å —à–æ—Å—Å–µ –ø–µ—Ä–µ–º—ã—à–ª—å —Å–∞–Ω–æ–∫ –æ–±—Å—Ç—Ä–µ–ª–∏–≤–∞—Ç—å—Å—è –≤—ã—Å–æ—Ç–∞ –±–∞—Ç–∞—Ä–µ—è –±–µ–∂–∞—Ç—å –±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–∫–∞ –æ–±–æ–∑ –∞–≤—Ç–æ–º–æ–±–∏–ª—å –≤—ã–ª–∞–∑–∫–∞ –≥–∞—Ä–Ω–∏–∑–æ–Ω –ø–µ—Ä–µ–º—ã—à–ª—å –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –±–µ–∑—É—Å–ø–µ—à–Ω—ã–π –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å—Å—è –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ –∞–≤—Å—Ç—Ä–∏–µ—Ü –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å—Å—è –ø–æ–ª–Ω—ã–π –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ —á–∞—Å—Ç—å –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å—Å—è –Ω–æ–≤—ã–π –ø–∞—Ä—Ç–∏—è –ø–ª–µ–Ω–Ω—ã–π –æ—Ä—É–¥–∏–µ –ø—Ä–æ—á–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–π —á–∞—Å—Ç—å –ø–µ—Ä–µ–≤–∞–ª —É–∂–æ–∫ —Ä–∞–∑–±–∏—Ç—å –Ω–µ–ø—Ä–∏—è—Ç–µ–ª—å—Å–∫–∏–π –æ—Ç—Ä—è–¥ –≤–∑—è—Ç—å –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏—è –ø–ª–µ–Ω–Ω—ã–π –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ø—Ä–µ—Å–ª–µ–¥–æ–≤–∞—Ç—å –≤—Å—Ç—É–ø–∏—Ç—å –ø—Ä–µ–¥–µ–ª –≤–µ–Ω–≥—Ä–∏—è —Ä—É—Å—Å–∫–∏–π –∏–Ω–≤–∞–ª–∏–¥ —Å–µ–Ω—Ç—è–±—Ä—å


Joining everything to one function:


```python
def clean_text(text):
    tokens = words_only(text)
    lemmas = lemmatize_text(tokens)
    
    return remove_stopwords(lemmas)
```


```python
print(*clean_text(data.text[3]))
```

    —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ –∫–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç daily mirror —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª—É—á–∞–π –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Ä–∞–¥–æ–≤–∞—Ç—å –≤–µ—Å—å –¥—Ä—É–≥ –∂–∏–≤–æ—Ç–Ω–æ–µ –ª–µ–π—Ç–µ–Ω–∞–Ω—Ç –±–µ–ª—å–≥–∏–π—Å–∫–∏–π –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏—è —Ä—É–∫–æ–≤–æ–¥–∏—Ç—å –æ–±–æ—Ä–æ–Ω–∞ —Ñ–æ—Ä—Ç–æ–≤ –ª—å–µ–∂ —Ö–æ—Ç–µ—Ç—å —Ä–∞—Å—Å—Ç–∞—Ç—å—Å—è —Å–≤–æ–π —Å–æ–±–∞–∫–∞ –±—É–ª—å–¥–æ–≥ –ø—Ä—É—Å—Å–∞–∫ –ø—Ä–æ–±—Ä–∞—Ç—å—Å—è —Ñ–æ—Ä—Ç–æ–≤ —Å–∞–º—ã–π –≥–æ—Ä–æ–¥ –æ—Ñ–∏—Ü–µ—Ä –ø—Ä–∏–π—Ç–∏ –≥–æ–ª–æ–≤–∞ –¥–æ–≤–µ—Ä–∏—Ç—å –±—É–ª—å–¥–æ–≥ –ø–∏—Å—å–º–æ –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Å—ã–ª–∞—Ç—å —É—Å–ø–æ–∫–æ–∏—Ç–µ–ª—å–Ω—ã–π –≤–µ–¥–∞—Ç—å —Å–≤–æ–π —Ä–æ–¥–∏—Ç–µ–ª—å –±–ª–∞–≥–æ—Ä–æ–¥–Ω—ã–π —á–µ—Å—Ç–Ω–æ –∏—Å–ø–æ–ª–Ω–∏—Ç—å —Å–≤–æ–π –º–∏—Å—Å–∏—è –¥–µ—Å—è—Ç—å —Å–ø—É—Å—Ç—è –±—É–ª—å–¥–æ–≥ –ø—Ä–æ–Ω–∏–∫–Ω—É—Ç—å –æ–±—Ä–∞—Ç–Ω–æ —Ñ–æ—Ä—Ç –ø—Ä–∏–Ω–µ—Å—Ç–∏ –æ—Ç–≤–µ—Ç –º–æ–º–µ–Ω—Ç –±—É–ª—å–¥–æ–≥ —Å—Ç–∞—Ç—å –Ω–∞—Å—Ç–æ—è—â–∏–π –≥–æ–Ω–µ—Ü –ø—Ä–æ–±–∏—Ä–∞—Ç—å—Å—è –ª–∏–Ω–∏—è –≥–µ—Ä–º–∞–Ω—Å–∫–∏–π –≤–æ–π—Å–∫–æ –Ω–µ—Å—Ç–∏ —Å–ø—Ä—è—Ç–∞—Ç—å –æ—à–µ–π–Ω–∏–∫ —à–∏—Ñ—Ä–æ–≤–∞—Ç—å –¥–µ–ø–µ—à–∞ –∂—É—Ä–Ω–∞–ª –Ω–∏–≤–∞ —Å–µ–Ω—Ç—è–±—Ä—å


If you need to preprocess large texts, you may also use `Pool` method of the `multiprocessing` library:


```python
from multiprocessing import Pool
from tqdm.notebook import tqdm

N = 200

with Pool(8) as p:
    lemmas = list(tqdm(p.imap(clean_text, data['text'][:N]), total=N))
```


      0%|          | 0/200 [00:00<?, ?it/s]



```python
data = data.head(200)
data['lemmas'] = lemmas
data.sample(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>lemmas</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>146</th>
      <td>–í —á–µ—Ç–≤–µ—Ä–≥ –≤–µ—á–µ—Ä–æ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–∏ –ì–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–π –ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä—ã –†–æ—Å—Å–∏–∏ –ø—Ä–∏—Å—Ç—É–ø–∏–ª–∏ –∫ –æ–±—ã—Å–∫–∞–º –Ω–∞ –¥–∞—á–µ –∏ –¥–≤—É—Ö –º–æ—Å–∫–æ–≤—Å–∫–∏—Ö –∫–≤–∞—Ä—Ç–∏—Ä–∞—Ö –Æ—Ä–∏—è –°–∫—É—Ä–∞—Ç–æ–≤–∞, –æ—Ç—Å—Ç—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –æ—Ç –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –≥–µ–Ω–ø—Ä–æ–∫—É—Ä–æ—Ä–∞. –ö–∞–∫ –ø–µ—Ä–µ–¥–∞–µ—Ç "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å", —Å—Å—ã–ª–∞—è—Å—å –Ω–∞ –¥–æ—á—å  –Æ—Ä–∏—è –°–∫—É—Ä–∞—Ç–æ–≤–∞, –ø–æ  –¥–∞–Ω–Ω—ã–º  –Ω–∞ 20 —á–∞—Å–æ–≤ 40 –º–∏–Ω—É—Ç –ø–æ –º–æ—Å–∫–æ–≤—Å–∫–æ–º—É –≤—Ä–µ–º–µ–Ω–∏  –≥—Ä—É–ø–ø–∞ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤  –ì–µ–Ω–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä—ã –∏–∑  —à–µ—Å—Ç–∏  —á–µ–ª–æ–≤–µ–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏–ª–∞ –æ–±—ã—Å–∫ –Ω–∞ –¥–∞—á–µ –≥–µ–Ω–ø—Ä–æ–∫—É—Ä–æ—Ä–∞   –≤   –ø–æ–¥–º–æ—Å–∫–æ–≤–Ω–æ–º –ê—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫–æ–º. –ü–æ —Å–ª–æ–≤–∞–º –¥–æ—á–µ—Ä–∏ –°–∫—É—Ä–∞—Ç–æ–≤–∞, –¥–Ω–µ–º –≤ —ç—Ç–æ—Ç –∂–µ –¥–µ–Ω—å –±—ã–ª–∏ –ø—Ä–æ–≤–µ–¥–µ–Ω—ã –æ–±—ã—Å–∫–∏ –≤  –¥–≤—É—Ö –º–æ—Å–∫–æ–≤—Å–∫–∏—Ö  –∫–≤–∞—Ä—Ç–∏—Ä–∞—Ö, –≤  –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–æ–ø–∏—Å–∞–Ω–∞  —Å–µ–º—å—è –Æ—Ä–∏—è –°–∫—É—Ä–∞—Ç–æ–≤–∞ –∏ —Ä–æ–¥–∏—Ç–µ–ª–∏ –µ–≥–æ –∂–µ–Ω—ã. –í –ì–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–π  –ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–µ –†–§  "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å—É" –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∏, —á—Ç–æ –æ–±—ã—Å–∫–∏ –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è  —Å —Å–∞–Ω–∫—Ü–∏–∏  –ø—Ä–æ–∫—É—Ä–æ—Ä–∞  –≤  —Ä–∞–º–∫–∞—Ö  —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —É–≥–æ–ª–æ–≤–Ω–æ–≥–æ –¥–µ–ª–∞,  –≤–æ–∑–±—É–∂–¥–µ–Ω–Ω–æ–≥–æ 2  –∞–ø—Ä–µ–ª—è –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –°–∫—É—Ä–∞—Ç–æ–≤–∞ –ø–æ —Å—Ç–∞—Ç—å–µ "–ó–ª–æ—É–ø–æ—Ç—Ä...</td>
      <td>[—á–µ—Ç–≤–µ—Ä–≥, –≤–µ—á–µ—Ä–æ–º, –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å, –≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π, –ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞, —Ä–æ—Å—Å–∏—è, –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å, –æ–±—ã—Å–∫, –¥–∞—á–∞, –º–æ—Å–∫–æ–≤—Å–∫–∏–π, –∫–≤–∞—Ä—Ç–∏—Ä–∞, —é—Ä–∏–π, —Å–∫—É—Ä–∞—Ç–æ–≤, –æ—Ç—Å—Ç—Ä–∞–Ω–∏—Ç—å, –¥–æ–ª–∂–Ω–æ—Å—Ç—å, –≥–µ–Ω–ø—Ä–æ–∫—É—Ä–æ—Ä, –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å, –∏–Ω—Ç–µ—Ä—Ñ–∞–∫—Å, —Å—Å—ã–ª–∞—Ç—å—Å—è, –¥–æ—á—å, —é—Ä–∏–π, —Å–∫—É—Ä–∞—Ç–æ–≤, –¥–∞–Ω–Ω—ã–µ, –º–∏–Ω—É—Ç–∞, –º–æ—Å–∫–æ–≤—Å–∫–∏–π, –≤—Ä–µ–º—è, –≥—Ä—É–ø–ø–∞, —Å–æ—Ç—Ä—É–¥–Ω–∏–∫, –≥–µ–Ω–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞, —à–µ—Å—Ç—å, —á–µ–ª–æ–≤–µ–∫, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å, –æ–±—ã—Å–∫, –¥–∞—á–∞, –≥–µ–Ω–ø—Ä–æ–∫—É—Ä–æ—Ä, –ø–æ–¥–º–æ—Å–∫–æ–≤–Ω—ã–π, –∞—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫, —Å–ª–æ–≤–æ, –¥–æ—á—å, —Å–∫—É—Ä–∞—Ç–æ–≤, –¥–µ–Ω—å, –¥–µ–Ω—å, –ø—Ä–æ–≤–µ—Å—Ç–∏, –æ–±—ã—Å–∫, –º–æ—Å–∫–æ–≤—Å–∫–∏–π, –∫–≤–∞—Ä—Ç–∏—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π, –ø—Ä–æ–ø–∏—Å–∞—Ç—å, —Å–µ–º—å—è, —é—Ä–∏–π, —Å–∫—É—Ä–∞—Ç–æ–≤, —Ä–æ–¥–∏—Ç–µ–ª—å, –∂–µ–Ω–∞, –≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π, –ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞, –∏–Ω—Ç–µ—Ä—Ñ–∞–∫—Å, –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å, –æ–±—ã—Å–∫, –ø—Ä–æ–≤–æ–¥–∏—Ç—å—Å—è, —Å–∞–Ω–∫—Ü–∏—è, –ø—Ä–æ–∫—É—Ä–æ—Ä, —Ä–∞–º–∫–∞, —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, —É–≥–æ–ª–æ–≤–Ω—ã–π, –¥–µ–ª–æ, –≤–æ–∑–±—É–¥–∏—Ç—å, –∞–ø—Ä–µ–ª—å, –æ—Ç–Ω–æ—à–µ–Ω–∏–µ, —Å–∫—É—Ä–∞—Ç–æ–≤, —Å—Ç–∞—Ç—å—è, –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ, –¥–æ–ª–∂–Ω–æ—Å—Ç–Ω–æ–π, –ø–æ–ª–Ω–æ–º–æ—á–∏–µ, —Å–∫—É—Ä–∞—Ç–æ–≤, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π, –≤–æ–∑–¥–µ—Ä–∂–∞—Ç—å—Å—è]</td>
    </tr>
    <tr>
      <th>165</th>
      <td>–ù–µ—Ñ—Ç–µ–ø–µ—Ä–µ—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–π –∑–∞–≤–æ–¥ "–Ø—Ä–æ—Å–ª–∞–≤–Ω–µ—Ñ—Ç–µ–æ—Ä–≥—Å–∏–Ω—Ç–µ–∑" —É–≤–µ–ª–∏—á–∏–ª –æ—Ç–ø—É—Å–∫–Ω—ã–µ –æ–ø—Ç–æ–≤—ã–µ —Ü–µ–Ω—ã –Ω–∞ –Ω–∞–∏–±–æ–ª–µ–µ —Ö–æ–¥–æ–≤—ã–µ –º–∞—Ä–∫–∏ –±–µ–Ω–∑–∏–Ω–∞ –≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ 25% –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ —Ü–µ–Ω–∞–º –∞–≤–≥—É—Å—Ç–∞, - –ø–µ—Ä–µ–¥–∞–µ—Ç –†–ò–ê "–ù–æ–≤–æ—Å—Ç–∏" —Å–æ —Å—Å—ã–ª–∫–æ–π –Ω–∞ –æ—Ç–¥–µ–ª —Å–±—ã—Ç–∞ –∑–∞–≤–æ–¥–∞. –¶–µ–Ω–∞ –±–µ–Ω–∑–∏–Ω–∞ –ê–∏-92 –≤–æ–∑—Ä–æ—Å–ª–∞ –Ω–∞ 21% –∏ —Ç–µ–ø–µ—Ä—å —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 5 —Ç—ã—Å—è—á 500 —Ä—É–±–ª–µ–π –∑–∞ —Ç–æ–Ω–Ω—É; –±–µ–Ω–∑–∏–Ω–∞ –ê-76 - –Ω–∞ 33% (5 —Ç—ã—Å—è—á 200 —Ä—É–±–ª–µ–π –∑–∞ —Ç–æ–Ω–Ω—É); –±–µ–Ω–∑–∏–Ω–∞ –ê–∏-95 - –Ω–∞ 18% (6 —Ç—ã—Å—è—á 400 —Ä—É–±–ª–µ–π –∑–∞ —Ç–æ–Ω–Ω—É). –î–∏–∑–µ–ª—å–Ω–æ–µ —Ç–æ–ø–ª–∏–≤–æ –∑–∞–≤–æ–¥ –æ—Ç–ø—É—Å–∫–∞–µ—Ç —Ç–µ–ø–µ—Ä—å –ø–æ —Ü–µ–Ω–µ 3 —Ç—ã—Å—è—á–∏ 600 —Ä—É–±–ª–µ–π –∑–∞ —Ç–æ–Ω–Ω—É, —á—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–¥–æ—Ä–æ–∂–∞–Ω–∏—é –Ω–∞ 23% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–æ—à–ª—ã–º –º–µ—Å—è—Ü–µ–º. –ù–µ—Ñ—Ç–µ–ø–µ—Ä–µ—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–π –∑–∞–≤–æ–¥ "–Ø—Ä–æ—Å–ª–∞–≤–Ω–µ—Ñ—Ç–µ–æ—Ä–≥—Å–∏–Ω—Ç–µ–∑" —è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–∏–º –∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤ –Ω–µ—Ñ—Ç–µ–ø—Ä–æ–¥—É–∫—Ç–æ–≤ –Ω–∞ —Ä—ã–Ω–æ–∫ –ú–æ—Å–∫–≤—ã –∏ –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞ –†–æ—Å—Å–∏–∏ –≤ —Ü–µ–ª–æ–º.</td>
      <td>[–Ω–µ—Ñ—Ç–µ–ø–µ—Ä–µ—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–π, –∑–∞–≤–æ–¥, —è—Ä–æ—Å–ª–∞–≤–Ω–µ—Ñ—Ç–µ–æ—Ä–≥—Å–∏–Ω—Ç–µ–∑, —É–≤–µ–ª–∏—á–∏—Ç—å, –æ—Ç–ø—É—Å–∫–Ω–æ–π, –æ–ø—Ç–æ–≤—ã–π, —Ü–µ–Ω–∞, –Ω–∞–∏–±–æ–ª–µ–µ, —Ö–æ–¥–æ–≤–æ–π, –º–∞—Ä–∫–∞, –±–µ–Ω–∑–∏–Ω, —Å—Ä–µ–¥–Ω–µ–µ, –æ—Ç–Ω–æ—à–µ–Ω–∏–µ, —Ü–µ–Ω–∞, –∞–≤–≥—É—Å—Ç, –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å, –Ω–æ–≤–æ—Å—Ç—å, —Å—Å—ã–ª–∫–∞, –æ—Ç–¥–µ–ª, —Å–±—ã—Ç, –∑–∞–≤–æ–¥, —Ü–µ–Ω–∞, –±–µ–Ω–∑–∏–Ω, –≤–æ–∑—Ä–∞—Å—Ç–∏, —Å–æ—Å—Ç–∞–≤–ª—è—Ç—å, —Ç—ã—Å—è—á–∞, —Ä—É–±–ª—å, —Ç–æ–Ω–Ω–∞, –±–µ–Ω–∑–∏–Ω, —Ç—ã—Å—è—á–∞, —Ä—É–±–ª—å, —Ç–æ–Ω–Ω–∞, –±–µ–Ω–∑–∏–Ω, —Ç—ã—Å—è—á–∞, —Ä—É–±–ª—å, —Ç–æ–Ω–Ω–∞, –¥–∏–∑–µ–ª—å–Ω—ã–π, —Ç–æ–ø–ª–∏–≤–æ, –∑–∞–≤–æ–¥, –æ—Ç–ø—É—Å–∫–∞—Ç—å, —Ü–µ–Ω–∞, —Ç—ã—Å—è—á–∞, —Ä—É–±–ª—å, —Ç–æ–Ω–Ω–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å, –ø–æ–¥–æ—Ä–æ–∂–∞–Ω–∏–µ, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ, –ø—Ä–æ—à–ª–æ–µ, –º–µ—Å—è—Ü, –Ω–µ—Ñ—Ç–µ–ø–µ—Ä–µ—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–π, –∑–∞–≤–æ–¥, —è—Ä–æ—Å–ª–∞–≤–Ω–µ—Ñ—Ç–µ–æ—Ä–≥—Å–∏–Ω—Ç–µ–∑, —è–≤–ª—è—Ç—å—Å—è, –æ—Å–Ω–æ–≤–Ω—ã–π, –ø–æ—Å—Ç–∞–≤—â–∏–∫, –Ω–µ—Ñ—Ç–µ–ø—Ä–æ–¥—É–∫—Ç, —Ä—ã–Ω–æ–∫, –º–æ—Å–∫–≤–∞, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π, —Ä–µ–≥–∏–æ–Ω, —Ä–æ—Å—Å–∏—è, —Ü–µ–ª–æ–µ]</td>
    </tr>
    <tr>
      <th>105</th>
      <td>–°–µ–≥–æ–¥–Ω—è –¥–Ω–µ–º –≤ —Å—Ç–æ–ª–∏—Ü–µ –ì—Ä–µ—Ü–∏–∏ –ø—Ä–æ–∏–∑–æ—à–ª–æ –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–µ —Å–∏–ª–æ–π 5,9 –±–∞–ª–ª–∞ –ø–æ —à–∫–∞–ª–µ –†–∏—Ö—Ç–µ—Ä–∞. –°–æ–≥–ª–∞—Å–Ω–æ –¥–∞–Ω–Ω—ã–º –≥—Ä–µ—á–µ—Å–∫–∏—Ö —Å–µ–π—Å–º–æ–ª–æ–≥–æ–≤, —ç–ø–∏—Ü–µ–Ω—Ç—Ä –Ω–∞—Ö–æ–¥–∏–ª—Å—è –≤ 20 –∫–∏–ª–æ–º–µ—Ç—Ä–∞—Ö –∫ —Å–µ–≤–µ—Ä–æ-–∑–∞–ø–∞–¥—É –æ—Ç –ê—Ñ–∏–Ω. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–æ–¥–∑–µ–º–Ω—ã—Ö —Ç–æ–ª—á–∫–æ–≤ –≤–∏–±—Ä–∏—Ä–æ–≤–∞–ª–∏ –∑–¥–∞–Ω–∏—è, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –Ω–∏—Ö –ø–æ–∫—Ä—ã–ª–∏—Å—å —Ç—Ä–µ—â–∏–Ω–∞–º–∏, –≤–æ –º–Ω–æ–≥–∏—Ö –º–µ—Å—Ç–∞—Ö –æ—Å—ã–ø–∞–ª–∞—Å—å —à—Ç—É–∫–∞—Ç—É—Ä–∫–∞. –ü–æ –¥–∞–Ω–Ω—ã–º –ø–æ–ª–∏—Ü–∏–∏, –≤ —Å–µ–≤–µ—Ä–Ω–æ–π —á–∞—Å—Ç–∏ –ê—Ñ–∏–Ω —Ä—É—Ö–Ω—É–ª –æ–¥–∏–Ω –¥–æ–º. –ü–æ —Ä–∞–¥–∏–æ –±—ã–ª–æ –ø–µ—Ä–µ–¥–∞–Ω–æ –æ–±—Ä–∞—â–µ–Ω–∏–µ –º–µ—Å—Ç–Ω—ã—Ö –≤–ª–∞—Å—Ç–µ–π –∫ —Å—Ç–æ–ª–∏—á–Ω—ã–º –∂–∏—Ç–µ–ª—è–º —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –ø–æ–∫–∏–Ω—É—Ç—å –∂–∏–ª—ã–µ –ø–æ–º–µ—â–µ–Ω–∏—è –≤ —Å–≤—è–∑–∏ —Å –ø—Ä–æ–¥–æ–ª–∂–∞—é—â–µ–π—Å—è —Å–µ–π—Å–º–∏—á–µ—Å–∫–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é. –õ—é–¥–∏ —Å–∏–ª—å–Ω–æ –æ–∑–∞–±–æ—á–µ–Ω—ã –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–∏–º, –ø—ã—Ç–∞—é—Ç—Å—è –¥–æ–∑–≤–æ–Ω–∏—Ç—å—Å—è –¥–æ —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤ –∏–ª–∏ –¥–æ–±—Ä–∞—Ç—å—Å—è –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ —Ç—É–¥–∞, –≥–¥–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –∏—Ö –¥–µ—Ç–∏ - –≤ —à–∫–æ–ª—ã, –¥–µ—Ç—Å–∫–∏–µ —Å–∞–¥—ã. –õ—é–¥–∏ –∫—Ä–∞–π–Ω–µ –≤–∑–≤–æ–ª–Ω–æ–≤–∞–Ω—ã, –≤—Å–µ –≤—Å–ø–æ–º–∏–Ω–∞—é—Ç –æ –Ω–µ–¥–∞–≤–Ω–µ–º —Ä–∞–∑—Ä—É—à–∏—Ç–µ–ª—å–Ω–æ–º –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–∏ –≤ —Å–æ—Å–µ–¥–Ω–µ–π –¢—É—Ä—Ü–∏–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ...</td>
      <td>[—Å–µ–≥–æ–¥–Ω—è, –¥–µ–Ω—å, —Å—Ç–æ–ª–∏—Ü–∞, –≥—Ä–µ—Ü–∏—è, –ø—Ä–æ–∏–∑–æ–π—Ç–∏, –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–µ, —Å–∏–ª–∞, –±–∞–ª–ª, —à–∫–∞–ª–∞, —Ä–∏—Ö—Ç–µ—Ä, —Å–æ–≥–ª–∞—Å–Ω–æ, –¥–∞–Ω–Ω—ã–µ, –≥—Ä–µ—á–µ—Å–∫–∏–π, —Å–µ–π—Å–º–æ–ª–æ–≥, —ç–ø–∏—Ü–µ–Ω—Ç—Ä, –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è, –∫–∏–ª–æ–º–µ—Ç—Ä, —Å–µ–≤–µ—Ä–æ, –∑–∞–ø–∞–¥, –∞—Ñ–∏–Ω—ã, —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –ø–æ–¥–∑–µ–º–Ω—ã–π, —Ç–æ–ª—á–æ–∫, –≤–∏–±—Ä–∏—Ä–æ–≤–∞—Ç—å, –∑–¥–∞–Ω–∏–µ, –Ω–µ–∫–æ—Ç–æ—Ä—ã–π, –ø–æ–∫—Ä—ã—Ç—å—Å—è, —Ç—Ä–µ—â–∏–Ω–∞, –º–Ω–æ–≥–∏–π, –º–µ—Å—Ç–æ, –æ—Å—ã–ø–∞—Ç—å—Å—è, —à—Ç—É–∫–∞—Ç—É—Ä–∫–∞, –¥–∞–Ω–Ω—ã–µ, –ø–æ–ª–∏—Ü–∏—è, —Å–µ–≤–µ—Ä–Ω—ã–π, —á–∞—Å—Ç—å, –∞—Ñ–∏–Ω—ã, —Ä—É—Ö–Ω—É—Ç—å, —Ä–∞–¥–∏–æ, –ø–µ—Ä–µ–¥–∞—Ç—å, –æ–±—Ä–∞—â–µ–Ω–∏–µ, –º–µ—Å—Ç–Ω—ã–π, –≤–ª–∞—Å—Ç—å, —Å—Ç–æ–ª–∏—á–Ω—ã–π, –∂–∏—Ç–µ–ª—å, —É–∫–∞–∑–∞–Ω–∏–µ, –ø–æ–∫–∏–Ω—É—Ç—å, –∂–∏–ª–æ–π, –ø–æ–º–µ—â–µ–Ω–∏–µ, —Å–≤—è–∑—å, –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å—Å—è, —Å–µ–π—Å–º–∏—á–µ—Å–∫–∏–π, –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, —á–µ–ª–æ–≤–µ–∫, —Å–∏–ª—å–Ω–æ, –æ–∑–∞–±–æ—Ç–∏—Ç—å, –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å, –ø—ã—Ç–∞—Ç—å—Å—è, –¥–æ–∑–≤–æ–Ω–∏—Ç—å—Å—è, —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫, –¥–æ–±—Ä–∞—Ç—å—Å—è, –∞–≤—Ç–æ–º–æ–±–∏–ª—å, —Ç—É–¥–∞, –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è, —Ä–µ–±—ë–Ω–æ–∫, —à–∫–æ–ª–∞, –¥–µ—Ç—Å–∫–∏–π, —á–µ–ª–æ–≤–µ–∫, –∫—Ä–∞–π–Ω–µ, –≤–∑–≤–æ–ª–Ω–æ–≤–∞—Ç—å, –≤—Å–ø–æ–º–∏–Ω–∞—Ç—å, –Ω–µ–¥–∞–≤–Ω–∏–π, —Ä–∞–∑—Ä—É—à–∏—Ç–µ–ª—å–Ω—ã–π, –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–µ, —Å–æ—Å–µ–¥–Ω–∏–π, —Ç—É—Ä—Ü–∏—è, —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –ø–æ–¥–∑–µ–º–Ω—ã–π, —Ç–æ–ª—á–æ–∫, –∞—Ñ–∏–Ω—ã, –≤—ã–π—Ç–∏, —Å—Ç—Ä–æ–π, —Å—Ç...</td>
    </tr>
  </tbody>
</table>
</div>



## Now it is your time to analyse text in English


```python
from sklearn.datasets import fetch_20newsgroups
```


```python
def twenty_newsgroup_to_csv():
    newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))

    df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T
    df.columns = ['text', 'target']

    targets = pd.DataFrame( newsgroups_train.target_names)
    targets.columns=['title']

    out = pd.merge(df, targets, left_on='target', right_index=True)
    out['date'] = pd.to_datetime('now')
    return out
```


```python
df = twenty_newsgroup_to_csv()
df.head(5)
```


```python
# your code here
```

### Result:

- learned about standard text preprocessing pipeline
- learned how to work with morphological parsers


```python

```
