{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ypt7ymI8X-rM"
      },
      "source": [
        "# Embeddings\n",
        "\n",
        "## Word2Vec\n",
        "\n",
        "Vector models that we considered before (tf-idf, BOW) are conventionally called *countable* ones. They are based on \"counting\" words and their neighbors, and on build word vectors based on this information.\n",
        "\n",
        "Another class of models that is more ubiquitous today is called *predictive* (or neural) models. The idea behind these models is to use neural network architectures that \"predict\" (rather than count) the neighbors of words. One of the most famous model of this type is **word2vec**. It is based on a neural network that predicts the probability to meet a word in a given context. This tool was developed by a group of Google researchers in 2013, led by Tomas Mikolov (now at Facebook). Here are the two most important articles:\n",
        "\n",
        "* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
        "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
        "\n",
        "The vectors obtained using such models are called *distributed representations of words*, or **embeddings**.\n",
        "\n",
        "### How to train ?\n",
        "For each word we define the vector using the $w$ matrix and the context vector using the $W$ matrix. In fact, word2vec is a generic name for two \n",
        "architectures: Skip-Gram and Continuous Bag-Of-Words (CBOW).\n",
        "\n",
        "**CBOW** predicts the current word based on context around it.\n",
        "\n",
        "**Skip-gram** predicts the context, given the current word vice versa.\n",
        "\n",
        "### How it works ?\n",
        "Word2vec takes a large corpus of text as input and maps each word to a vector, giving the coordinates of the words as output. First, it creates a dictionary from the input text data, and then calculates the vector representation of the words. Vector representation is based on contextual similarity: words that appear in the same context (and therefore, according to the distributive hypothesis, have a similar meaning) will have close vectors after the model training. To calculate the similarity of words, the cosine distance between their vectors is used.\n",
        "\n",
        "You can build semantic analogies and compute simple vector addition tasks using distributive vector models, for example:\n",
        "\n",
        "* *king: male = queen: female*\n",
        "$\\Rightarrow$\n",
        "* *king - man + woman = queen*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3sSyfHaX-rN"
      },
      "source": [
        "![w2v](https://cdn-images-1.medium.com/max/2600/1*sXNXYfAqfLUeiDXPCo130w.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oH_UDyFX-rO"
      },
      "source": [
        "### Problems\n",
        "\n",
        "Word2Vec models can not establish the type of semantic relationship between words: synonyms, antonyms, hyponyms and hyperonyms will be equally close to the target word because they are usually used in similar contexts. Therefore, words that are close in the vector space are called *semantic associates*. This means that they are semantically related, but it is not clear how exactly.\n",
        "\n",
        "It is impossible to establish the type of semantic relationship between words: synonyms, antonyms, etc. will be equally close because they are usually used in similar contexts. Therefore, words that are close in the vector space are called *semantic associates*. This means that they are semantically related, but it is not clear how exactly.\n",
        "\n",
        "### RusVectōrēs\n",
        "\n",
        "You can find different pre-trained models for the Russian language at the site [RusVectōrēs](https://rusvectores.org/ru/). Models are trained on various data, starting from Wikipedia and ending News data and Social Media. RusVectōrēs also provides the interface to search for the closest words to a given one, calculate the semantic similarity of several words and solve vector addition tasks using the \"semantic similarity calculator\".\n",
        "\n",
        "You can also find pre-trained models for other languages, for example, models [fastText](https://fasttext.cc/docs/en/english-vectors.html) and [GloVe](https://nlp.stanford.edu/projects/glove/) (more on them later).\n",
        "\n",
        "### Visualization\n",
        "There is a good visualization for English [here](https://projector.tensorflow.org/) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXNm8flVX-rP"
      },
      "source": [
        "## Gensim\n",
        "\n",
        "You can use the pretrained embedding model or train your own using the `gensim` library. Here is [gensim documentation](https://radimrehurek.com/gensim/models/word2vec.html).\n",
        "\n",
        "### How to use the pretrained model\n",
        "Word2vec models come in different formats:\n",
        "\n",
        "* .vec.gz — vector file\n",
        "* .bin.gz — binary file\n",
        "\n",
        "They can be loaded using the same class `KeyedVectors`, you only need to change the` binary` flag of the `load_word2vec_format` function.\n",
        "\n",
        "If the model is **not** trained using word2vec, then the `load` function must be used to load. It may be useful to load pretrained embeddings from *glove, fasttext, bpe* and other modles.\n",
        "\n",
        "Let us download the model for the Russian language, trained in the National Corpus of Russian Language (`НКРЯ`/`NCRL`) dated 2015."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTLnNKaNX-rP",
        "outputId": "d24a7585-5007-4576-ba0e-c66a9a1e7949",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import gensim\n",
        "import logging\n",
        "import nltk.data \n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import word2vec\n",
        "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm9z6SN3X-rS",
        "outputId": "05572785-7f13-43b0-b6c4-636db0ecacbe",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-03 15:06:01--  https://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\n",
            "Resolving rusvectores.org (rusvectores.org)... 172.104.228.108\n",
            "Connecting to rusvectores.org (rusvectores.org)|172.104.228.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2023-03-03 15:06:02 ERROR 403: Forbidden.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "mkuCawyFX-rW",
        "outputId": "08d4935d-73e8-4e66-c97e-a4067c2d2094",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f3e214ad7530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ruscorpora_mystem_cbow_300_2_2015.bin.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_ru\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \"\"\"\n\u001b[1;32m   1435\u001b[0m         \u001b[0;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m             limit=limit, datatype=datatype)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mso_compression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFER_FROM_EXTENSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_binary_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0mdecompressed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mso_compression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_open_binary_stream\u001b[0;34m(uri, mode, transport_params)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mscheme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sniff_scheme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0msubmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/smart_open/local_file.py\u001b[0m in \u001b[0;36mopen_uri\u001b[0;34m(uri_as_string, mode, transport_params)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri_as_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mparsed_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri_as_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uri_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'"
          ]
        }
      ],
      "source": [
        "model_path = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'\n",
        "\n",
        "model_ru = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvvO1ew5p3PP"
      },
      "source": [
        "Let's take several words as an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4D0hMgkX-rY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "words = ['клавиатура_S', 'мышь_S', 'кошка_S']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbTShU9aX-ra"
      },
      "source": [
        "Tags like `_S` specify part of speech (`S` - `существительное`) for the given word. Downloaded model was trained on lemmatized and annotated with parts of speech (POS) words. **NB!** Model names on `rusvectores` indicate which tagset they are using (mystem, upos, etc.)\n",
        "\n",
        "Let's ask the model to find 10 nearest neighbors and the cosine similarity for each word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Uer5sHyX-ra",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "for word in words:\n",
        "    # model contains word? \n",
        "    if word in model_ru:\n",
        "        print(word)\n",
        "        # show resulting vector (it has dimension 300, so let us print first 10 elements)\n",
        "        print(model_ru[word][:10])\n",
        "        # show 10 nearest neighbours :\n",
        "        for word, sim in model_ru.most_similar(positive=[word], topn=10):\n",
        "            # word + cosine similarity coefficient\n",
        "            print(f'{word} : {sim}')\n",
        "        print('\\n')\n",
        "    else:\n",
        "        # model does not contain the word? \n",
        "        print(f'model does not contain the word {word}!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GqDmAcJX-rc"
      },
      "source": [
        "Let us find the cosine similarity for the pair or words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDFjOSJjX-rd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(model_ru.similarity('человек_S', 'обезьяна_S'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0w4pQooX-rf"
      },
      "source": [
        "What happens if we subtract Italy from pizza and add Siberia?\n",
        "\n",
        "* positive - vectors that we add\n",
        "* negative - vectors that we subtract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0L5_TCQX-rf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(model_ru.most_similar(positive=['человек_S'], negative=['обезьяна_S'])[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrN2Jc31X-rh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model_ru.doesnt_match('пицца_S пельмень_S хот-дог_S ананас_S'.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9XsGSE_HuFJ"
      },
      "source": [
        "**Warm Up Exercise**\n",
        "\n",
        "Find homonymic word with different meanings in top 10 neighbours (`most_similar` method):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adiAA7AqIY4g"
      },
      "source": [
        "By analogy with Italy - pizza, Siberia - dumplings, find similar bunch of words to check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UV2yj-xpuO1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "word = 'галера_S'\n",
        "if word in model_ru:\n",
        "        print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5bzD-ZXKOSY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(model_ru.most_similar(positive=['двигатель_S', 'машина_S'], negative=['галера_S'])[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpIDtQIzKO1M"
      },
      "source": [
        "Give an example of three words w1, w2, w3 such that w1 and w2 are synonyms, w1 and w3 are antonyms, but similarity (w1, w2) is less than similarity (w1, w3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFXaxVhlKPb2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "w1 = 'мышь_S',\n",
        "w2 = 'клавиатура_S'\n",
        "w3 = 'кошка_S'\n",
        "print(f\"D(w1,w2) = {model_ru.similarity(w1, w2)}\")\n",
        "print(f\"D(w1,w3) = {model_ru.similarity(w1, w3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rfihwVJKwzC"
      },
      "source": [
        "### Excersise\n",
        "\n",
        "Write a function that takes a sentence as input and replaces a random `Noun` with its \"associate\" (the closest word from the word2vec model).\n",
        "\n",
        "**NB:** you need a morphology analyzer like pymorphy for this (we briefly talked about it at the last seminar)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCMPmoRrM3ES"
      },
      "source": [
        "how to use pymorphy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHf_0AeCKxIs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRWeAurhM7IW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from pymorphy2 import MorphAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYvl-ttDNCJl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "analyser = MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUvP-U-nNF6Y",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# parse a word (in this case, two results are possible, so we get a list of two elements)\n",
        "result = analyser.parse('слово')\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enrQF-SQOtQP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# get POS of the result\n",
        "result[0].tag.POS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8P6VpjDPR_z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# convert result to the the dative case\n",
        "result[0].inflect(frozenset(['datv'])).word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNhjJC-xPwMg"
      },
      "source": [
        "Implement your function here (for simplicity, you may not convert the word to \n",
        "the \"proper\" form and limit yourself to the nominative case):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5slDKGtP23t",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "sentence = 'мама мыла раму'\n",
        "import random\n",
        "\n",
        "def change_random_noun(sentence):\n",
        "  tokens = sentence.split()\n",
        "  print(f'we have {len(tokens)} words in the sentence')\n",
        "  while True:\n",
        "    idx = random.randint(0, len(tokens)-1)\n",
        "    result = analyser.parse(tokens[idx])\n",
        "    print(f'normal form for {idx} word {result[0].word} is {result[0].normal_form}')\n",
        "    if result[0].tag.POS == 'NOUN':\n",
        "      sim_words = model_ru.most_similar(positive=[result[0].normal_form + \"_S\"], topn=1)\n",
        "      for sim_word in sim_words:\n",
        "        if sim_word[0].endswith('_S'):\n",
        "          print(f'the 1st closest similar NOUN to {result[0].normal_form} is {sim_word[0]}')\n",
        "          tokens[idx] = sim_word[0][:-2]\n",
        "          break\n",
        "      break\n",
        "  return ' '.join(tokens)\n",
        "  \n",
        "change_random_noun(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNHMtmbqX-rl"
      },
      "source": [
        "## How to train your ̶d̶r̶a̶g̶o̶n̶ model ?\n",
        "\n",
        "We will use tagged and untagged movie reviews (dataset taken from Kaggle) as training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1Vz9le-X-rl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/unlabeledTrainData.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwR6tLP8NwXF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN3NIKVm2FJH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!ls -la | grep Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2wv-vSxX-ro",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEGmrh-nX-rq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve2bKboVxVVn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIZaQ3kYX-rt"
      },
      "source": [
        "We need to perform some preprocessing: remove links, html markup and non-alphabetic characters from the data, and then bring everything to lowercase and tokenize. The output is an array of sentences, each sentence is an array of words. We use the tokenizer from the `nltk` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAzxyk3WTgrd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YHaoVqk1WZ8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irbunSOfX-rt",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def review_to_wordlist(review, remove_stopwords=False ):\n",
        "    # remove links\n",
        "    review = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n",
        "    # get the text from the page\n",
        "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
        "    # leave only words\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    # convert to lowercase and split into words using space character\n",
        "    words = review_text.lower().split()\n",
        "    if remove_stopwords: # remove stopwords\n",
        "        stops = stopwords.words(\"english\")\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return(words)\n",
        "\n",
        "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
        "    # break the review oto sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    # apply the function to each sentence\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bolPEuQgX-rv",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "sentences = []  \n",
        "\n",
        "print(\"Parsing sentences from training set...\")\n",
        "for review in data[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbrH6TdfX-rx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(len(sentences))\n",
        "print(sentences[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqb_NOteX-rz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# we'll need it later\n",
        "\n",
        "with open('clean_text.txt', 'w') as f:\n",
        "    for s in sentences[:5000]:\n",
        "        f.write(' '.join(s))\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqFhuv7XX-r1"
      },
      "source": [
        "Train and save the model.\n",
        "\n",
        "Main parameters:\n",
        "* datset must be an iterable object\n",
        "* size - output vector size,\n",
        "* window - the size of the observation window,\n",
        "* min_count - min. the frequency of the word in the corpus,\n",
        "* sg - learning algorithm (0 - CBOW, 1 - Skip-gram),\n",
        "* sample - threshold for downsampling high-frequency words,\n",
        "* workers - the number of threads,\n",
        "* alpha - learning rate,\n",
        "* iter - number of iterations,\n",
        "* max_vocab_size - allows you to set a memory limit when creating a dictionary (i.e. if the limit is exceeded, then low-frequency words will be thrown out). For comparison: 10 million words = 1 GB of RAM.\n",
        "\n",
        "**NB!** Please note that model training does not include preprocessing! This means that you will have to get rid of punctuation, bring words to lower case, lemmatize them, and put down part-of-speech tags before training the model (if, of course, this is necessary for your task). Those. in what form the words will be in the source text, in this way they will be in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB2FqIioX-r1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(\"Training model...\")\n",
        "\n",
        "%time model_en = word2vec.Word2Vec(sentences, workers=4, size=300, min_count=10, window=10, sample=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFU6SYAZX-r3"
      },
      "source": [
        "Let's see how many words are in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HA3P0CUYX-r4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(len(model_en.wv.vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mU9sbGYX-r8"
      },
      "source": [
        "Let's try to evaluate the model manually by solving examples. A few are given below, try to come up with your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8OQxmUyX-r9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(model_en.wv.most_similar(positive=[\"woman\", \"actor\"], negative=[\"man\"], topn=1))\n",
        "print(model_en.wv.most_similar(positive=[\"dogs\", \"man\"], negative=[\"dog\"], topn=1))\n",
        "\n",
        "print(model_en.wv.most_similar(\"usa\", topn=3))\n",
        "\n",
        "print(model_en.wv.doesnt_match(\"comedy thriller western novel\".split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVnNLAlF2_vb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(model_en.wv.most_similar(\"pizza\", topn=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juR4NX0wX-r_"
      },
      "source": [
        "### How to fit an existing model to your data\n",
        "\n",
        "When training a model \"from scratch\", weights are initialized randomly, but we  can initialize weights from a pre-trained model, thus, as if we fit it.\n",
        "\n",
        "First, let's see the similarity of a pair of words in the existing model, in order to then compare the result with the retrained one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAUVp96kX-sA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model_en.wv.similarity('lion', 'rabbit')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxsz8DKMX-sD"
      },
      "source": [
        "We take the text \"Alice in Wonderland\" as additional data for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eTCOgpMX-sE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/alice.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHuXJgB_X-sI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "with open(\"alice.txt\", 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# remove end of lines and tokenize text to sents\n",
        "text = re.sub('\\n', ' ', text)\n",
        "sents = sent_tokenize(text)\n",
        "\n",
        "# remove punctuation and tokenize to tokens\n",
        "punct = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~„“«»†*—/\\-‘’'\n",
        "clean_sents = []\n",
        "for sent in sents:\n",
        "    s = [w.lower().strip(punct) for w in sent.split()]\n",
        "    clean_sents.append(s)\n",
        "    \n",
        "print(clean_sents[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH8f7GNBX-sK"
      },
      "source": [
        "We will save and then load the model to get necessary files for fitting. All training parameters (vector size, min. Word frequency, etc.) will be taken from the loaded model, i.e. you cannot set them again.\n",
        "\n",
        "**NB!** You can only fit the full model, not `KeyedVectors` output. Therefore, you need to save the model in the appropriate format. More about the difference [here](https://radimrehurek.com/gensim/models/keyedvectors.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv--44TmX-sL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model_path = \"movie_reviews.model\"\n",
        "\n",
        "print(\"Saving model...\")\n",
        "model_en.save(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6eZwBQzX-sQ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = word2vec.Word2Vec.load(model_path)\n",
        "model.build_vocab(clean_sents, update=True)\n",
        "model.train(clean_sents, total_examples=model.corpus_count, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMLhISWiX-sS"
      },
      "source": [
        "**Lion** and **rabbit** have become closer to each other!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4USaUjmzX-sS",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model.wv.similarity('lion', 'rabbit')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5bYd1qmX-sT"
      },
      "source": [
        "You can normalize vectors, then the model will take up less RAM. However it cannot be trained after that. This uses L2 normalization: the vectors are normalized so that if you add the squares of all the elements of the vector, they add up to 1.\n",
        "\n",
        "In addition, we will save `KeyedVectors`, but not full vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAVaTYvQX-sU",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model.init_sims(replace=True)\n",
        "model_path = \"movies_alice.bin\"\n",
        "\n",
        "print(\"Saving model...\")\n",
        "model_en.wv.save_word2vec_format(model_path, binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXKnPffPX-sY"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "How do you know which model is better? How to find out if the model become better?\n",
        "\n",
        "For this, there are special datasets for assessing the quality of distribution models. There are two main ones: one measures the accuracy of solving problems by analogy (about Russia and dumplings), and the second is used to assess the coefficient of semantic similarity.\n",
        "\n",
        "### Word Similarity\n",
        "\n",
        "Assert if semantic similarity for the given model correlate with the common sense.\n",
        "\n",
        "| word 1 | word 2 | similarity |\n",
        "| --- | --- | --- |\n",
        "| cat | dog | 0.7 |\n",
        "| cup | mug | 0.9 |\n",
        "\n",
        "For each pair of words from a predefined dataset, we can calculate the cosine distance, and get a list of such similarity values. At the same time, we already have a list of similarity values, made by people. We can compare those lists and see how close they are (for example, by calculating the rank correlation). This measure of similarity should tell you how well the model simulates word distances.\n",
        "\n",
        "### Analogies\n",
        "\n",
        "Another popular way \"internal\" evaluation can be done by analogy. As we already discussed above, using simple arithmetic operations, we can modify the meaning of a word. If we collect in advance a set of modifier words, as well as words that we want to receive in the modification results, then based on the counting of the number of \"hits\" in the desired word, we can estimate how well the model works.\n",
        "\n",
        "We can use semantic analogies as modifier words. For example, if we have some kind of country-capital relationship, then to evaluate the model we can use pairs like Russia-Moscow, Norway-Oslo, and so on. The dataset will look like this:\n",
        "\n",
        "| word 1 | word 2 | attitude |\n",
        "| ------------ | ------------ | --------------- |\n",
        "| Russia | Moscow | capital - country |\n",
        "| Norway | Oslo | capital - country |\n",
        "\n",
        "For two random pairs from the set and given a triplet (Russia, Moscow, Norway) we want to get the word \"Oslo\", ie find a word that will have the same relationship with the word \"Norway\" as \"Russia\" is with Moscow.\n",
        "\n",
        "Datasets for the Russian language can be downloaded on the page with models at RusVectores. Let's calculate the quality of our NCRL model on a dataset about analogies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YhZcONZX-sY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/evaluation/ru_analogy_tagged.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z54TWXlcX-sb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "res = model_ru.accuracy('ru_analogy_tagged.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6a-XSp2hcp7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(res[4]['correct'][:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWNppFBeX-sd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(res[4]['incorrect'][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv0wxUXIX-se"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "The resulting model can be described using 2D visualization.\n",
        "\n",
        "### t-SNE\n",
        "\n",
        "**t-SNE** (*t-distributed Stochastic Neighbor Embedding*) is a technique for nonlinear dimensionality reduction and visualization of multidimensional variables. It was specially designed for high dimensional data by L. van der Maaten and D. Hinton, [here is their article](http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf). t-SNE is an iterative algorithm based on calculating pairwise distances between all objects (which is why it is rather slow).\n",
        "\n",
        "Let's show 1000 most frequent words from the collection of texts about cinema:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UH91gFyX-sf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from nltk import FreqDist\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "top_words = []\n",
        "\n",
        "\n",
        "fd = FreqDist()\n",
        "for s in tqdm(sentences):\n",
        "    fd.update(s)\n",
        "\n",
        "for w in fd.most_common(1000):\n",
        "    top_words.append(w[0])\n",
        "    \n",
        "print(top_words[:50:])\n",
        "top_words_vec = model[top_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jx_BhKHTgsG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "top_words_vec = model[top_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74FvTsuuX-sg",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "top_words_tsne = tsne.fit_transform(top_words_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_NKiuugTgsJ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# !pip install bokeh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0A__XKwX-si",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from bokeh.models import ColumnDataSource, LabelSet\n",
        "from bokeh.plotting import figure, show, output_file\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
        "           toolbar_location=\"above\",\n",
        "           title=\"word2vec T-SNE (eng model, top1000 words)\")\n",
        "\n",
        "source = ColumnDataSource(data=dict(x1=top_words_tsne[:,0],\n",
        "                                    x2=top_words_tsne[:,1],\n",
        "                                    names=top_words))\n",
        "\n",
        "p.scatter(x=\"x1\", y=\"x2\", size=8, source=source)\n",
        "\n",
        "labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
        "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
        "                  source=source, text_align='center')\n",
        "p.add_layout(labels)\n",
        "\n",
        "show(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Szes63RX-sl"
      },
      "source": [
        "To calculate the t-SNE transformation faster (and sometimes even more efficient), you can first reduce the dimension of the original data using, for example, SVD, and then apply t-SNE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0R7hvIfX-sm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd_50 = TruncatedSVD(n_components=50)\n",
        "top_words_vec_50 = svd_50.fit_transform(top_words_vec)\n",
        "top_words_tsne2 = TSNE(n_components=2, random_state=0).fit_transform(top_words_vec_50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpUJyJibX-sn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "output_notebook()\n",
        "\n",
        "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
        "           toolbar_location=\"above\",\n",
        "           title=\"word2vec T-SNE (eng model, top1000 words, +SVD)\")\n",
        "\n",
        "source = ColumnDataSource(data=dict(x1=top_words_tsne2[:,0],\n",
        "                                    x2=top_words_tsne2[:,1],\n",
        "                                    names=top_words))\n",
        "\n",
        "p.scatter(x=\"x1\", y=\"x2\", size=8, source=source)\n",
        "\n",
        "labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
        "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
        "                  source=source, text_align='center')\n",
        "p.add_layout(labels)\n",
        "\n",
        "show(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qZcF3nGX-sp"
      },
      "source": [
        "## FastText\n",
        "\n",
        "FastText uses not only word embeddings, but also n-gram embeddings. In the corpus, each word is automatically represented as a set of character n-grams. Say, if we set n = 3, then the vector for the word \"where\" will be the sum of the vectors of the following triggers: \"<wh\", \"whe\", \"her\", \"ere\", \"re>\" (where \"<\" and \">\" symbols denoting the beginning and end of a word). Thanks to this, we can also obtain vectors for words that are not in the dictionary, as well as efficiently work with texts containing errors and typos.\n",
        "\n",
        "* [Article](https://aclweb.org/anthology/Q17-1010)\n",
        "* [Website](https://fasttext.cc/)\n",
        "* [Tutorial](https://fasttext.cc/docs/en/support.html)\n",
        "* [Vectors for 157 languages](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
        "* [Vectors trained on wikipedia](https://fasttext.cc/docs/en/pretrained-vectors.html) (separate for 294 different languages)\n",
        "* [Repository](https://github.com/facebookresearch/fasttext)\n",
        "\n",
        "There is a `fasttext` library for python (you can work with ready-made models through` gensim`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvzM2AL7Z8gl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/facebookresearch/fastText.git\n",
        "! pip3 install fastText/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-IW_cBDX-sp",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "# train your model\n",
        "ft_model = fasttext.train_unsupervised('clean_text.txt', minn=3, maxn=4, dim=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNiuzf4DX-sr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "ft_model.get_word_vector(\"movie\")[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsZnXqH8X-st",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "ft_model.get_nearest_neighbors('acttor')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKR3FzCFX-sv",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "ft_model.get_analogies(\"man\", \"woman\", \"actor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttAgMPC_X-sx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# problem with typos is now solved\n",
        "\n",
        "ft_model.get_nearest_neighbors('actr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9alEdz0X-s0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# problem with out of vocabulary is solved too\n",
        "\n",
        "ft_model.get_nearest_neighbors('moviegeek')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSzVZF3bX-s3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!wget -O positive.csv https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVDNoIcuX-s5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!wget -O negative.csv https://dl.dropboxusercontent.com/s/r6u59ljhhjdg6j0/negative.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLidtYsuX-s8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
        "positive['label'] = ['positive'] * len(positive)\n",
        "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
        "negative['label'] = ['negative'] * len(negative)\n",
        "df = positive.append(negative)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YB6B4CvmNJ8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jtZR-uYX-tA"
      },
      "source": [
        "Let's do the standard preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI706u75jFdq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "! pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qAiiLvsX-tB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import pymorphy2\n",
        "from functools import lru_cache\n",
        "from multiprocessing import Pool\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import re\n",
        "\n",
        "m = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "regex = re.compile(\"[А-Яа-я:=!\\)\\()A-z\\_\\%/|]+\")\n",
        "\n",
        "def words_only(text, regex=regex):\n",
        "    try:\n",
        "        return regex.findall(text)\n",
        "    except:\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zakhvVrX-tC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@lru_cache(maxsize=128)\n",
        "# if you are *not* working in colab, you can replace pymorphy with mystem and uncomment the first line about lru_cache\n",
        "def lemmatize(text, pymorphy=m):\n",
        "    try:\n",
        "        return \" \".join([pymorphy.parse(w)[0].normal_form for w in text])\n",
        "    except:\n",
        "        return \" \"    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDQdq4MLX-tE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    return lemmatize(words_only(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj6dS127X-tG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "with Pool(8) as p:\n",
        "    lemmas = list(tqdm(p.imap(clean_text, df['text']), total=len(df)))\n",
        "\n",
        "    \n",
        "df['lemmas'] = lemmas\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QuPeLrvX-tI"
      },
      "source": [
        "Let's write the received data in the format for training the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGOFQSekX-tI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "X = df.lemmas.tolist()\n",
        "y = df.label.tolist()\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
        "print (\"total train examples %s\" % len(y_train))\n",
        "print (\"total test examples %s\" % len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mogTwRfnX-tK",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "with open('data.train.txt', 'w+') as outfile:\n",
        "    for i in range(len(X_train)):\n",
        "        outfile.write('__label__' + y_train[i] + ' '+ X_train[i] + '\\n')\n",
        "    \n",
        "\n",
        "with open('test.txt', 'w+') as outfile:\n",
        "    for i in range(len(X_test)):\n",
        "        outfile.write('__label__' + y_test[i] + ' ' + X_test[i] + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0rt2mhjX-tL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "classifier = fasttext.train_supervised('data.train.txt')\n",
        "result = classifier.test('test.txt')\n",
        "\n",
        "print('P@1:', result[1])\n",
        "print('R@1:', result[2])\n",
        "print('Number of examples:', result[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkb-ckInX-tM"
      },
      "source": [
        "## Optional Homework\n",
        "\n",
        "1. We will work with (partial) PUT YOUR DATASOURCE HERE data from [here](https://www.kaggle.com/yutkin/corpus-of-russian-news-articles-from-lenta/)\n",
        "2. Perform preprocessing of the text. Break the data into train and test for the classification task (we will use the topic field as the class label). While working on steps 3 and 5, take the following data as inputs for the classification:\n",
        "    - only titles (title)\n",
        "    - only news texts (text)\n",
        "    - both\n",
        "3. Train fastText to categorize texts by topic. Compare the quality for different data from step 2.\n",
        "4. Train your w2v model (or take any suitable pre-trained model). Implement a function to compute a vector of text / title / text + title as the average of the vector of the words it contains.\n",
        "     - (Bonus) Modify the vector mean calculation function: weight the word vectors with the appropriate tf-idf weights.\n",
        "5. Train the classification algorithm on the obtained average vectors. Compare the obtained quality with the fastText classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLJXu8KKX-tM",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install corus\n",
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFq4N-RG3abl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from corus import load_lenta\n",
        "\n",
        "path = 'lenta-ru-news.csv.gz'\n",
        "records = load_lenta(path)\n",
        "data = [(record.title, record.topic, record.text, record.tags) for record in records]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl7QEb263bzd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "lenta = pd.DataFrame(data, columns=['title','topic','text','tags'])\n",
        "lenta = lenta[lenta['topic'].isin(['Экономика','Спорт','Культура','Наука и техника','Бизнес'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5biB_zbX-tP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "lenta.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbnulE7-X-tR",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "len(lenta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7s5qJEiX-tS",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "lenta.topic.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NSv9Mo9X-tT",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "phrases = Phrases(lemmas, min_count=3)\n",
        "phraser = Phraser(phrases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeycAlaXtiVn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pza9RBqLRvTe"
      },
      "source": [
        "Let's [download created model from Colaboratory workspace](https://stackoverflow.com/questions/48774285/how-to-download-file-created-in-colaboratory-workspace) and evaluate it with Parallax. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ-e3rOnWAsz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = word2vec.Word2Vec.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJJRNv6WV-Rr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import files\n",
        "model = word2vec.Word2Vec.load(model_path)\n",
        "model.wv.save_word2vec_format('model_v1.txt', binary=False)\n",
        "files.download('model_v1.txt') \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgoIPv9OSx5N",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model.build_vocab(clean_sents, update=True)\n",
        "model.train(clean_sents, total_examples=model.corpus_count, epochs=5)\n",
        "model.wv.save_word2vec_format('model_v2.txt', binary=False)\n",
        "files.download('model_v2.txt') \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCjdOFSxU24L",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!ls -la\n",
        "!head -n 3 model_v2.txt"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0oH_UDyFX-rO"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
