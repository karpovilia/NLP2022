{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"course_description/","title":"\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u043a\u0443\u0440\u0441\u0430","text":"<p>\u041d\u0430\u0441\u0442\u043e\u044f\u0449\u0430\u044f \u0443\u0447\u0435\u0431\u043d\u0430\u044f \u0434\u0438\u0441\u0446\u0438\u043f\u043b\u0438\u043d\u0430 \u043f\u043e\u0441\u0432\u044f\u0449\u0435\u043d\u0430 \u0432\u043e\u043f\u0440\u043e\u0441\u0430\u043c \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 [natural language processing], \u043e\u0431\u043b\u0430\u0441\u0442\u0438, \u043d\u0430\u0445\u043e\u0434\u044f\u0449\u0435\u0439\u0441\u044f \u043d\u0430 \u0441\u0442\u044b\u043a\u0435 \u0442\u0430\u043a\u0438\u0445 \u0434\u0438\u0441\u0446\u0438\u043f\u043b\u0438\u043d, \u043a\u0430\u043a \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u043b\u0438\u043d\u0433\u0432\u0438\u0441\u0442\u0438\u043a\u0430. \u041a\u0443\u0440\u0441 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 \u0434\u0432\u0443\u0445 \u0447\u0430\u0442\u0441\u0435\u0439: \u0431\u0430\u0437\u043e\u0432\u043e\u0439, \u0432 \u0445\u043e\u0434\u0435 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0431\u0443\u0434\u0443\u0442 \u0432\u0432\u0435\u0434\u0435\u043d\u044b \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0438, \u0438 \u043f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u043e\u0439, \u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u043d\u0430 \u0438\u043d\u0434\u0443\u0441\u0442\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0438 \u0438 \u043d\u0430 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043d\u0430\u0443\u0447\u043d\u044b\u0435 \u0432\u043e\u043f\u0440\u043e\u0441\u044b. As a result, students should:</p>"},{"location":"course_description/#\u0426\u0435\u043b\u0438-\u043e\u0441\u0432\u043e\u0435\u043d\u0438\u044f-\u043a\u0443\u0440\u0441\u0430","title":"\u0426\u0435\u043b\u0438 \u043e\u0441\u0432\u043e\u0435\u043d\u0438\u044f \u043a\u0443\u0440\u0441\u0430","text":"<ol> <li>\u0418\u0437\u0443\u0447\u0435\u043d\u0438\u0435 \u0431\u0430\u0437\u043e\u0432\u044b\u0445 \u0437\u0430\u0434\u0430\u0447 \u0438 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0442\u0435\u043a\u0441\u0442\u043e\u0432</li> <li>\u0418\u0437\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0442\u0435\u043a\u0441\u0442\u043e\u0432</li> <li>\u041e\u0441\u0432\u043e\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c \u0438 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0434\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0442\u0435\u043a\u0441\u0442\u043e\u0432</li> </ol>"},{"location":"course_description/#\u041f\u043b\u0430\u043d\u0438\u0440\u0443\u0435\u043c\u044b\u0435-\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b-\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","title":"\u041f\u043b\u0430\u043d\u0438\u0440\u0443\u0435\u043c\u044b\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","text":"<ol> <li>\u0417\u043d\u0430\u0442\u044c \u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c \u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0442\u0435\u043a\u0441\u0442\u043e\u0432</li> <li>\u0417\u043d\u0430\u0442\u044c \u044d\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0430\u0441\u043f\u0435\u043a\u0442\u044b \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432</li> <li>\u0423\u043c\u0435\u0442\u044c \u0440\u0435\u0448\u0430\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0438, \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0435 \u0441 \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u044f\u0437\u044b\u043a\u0430</li> <li>\u0423\u043c\u0435\u0442\u044c \u0440\u0435\u0448\u0430\u0442\u044c \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438 \u043d\u0430 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</li> </ol>"},{"location":"course_description/#\u0422\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0435-\u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u044f","title":"\u0422\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u044f","text":"<ol> <li>\u0417\u043d\u0430\u043a\u043e\u043c\u0441\u0442\u0432\u043e \u0441 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u043e\u043c pytorch</li> </ol>"},{"location":"course_description/#\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430-\u043a\u0443\u0440\u0441\u0430","title":"\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u043a\u0443\u0440\u0441\u0430","text":""},{"location":"course_description/#week-01-\u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435-\u0432-\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443-\u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445-\u044f\u0437\u044b\u043a\u043e\u0432-\u0438-\u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443-\u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0445-\u0434\u0430\u043d\u043d\u044b\u0445","title":"week 01 \u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0432 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445 \u044f\u0437\u044b\u043a\u043e\u0432 \u0438 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","text":"<ul> <li>\u041a\u0440\u0443\u0433 \u0437\u0430\u0434\u0430\u0447 NLP, \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0438 \u043e\u0431\u043b\u0430\u0441\u0442\u0438.</li> <li>\u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f, \u043e\u0447\u0438\u0441\u0442\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430, \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f</li> </ul>"},{"location":"course_description/#week-02-\u0412\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435-\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f-\u0441\u043b\u043e\u0432","title":"week 02 \u0412\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0432","text":"<ul> <li>\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0445 \u0441\u043b\u043e\u0432</li> <li>one hot encoding, </li> </ul>"},{"location":"course_description/#week-03-\u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f-\u0442\u0435\u043a\u0441\u0442\u043e\u0432","title":"week 03 \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u043e\u0432","text":"<ul> <li>\u0417\u0430\u0434\u0430\u0447\u0430 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438.</li> <li>\u0421\u0432\u0451\u0440\u0442\u043e\u0447\u043d\u044b\u0435 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0435 \u0441\u0435\u0442\u0438 \u0432 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432.</li> <li>\u041c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438</li> </ul>"},{"location":"course_description/#week-04-\u042f\u0437\u044b\u043a\u043e\u0432\u044b\u0435-\u043c\u043e\u0434\u0435\u043b\u0438","title":"week 04 \u042f\u0437\u044b\u043a\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438","text":"<ul> <li>\u0417\u0430\u0434\u0430\u0447\u0430 Representation Learning</li> <li>\u041c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</li> <li>\u0420\u0435\u043a\u0443\u0440\u0440\u0435\u043d\u0442\u043d\u044b\u0435 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0435 \u0441\u0435\u0442\u0438 \u0432 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432</li> <li>LSTM \u0438 GRU \u044f\u0447\u0435\u0439\u043a\u0438</li> </ul>"},{"location":"course_description/#week-05-\u041c\u043e\u0434\u0435\u043b\u0438-Seq2Seq","title":"week 05 \u041c\u043e\u0434\u0435\u043b\u0438 Seq2Seq","text":"<ul> <li>\u041a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u044f \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f</li> <li>\u0420\u0435\u043a\u0443\u0440\u0440\u0435\u043d\u0442\u043d\u044b\u0435 \u0441\u0435\u0442\u0438 \u0441 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435\u043c, </li> </ul>"},{"location":"course_description/#week-06-\u041c\u043e\u0434\u0435\u043b\u044c-\u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440","title":"week 06 \u041c\u043e\u0434\u0435\u043b\u044c \u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440","text":"<ul> <li>\u041c\u043e\u0434\u0435\u043b\u044c \u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440</li> </ul>"},{"location":"course_description/#week-07-\u041c\u043e\u0434\u0435\u043b\u044c-BERT","title":"week 07 \u041c\u043e\u0434\u0435\u043b\u044c BERT","text":"<ul> <li>\u041c\u043e\u0434\u0435\u043b\u044c BERT</li> </ul>"},{"location":"course_description/#week-07-\u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435-\u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438-\u0438-\u043f\u043e\u0438\u0441\u043a","title":"week 07 \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0438 \u043f\u043e\u0438\u0441\u043a","text":"<ul> <li>\u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u043f\u043e\u0438\u0441\u043a</li> </ul>"},{"location":"course_description/#week-08-\u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435-\u0438\u043c\u0435\u043d-\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445","title":"week 08 \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0438\u043c\u0435\u043d \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445","text":"<ul> <li>\u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0438\u043c\u0435\u043d \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445</li> </ul>"},{"location":"course_description/#week-09-Question-answering","title":"week 09 Question answering","text":"<ul> <li>\u0412\u043e\u043f\u0440\u043e\u0441-\u043e\u0442\u0432\u0435\u0442\u043d\u044b\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0434\u043b\u044f \u0440\u0443\u0441\u0441\u043a\u043e\u0433\u043e \u0438 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430</li> <li>Zero \u0438 Few shot learning</li> </ul>"},{"location":"course_description/#week-10-\u0421\u0443\u043c\u043c\u0430\u0440\u0438\u0437\u0430\u0446\u0438\u044f-\u0442\u0435\u043a\u0441\u0442\u043e\u0432","title":"week 10 \u0421\u0443\u043c\u043c\u0430\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u043e\u0432","text":"<ul> <li>\u0421\u0443\u043c\u043c\u0430\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u043e\u0432, \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u0438 \u0430\u0431\u0441\u0442\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u0438</li> </ul>"},{"location":"course_description/#week-11-\u0421\u0436\u0430\u0442\u0438\u0435-\u043c\u043e\u0434\u0435\u043b\u0435\u0439","title":"week 11 \u0421\u0436\u0430\u0442\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439","text":"<ul> <li>\u0421\u0436\u0430\u0442\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, Pruning \u0438 \u0434\u0438\u0441\u0442\u0438\u043b\u044f\u0446\u0438\u044f</li> </ul>"},{"location":"course_description/#week-12-\u0410\u0442\u0430\u043a\u0438-\u043d\u0430-\u043c\u043e\u0434\u0435\u043b\u0438-\u0438-\u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f","title":"week 12 \u0410\u0442\u0430\u043a\u0438 \u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f","text":"<ul> <li>\u0410\u0442\u0430\u043a\u0438 \u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f.</li> </ul>"},{"location":"course_description/#\u041e\u0446\u0435\u043d\u0438\u0432\u0430\u043d\u0438\u0435-\u043f\u043e-\u043a\u0443\u0440\u0441\u0443","title":"\u041e\u0446\u0435\u043d\u0438\u0432\u0430\u043d\u0438\u0435 \u043f\u043e \u043a\u0443\u0440\u0441\u0443","text":""},{"location":"course_description/#\u0424\u043e\u0440\u043c\u0443\u043b\u0430-\u043e\u0446\u0435\u043d\u0438\u0432\u0430\u043d\u0438\u044f","title":"\u0424\u043e\u0440\u043c\u0443\u043b\u0430 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u043d\u0438\u044f","text":"<p><code>\u0418\u0442\u043e\u0433\u043e\u0432\u0430\u044f \u043e\u0446\u0435\u043d\u043a\u0430</code> = 0.3 \u0441\u0440\u0435\u0434\u043d\u0435\u0435(<code>\u0414\u043e\u043c\u0430\u0448\u043d\u044f\u044f \u0440\u0430\u0431\u043e\u0442\u0430</code>) + 0.2 \u0441\u0440\u0435\u0434\u043d\u0435\u0435(<code>\u041a\u0432\u0438\u0437\u044b</code>) + 0.5<code>\u042d\u043a\u0437\u0430\u043c\u0435\u043d</code>. - \u041f\u0440\u0430\u0432\u0438\u043b\u0430 \u043e\u043a\u0440\u0443\u0433\u043b\u0435\u043d\u0438\u044f \u2013 \u0430\u0440\u0438\u0444\u043c\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 - \u041f\u0440\u0435\u0434\u0443\u0441\u043c\u043e\u0442\u0440\u0435\u043d\u044b \u043b\u0438 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u044b. \u0414\u0430 - \u0423\u0441\u043b\u043e\u0432\u0438\u044f \u0434\u043b\u044f \u0432\u044b\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0430: \u0415\u0441\u043b\u0438 = 0.3 \u0441\u0440\u0435\u0434\u043d\u0435\u0435(\u0414\u043e\u043c\u0430\u0448\u043d\u044f\u044f \u0440\u0430\u0431\u043e\u0442\u0430) + 0.2*\u0441\u0440\u0435\u0434\u043d\u0435\u0435(\u041a\u0432\u0438\u0437\u044b) &gt; 4, \u0442\u043e \u0432\u044b\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0430\u0432\u0442\u043e\u043c\u0430\u0442, \u043f\u0440\u0438 \u044d\u0442\u043e\u043c \u043d\u0430\u043a\u043e\u043f\u043b\u0435\u043d\u043d\u0430\u044f \u043e\u0446\u0435\u043d\u043a\u0430 \u0443\u0434\u0432\u0430\u0438\u0432\u0430\u0435\u0442\u0441\u044f. - \u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u0435\u0440\u0435\u0441\u0434\u0430\u0447\u0438. \u041d\u0435\u0442</p>"},{"location":"course_description/#\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430","title":"\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430","text":""},{"location":"course_description/#\u0420\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u043c\u0430\u044f-\u043e\u0441\u043d\u043e\u0432\u043d\u0430\u044f-\u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430","title":"\u0420\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u043c\u0430\u044f \u043e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430","text":"<ol> <li>Manning, C. D., &amp; Sch\u00e8utze, H. (1999). Foundations of Statistical Natural Language Processing. Cambridge, Mass: The MIT Press. </li> </ol>"},{"location":"course_description/#\u0420\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u043c\u0430\u044f-\u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f-\u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430-\u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e","title":"\u0420\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u043c\u0430\u044f \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 (\u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e)","text":"<ol> <li>Shay Cohen. (2019). Bayesian Analysis in Natural Language Processing\u202f: Second Edition. San Rafael: Morgan &amp; Claypool Publishers.</li> <li>Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning</li> <li>Dan Jurafsky and James H. Martin. Speech and Language Processing (3rd ed. draft)</li> <li>Delip Rao and Brian McMahan. Natural Language Processing with PyTorch</li> </ol>"},{"location":"exam/","title":"\u0411\u0438\u043b\u0435\u0442\u044b \u043a \u044d\u043a\u0437\u0430\u043c\u0435\u043d\u0443","text":"<ol> <li>\u0412\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0432. \u041c\u043e\u0434\u0435\u043b\u044c word2vec</li> <li>\u0412\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0432. \u041c\u043e\u0434\u0435\u043b\u044c GloVe</li> <li>\u0412\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0432. \u041c\u0430\u0442\u0440\u0438\u0447\u043d\u0430\u044f \u0444\u0430\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u044f</li> <li>\u0417\u0430\u0434\u0430\u0447\u0430 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432. \u0411\u0430\u0437\u043e\u0432\u0430\u044f \u043f\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0437\u0430\u0434\u0430\u0447\u0438 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043f\u043e \u0442\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438. \u041c\u043e\u0434\u0435\u043b\u044c \u043c\u0435\u0448\u043a\u0430 \u0441\u043b\u043e\u0432 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432. Tf idf \u0432\u0437\u0432\u0435\u0448\u0438\u0432\u0430\u043d\u0438\u0435</li> <li>\u0417\u0430\u0434\u0430\u0447\u0430 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438. \u0411\u0430\u0437\u043e\u0432\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044b \u043a \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432. \u0424\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a EDA, \u043e\u0431\u0440\u0430\u0442\u043d\u044b\u0439 \u043f\u0435\u0440\u0435\u0432\u043e\u0434</li> <li>\u0424\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a fast text \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u0438 \u0434\u043b\u044f \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0432</li> <li>\u0421\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u044b\u0435 \u0441\u0435\u0442\u0438 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432</li> <li>\u0417\u0430\u0434\u0430\u0447\u0430 \u044f\u0437\u044b\u043a\u043e\u0432\u043e\u0433\u043e \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f. \u0421\u0447\u0435\u0442\u043d\u044b\u0435 \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438. \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u041b\u0430\u043f\u043b\u0430\u0441\u0430</li> <li>\u0417\u0430\u0434\u0430\u0447\u0430 \u044f\u0437\u044b\u043a\u043e\u0432\u043e\u0433\u043e \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f. \u041d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0432\u0430\u044f \u044f\u0437\u044b\u043a\u043e\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c Bengio.</li> <li>\u0417\u0430\u0434\u0430\u0447\u0430 \u044f\u0437\u044b\u043a\u043e\u0432\u043e\u0433\u043e \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f. \u0420\u0435\u043a\u0443\u0440\u0440\u0435\u043d\u0442\u043d\u044b\u0435 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0435 \u0441\u0435\u0442\u0438</li> <li>\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439. \u0416\u0430\u0434\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 (greedy search). \u041b\u0443\u0447\u0435\u0432\u043e\u0439 \u043f\u043e\u0438\u0441\u043a (beam search)</li> <li>\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439. \u041f\u0435\u0440\u043f\u043b\u0435\u043a\u0441\u0438\u044f</li> <li>\u0417\u0430\u0434\u0430\u0447\u0430 \u0442\u0435\u0433\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438. \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0443\u0449\u043d\u043d\u043e\u0441\u0442\u0435\u0439. \u041c\u0435\u0442\u043e\u0434\u044b \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430</li> <li>\u0417\u0430\u0434\u0430\u0447\u0430 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0430. \u041c\u043e\u0434\u0435\u043b\u044c \u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440</li> <li>\u041f\u0435\u0440\u0435\u043d\u043e\u0441 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0432 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432. \u041c\u043e\u0434\u0435\u043b\u044c BERT</li> <li>\u041f\u0435\u0440\u0435\u043d\u043e\u0441 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0432 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432. \u041c\u043e\u0434\u0435\u043b\u044c GPT2 (\u0438\u043b\u0438 GPT3)</li> <li>Task oriented chat bots. Intent detection, slot filling. \u0411\u0430\u0437\u043e\u0432\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044b</li> <li>\u041c\u043e\u0434\u0435\u043b\u0438 \u0441\u0432\u043e\u0431\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0438\u0430\u043b\u043e\u0433\u0430. \u0413\u0435\u043d\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434</li> <li>\u041c\u043e\u0434\u0435\u043b\u0438 \u0441\u0432\u043e\u0431\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0438\u0430\u043b\u043e\u0433\u0430. \u041f\u043e\u0434\u0445\u043e\u0434 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0433\u043e \u043f\u043e\u0438\u0441\u043a\u0430</li> <li>\u0412\u043e\u043f\u0440\u043e\u0441\u043d\u043e-\u043e\u0442\u0432\u0435\u0442\u043d\u044b\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b. \u0417\u0430\u0434\u0430\u0447\u0430 SQuAD. \u0411\u0430\u0437\u043e\u0432\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044b \u043a \u0440\u0435\u0448\u0435\u043d\u0438\u044e</li> <li>\u0421\u0436\u0430\u0442\u0438\u0435 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439. \u041f\u0440\u0443\u043d\u0438\u043d\u0433 \u0438 \u043a\u0432\u0430\u043d\u0442\u0438\u0437\u0430\u0446\u0438\u044f</li> <li>\u0421\u0436\u0430\u0442\u0438\u0435 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439. \u0414\u0438\u0441\u0442\u0438\u043b\u043b\u044f\u0446\u0438\u044f. \u041c\u043e\u0434\u0435\u043b\u044c DistilBERT</li> <li>\u041f\u043e\u0442\u043e\u043c\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 BERT. RoBERTa, ELECTRA, ALBERT</li> <li>\u042d\u043a\u0441\u0442\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u0430\u044f \u0441\u0443\u043c\u043c\u0430\u0440\u0438\u0437\u0430\u0446\u0438\u044f. \u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c TextRank</li> <li>\u0410\u0431\u0441\u0442\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u0430\u044f \u0441\u0443\u043c\u043c\u0430\u0440\u0438\u0437\u0430\u0446\u0438\u044f. \u041c\u043e\u0434\u0435\u043b\u0438 BART \u0438 T5</li> <li>\u041c\u0435\u0440\u044b \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0432 seq2seq \u0437\u0430\u0434\u0430\u0447\u0430\u0445. BLEU, ROUGE</li> <li>\u0427\u0435\u0442\u044b\u0440\u0435 \u0441\u0446\u0435\u043d\u0430\u0440\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 BERT</li> <li>Zero shot \u043f\u043e\u0434\u0445\u043e\u0434\u044b \u043a \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432</li> <li>\u041c\u0435\u0442\u043e\u0434\u044b \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u0447\u0442\u0435\u043d\u0438\u044f \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439</li> <li>Template-based \u043c\u0435\u0442\u043e\u0434\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439</li> <li>\u0417\u0430\u0434\u0430\u0447\u0430 \u0438 \u043c\u0435\u0442\u043e\u0434\u044b \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439 </li> <li>\u041f\u0435\u0440\u0435\u043d\u043e\u0441 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u0435\u0436\u0434\u0443 \u044f\u0437\u044b\u043a\u0430\u043c\u0438. \u041c\u0443\u043b\u044c\u0442\u0438\u044f\u0437\u044b\u0447\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438</li> <li>\u0410\u0434\u0432\u0435\u0440\u0441\u0430\u043b\u044c\u043d\u044b\u0435 \u0430\u0442\u0430\u043a\u0438, \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435 \u0443\u0441\u0442\u043e\u0439\u0447\u0438\u0432\u043e\u0441\u0442\u0438 \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439. </li> <li>\u0422\u0438\u043f\u044b \u0430\u0434\u0432\u0435\u0440\u0441\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u0442\u0430\u043a (\u0447\u0435\u0440\u043d\u044b\u0439, \u0441\u0435\u0440\u044b\u0439, \u0431\u0435\u043b\u044b\u0439 \u044f\u0449\u0438\u043a\u0438)</li> <li>\u0423\u0440\u043e\u0432\u043d\u0438 \u0430\u0434\u0432\u0435\u0440\u0441\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u0442\u0430\u043a (\u0441\u0438\u043c\u0432\u043e\u043b\u044b, \u0441\u043b\u043e\u0432\u0430, \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f)</li> <li>\u041a\u0430\u043a\u0438\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u043c \u044f\u0437\u044b\u043a\u043e\u043c \u0432\u044b \u0437\u043d\u0430\u0435\u0442\u0435?</li> <li>\u041a\u0430\u043a\u0438\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0440\u0443\u0441\u0441\u043a\u0438\u043c \u044f\u0437\u044b\u043a\u043e\u043c \u0432\u044b \u0437\u043d\u0430\u0435\u0442\u0435?</li> </ol>"},{"location":"readme/","title":"Main Page","text":"<ul> <li>\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u043a\u0443\u0440\u0441\u0430</li> <li>\u0421\u0442\u0440\u0430\u043d\u0438\u0446\u0430 \u043a\u0443\u0440\u0441\u0430</li> </ul>"},{"location":"readme/#\u0420\u0430\u0441\u043f\u0438\u0441\u0430\u043d\u0438\u0435","title":"\u0420\u0430\u0441\u043f\u0438\u0441\u0430\u043d\u0438\u0435","text":"Date Time Activity Topic Materials Group 08.09.2022 18:10-19:30 Lecture 01 \u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0432 \u0430\u043d\u0430\u043b\u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u043e\u0432, \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043d\u0433 \u0441\u043b\u0430\u0439\u0434\u044b \u0432\u0438\u0434\u0435\u043e 1,2 08.09.2022 19:40-21:00 Seminar 01 \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0439, \u0432\u0438\u0434\u0435\u043e 1,2 08.09.2022 \u041a\u0432\u0438\u0437 \u043f\u0440\u043e \u0431\u044d\u043a\u0433\u0440\u0430\u0443\u043d\u0434 Quiz 1,2 15.09.2022 18:10-19:30 Lecture 02 \u0412\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0432 \u0441\u043b\u0430\u0439\u0434\u044b, \u0432\u0438\u0434\u0435\u043e 1,2 15.09.2022 19:40-21:00 Seminar 02 \u0412\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0432 \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0439, \u0432\u0438\u0434\u0435\u043e 1,2 17.09.2022 19:40-21:00 Seminar 02 \u0412\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0432 \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0439, \u0432\u0438\u0434\u0435\u043e 1,2 17.09.2022 \u0412\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0432 Quiz 1,2 22.09.2022 18:10-19:30 Lecture 03 \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u043a\u043e\u043d\u0441\u043f\u0435\u043a\u0442, \u0432\u0438\u0434\u0435\u043e 1,2 22.09.2022 19:40-21:00 Seminar 03 \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f classification.ipynb, \u0432\u0438\u0434\u0435\u043e 1,2 22.09.2022 19:40-21:00 Seminar 03 \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f classification.ipynb, \u0432\u0438\u0434\u0435\u043e 1,2 29.09.2022 18:10-19:30 Lecture 04 \u042f\u0437\u044b\u043a\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0441\u043b\u0430\u0439\u0434\u044b, \u0432\u0438\u0434\u0435\u043e 1,2 29.09.2022 19:40-21:00 Seminar 04 \u042f\u0437\u044b\u043a\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 lm.ipynb, extra, video 1,2 29.09.2022 19:40-21:00 Seminar 04 \u042f\u0437\u044b\u043a\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 lm.ipynb, video 1,2 06.10.2022 18:10-19:30 Lecture 05 \u041c\u0430\u0448\u0438\u043d\u043d\u044b\u0439 \u043f\u0435\u0440\u0435\u0432\u043e\u0434 \u0441\u043b\u0430\u0439\u0434\u044b, \u0432\u0438\u0434\u0435\u043e 1,2 06.10.2022 19:40-21:00 Seminar 05 \u041c\u0430\u0448\u0438\u043d\u043d\u044b\u0439 \u043f\u0435\u0440\u0435\u0432\u043e\u0434 seq2seq.ipynb lost, \u0432\u0438\u0434\u0435\u043e 1,2 06.10.2022 19:40-21:00 Seminar 05 \u041c\u0430\u0448\u0438\u043d\u043d\u044b\u0439 \u043f\u0435\u0440\u0435\u0432\u043e\u0434 seq2seq.ipynb lost, \u0432\u0438\u0434\u0435\u043e 1,2 13.10.2022 18:10-19:30 Lecture 06 Transformer \u0441\u043b\u0430\u0439\u0434\u044b lost, \u0432\u0438\u0434\u0435\u043e 1,2 13.10.2022 19:40-21:00 Seminar 06 Transformer transformer.ipynb, \u0432\u0438\u0434\u0435\u043e 1,2 20.10.2022 18:10-19:30 Lecture 07 Bert \u0441\u043b\u0430\u0439\u0434\u044b lost, \u0432\u0438\u0434\u0435\u043e 1,2 20.10.2022 19:40-21:00 Seminar 07 Bert BERT.ipynb, \u0432\u0438\u0434\u0435\u043e 1,2 03.11.2022 18:10-19:30 Lecture 08 Information Extraction slides, video 1,2 03.11.2022 18:10-19:30 Seminar 08 Information Extraction ipynb_lastY, bonus: spacy lost, videolost 1,2 10.11.2022 18:10-19:30 Lecture 09 Dialogue Systems 1 notes, video 1,2 10.11.2022 18:10-19:30 Seminar 09 Dialogue Systems 1 Dialog Systems lost, GODEL lost, video 1,2 17.11.2022 18:10-19:30 Lecture 10 Dialogue Systems 2 notes, video 1,2 17.11.2022 18:10-19:30 Seminar 10 Dialogue Systems 2 st lost, qa lost, video 1,2 24.11.2022 18:10-19:30 Lecture 11 seq2seq models slides, video 1,2 24.11.2022 18:10-19:30 Seminar 11 Summarization Summarization.ipynb, video 1,2 01.12.2022 18:10-19:30 Lecture 12 QA slides, video_pt1, video_pt2 1,2 01.12.2022 18:10-19:30 Seminar 12 QA QA.ipynb, video 1,2 08.12.2022 18:10-19:30 Lecture 13 Model Compression nothing 1,2 08.12.2022 18:10-19:30 Seminar 13 Model Compression nothing 1,2 15.12.2022 18:10-19:30 Lecture 14 \u0410\u0442\u0430\u043a\u0438 \u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f nothing 1,2 15.12.2022 18:10-19:30 Seminar 14 \u0410\u0442\u0430\u043a\u0438 \u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f nothing 1,2"},{"location":"Week%2002%20-%20Word%20Embeddings/main/","title":"Main","text":"<p>\u042d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0441\u043b\u043e\u0432 2_1_embeddings.ipynb 2_2_rusvectores 2_3_parallax 2_4_Word_2_vec.ipynb</p>"},{"location":"Week%2003%20-%20Text%20Classification/lec3/","title":"Lec3","text":"<p>\u041a\u043e\u043d\u0441\u043f\u0435\u043a\u0442 \u0442\u0443\u0442: https://artemova.notion.site/d4ee7823399c47de9ebd6f2cb15cdb73</p>"},{"location":"Week%2004%20-%20Language%20Models/main/","title":"Main","text":""},{"location":"Week%2004%20-%20Language%20Models/main/#\u0412\u043e\u043f\u0440\u043e\u0441\u044b-\u043f\u043e-\u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0443","title":"\u0412\u043e\u043f\u0440\u043e\u0441\u044b \u043f\u043e \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0443","text":""},{"location":"Week%2004%20-%20Language%20Models/main/#\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430-\u0440\u0435\u043a\u0443\u0440\u0440\u0435\u043d\u0442\u043d\u044b\u0445-\u0441\u0435\u0442\u0435\u0439","title":"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430 \u0440\u0435\u043a\u0443\u0440\u0440\u0435\u043d\u0442\u043d\u044b\u0445 \u0441\u0435\u0442\u0435\u0439","text":"<p>\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0442\u043e\u0433\u043e \u043a\u0430\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442 \u0440\u0435\u043a\u0443\u0440\u0440\u0435\u043d\u0442\u043d\u044b\u0435 \u0441\u0435\u0442\u0438 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u0432 \u0437\u0430\u043c\u0435\u0447\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u043c \u043a\u0443\u0440\u0441\u0435 \u0410\u0440\u0434\u0440\u0435\u044f \u041a\u0430\u0440\u043f\u0430\u0442\u044b. \u0422\u0430\u043c \u0436\u0435 \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u043f\u0440\u0438\u043c\u0435\u0440 \u043f\u043e\u0441\u0438\u043c\u0432\u043e\u043b\u044c\u043d\u043e\u0439 \u0440\u0435\u043a\u0443\u0440\u0440\u0435\u043d\u0442\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u0432\u0438\u0434\u0435\u043e, \u0441\u043b\u0430\u0439\u0434\u044b, index \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u0443\u044e\u0449\u0435\u0439 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0435 \u0441\u043b\u043e\u0432\u043e.</p>"},{"location":"Week%2004%20-%20Language%20Models/main/#LSTM--GRU","title":"LSTM &amp; GRU","text":"<ul> <li>\u041c\u043e\u0434\u0435\u043b\u0438 \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0445 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u0435\u0439 sequence-to-sequence \u043d\u0430 PyTorch (\u0427\u0430\u0441\u0442\u044c 1) habr</li> <li>\u041c\u043e\u0434\u0435\u043b\u0438 \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0445 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u0435\u0439 sequence-to-sequence \u043d\u0430 PyTorch (\u0427\u0430\u0441\u0442\u044c 2) habr</li> </ul>"},{"location":"Week%2005%20-%20Machine%20Translation/mail/","title":"Mail","text":""},{"location":"Week%2005%20-%20Machine%20Translation/mail/#What-is-Residual-Connection","title":"What is Residual Connection?","text":"<p>From https://kharshit.github.io/blog/2018/09/07/skip-connections-and-residual-blocks</p>"},{"location":"Week%2013%20-Model%20Compression/02%20-%20Quantization/","title":"Quantization","text":"<p>Quantization aware training colab</p>"},{"location":"Week%2013%20-Model%20Compression/03%20-%20Knowledge%20Distillation/","title":"Knowledge Distillation","text":"<p>Train student to mimic a pre trained, larger teacher</p> <p> Decoupled Knowledge Distillation, CVPR-2022, pdf, git</p> <p>Distillation methods vary on: - different types of teacher model - different types of loss function     - squared error between the logits of the models     - KL divergence between the predictive distributions, or     - some other measure of agreement between the model predictions. - different choices for what dataset the student model trains on.     - a large unlabeled dataset     - a held-out data set, or     - the original training set. - Mimic what?     - Teacher's class probabilities     - Teacher's feature representation - Learn from whom?     - Teacher, teacher assistant, other fellow students - Adversarial learning</p>"},{"location":"Week%2013%20-Model%20Compression/03%20-%20Knowledge%20Distillation/#Distilling-the-Knowledge-in-a-Neural-Network","title":"Distilling the Knowledge in a Neural Network","text":"<p>Geoffrey Hinton, Oriol Vinyals, Jeff Dean Distilling the Knowledge in a Neural Network, 2015</p> <ul> <li>The relative probabilities of incorrect answers tell us a lot about how the / teacher model tends to generalize.</li> <li>Softmax with temperature \\(q_i = \\frac {exp(z_i/T)}{\\sum_j(z_j/T)}\\)</li> <li>Use cross entropy (H) for both the soft and hard part of the loss function. Typically smaller weight for hard part works better.</li> <li> <p>When using both hard and soft loss, since the magnitudes of the gradients produced by the soft targets scale as \\(1/T^2\\) it is important to multiply them by \\(T^2\\) when using both hard and soft targets.</p> <p>\\(L_{KD}(W_s) = H(y_true, P_s) + \\lambda H(P_T, P_S)\\)</p> </li> </ul> <p></p>"},{"location":"Week%2013%20-Model%20Compression/03%20-%20Knowledge%20Distillation/#Sobolev-Training-for-Neural-Networks","title":"Sobolev Training for Neural Networks","text":"<p> a) Sobolev Training of order 2. Diamond nodes \\(m\\) and \\(f\\) indicate parameterised functions, where \\(m\\) is trained to approximate \\(f\\). Green nodes receive supervision. Solid lines indicate connections through which error signal from loss \\(l\\), \\(l_1\\), and \\(l_2\\) are backpropagated through to train \\(m\\).  b) Stochastic Sobolev Training of order 2. If \\(f\\) and \\(m\\) are multivariate functions, the gradients are Jacobian matrices. To avoid computing these high dimensional objects, we can efficiently compute and fit their projections on a random vector \\(v_j\\) sampled from the unit sphere</p> <ul> <li>Sobolev Training for Neural Networks, NIPS 2017, pdf</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/03%20-%20Knowledge%20Distillation/#Noisy-teachers","title":"Noisy teachers","text":"<p>Let's include a noise-based regularizer while training the strudent from the teacher. - Noise is Gaussian noise with mean 0 and std dev \\(\\sigma\\) . This noise is added to teachers logits. - Noisy teachr is more helpful than a noisy student</p> <p> Training Shallow Students using the proposed Logit Perturbation Method</p> <p>Deep Model Compression: Distilling Knowledge from Noisy Teachers, 2016, pdf</p>"},{"location":"Week%2013%20-Model%20Compression/03%20-%20Knowledge%20Distillation/#Distillation-architectures","title":"Distillation architectures","text":"<ul> <li>Learning students and teacher together</li> <li>Multiple teachers</li> <li>Adversarial methods</li> </ul> <ul> <li>https://huggingface.co/docs/transformers/model_doc/distilbert</li> <li>https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D</li> <li>Fine Tuning DistilBERT for MultiLabel Text Classification colab</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/03%20-%20Knowledge%20Distillation/#Multi-step-knowledge-distillation","title":"Multi-step knowledge distillation","text":"<p>or chain distillation - Student network performance degrades when the gap between student and teacher is large. - Multi-step knowledge proposes to use intermediate sized network (teacher assistant) to bridge the gap between the student and the teacher  TA fills the gap between student &amp; teacher - Improved Knowledge Distillation via Teacher Assistant, 2019, pdf</p>"},{"location":"Week%2013%20-Model%20Compression/03%20-%20Knowledge%20Distillation/#Distillation-from-many-teachers","title":"Distillation from many teachers","text":"<p>or an ensemble of specialists - Teacher model could be an ensemble that contains:     - one generalist model, trained on all the data     - many specialist models, each of which is trained on data, that is highly enriched in examples from a very confusable subset of the classes - The softmax of this type of specialist can be made much smaller by combining all of the classes it doesn't care about into single \\(pad\\) class - Training the student     - 1. For each instance, we find the \\(n\\) most propable classes acording to the generalist model, (\\(K\\) - known)     - 2. We take all the specialist models \\(m\\) whose subset of confusable classes \\(S^m\\), has a non-empty intersection with \\(K\\) and call this the active set of specialists \\(A_k\\). Then, we find the full probability distribution \\(q\\) over all the classes that minimizes \\(KL(gen, q) + \\sum{KL(p^m,q)}\\)</p>"},{"location":"Week%2013%20-Model%20Compression/03%20-%20Knowledge%20Distillation/#KD-with-Adversarial-methods","title":"KD with Adversarial methods","text":"<p>Knowledge Distillation with Adversarial Samples Supporting Decision Boundary, 2018, pdf  The concept of knowledge distillation using samples close to the decision boundary. The dots in the figure represent the training sample and the circle around a dot represents the distance to the nearest decision boundary. The samples close to the decision boundary enable more accurate knowledge transfer.  Iterative scheme to find boundary supporting samples for a base sample</p>"},{"location":"Week%2013%20-Model%20Compression/04%20Parameter%20sharing/","title":"Parameter sharing","text":""},{"location":"Week%2013%20-Model%20Compression/05%20Matrix%20decomposition/","title":"05 Matrix decomposition","text":"<p>https://github.com/bjpark0805/LoBERT</p> <p>Oleksii Hrinchuk, Valentin Khrulkov, Leyla Mirvakhabova, Elena Orlova, Ivan Oseledets, Tensorized Embedding Layers, 2020</p> <p>Andros Tjandra, Sakriani Sakti, Satoshi Nakamura, Compressing Recurrent Neural Network with Tensor Train</p>"},{"location":"Week%2013%20-Model%20Compression/Index/","title":"Model compression","text":""},{"location":"Week%2013%20-Model%20Compression/Index/#Recommended-articles","title":"Recommended articles:","text":"<ul> <li>An Overview of Model Compression Techniques for Deep Learning in Space</li> <li>Compressing Large-Scale Transformer-Based Models: A Case Study on BERT</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/Index/#Why-should-we-compress-models","title":"Why should we compress models?","text":""},{"location":"Week%2013%20-Model%20Compression/Index/#Resource-constraints","title":"Resource constraints","text":"<ul> <li>RAM</li> <li>Prediction Latency - FLOPs</li> <li> <p>Power dissipation</p> <ul> <li>Battery drains fast, phone overheats</li> </ul> </li> <li> <p>Cloud problems</p> <ul> <li>Network delay</li> <li>Power consumption for communication</li> <li>User Privacy</li> </ul> </li> <li>Persistent Recurrent Neural Networks<ul> <li>Cache the weights in one-chip memory such as caches, block RAM, or register files across multiple timestamps</li> <li>High model compression allows significatly large RNNs to be stored in on-chip memory</li> <li>146x speedup if entire RNN can be stored in registers rather than GPU DRAM</li> </ul> </li> </ul> <p>Greg Diamos, Shubho Sengupta, Bryan Catanzaro, Mike Chrzanowski, Adam Coates, Erich Elsen, Jesse Engel, Awni Hannun, Sanjeev Satheesh Persistent RNNs: Stashing Recurrent Weights On-Chip, 2017</p> <p>Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, William J. Dally, EIE: Efficient Inference Engine on Compressed Deep Neural Network, 2016</p>"},{"location":"Week%2013%20-Model%20Compression/Index/#TrainingFinetuning-large-models-is-difficult","title":"Training/Finetuning large models is difficult","text":"<ul> <li>Impossible to fine-tune pretrained BERT-large on GPU with 16 GB RAM</li> <li>This poses large barrier of entry for communities without the resources to purchase either several large GPUs or time on Google\u2019s TPU\u2019s</li> <li>Tuning various configuration parameters needs lots of resources</li> <li>By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training required, and improved speed of learning and/or classification</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/Index/#Mobile-apps-limitations","title":"Mobile apps limitations","text":"<p>App stores are very sensitive to the size of the binary files Smaller models also decrease the communication overhead of distributed training of the models</p>"},{"location":"Week%2013%20-Model%20Compression/Index/#Is-it-possible","title":"Is it possible?","text":"<ul> <li>large amount of redundancy among the weights of the neural network</li> <li>a small subset of the weights are sufficient to reconstruct the entire network</li> <li>We can predict &gt;95% of the weights without any drop in accuracy</li> </ul> <p>Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando de Freitas. Predicting Parameters in Deep Learning 2014</p>"},{"location":"Week%2013%20-Model%20Compression/Index/#Model-compression-techniques","title":"Model compression techniques","text":""},{"location":"Week%2013%20-Model%20Compression/Index/#Pruning-sparsifying-weight-matrices","title":"Pruning: sparsifying weight matrices","text":"<ul> <li> <p>Pruning: sparsifying weight matrice Methods differ based on what is pruned and the actual logic used to prune</p> </li> <li> <p>Given a matrix, one can prune:</p> <ul> <li>some weight entries</li> <li>rows/columns</li> <li>bloks</li> <li>heads</li> <li>layers</li> </ul> </li> <li>Which weights/neurons/blocks/heads to prune?</li> <li>Should you prune large networks or build small networks?</li> <li>Iterative/gradual pruning? Iterative pruning and densification</li> <li>Interplay between pruning and regularization</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/Index/#Quantization-Reducing-bits-to-represent-each-weight","title":"Quantization: Reducing bits to represent each weight","text":"<p>02 - Quantization</p> <ul> <li>Weights can be quantized to two values (binary), three values(ternary) or multiple bits</li> <li>Uniform vs non-uniform</li> <li>Deterministic vs stochastic</li> <li>Loss-aware or not</li> <li>Trained vs tuned parameters</li> <li>How to quantize word vectors, RNN/LSTM weight matricies, transformers</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/Index/#Knowledge-distillation-Train-student-to-mimic-a-pretrained-larger-teacher","title":"Knowledge distillation: Train student to mimic a pretrained, larger teacher","text":"<ul> <li> <p>Knowledge Distillation: Train student to mimic a pre trained, larger teacher Distillation methods vary on:</p> </li> <li> <p>Different types of teacher model</p> </li> <li>Different types of loss function<ul> <li>Squared error between the logits of the models</li> <li>KL divergence between the predictive distributions, or</li> <li>Some other measure of agreement between the model predictions.</li> </ul> </li> <li>Different choices for what dataset the student model trains on.<ul> <li>A large unlabeled dataset</li> <li>A held-out data set, or</li> <li>The original training set.</li> </ul> </li> <li>Mimic what?<ul> <li>Teacher's class probabilities</li> <li>Teacher's feature representation</li> </ul> </li> <li>Learn from whom? Teacher, teacher assistant, other fellow students</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/Index/#Parameter-sharing","title":"Parameter sharing","text":"<p>04 Parameter sharing</p> <p>Methods differ depending on:</p> <ul> <li>Which parameters are shared</li> <li>Technique used to share parameters</li> <li>Level at which sharing is performed</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/Index/#Matrix-decomposition-Factorize-large-matrices-into-multiple-smaller-components","title":"Matrix decomposition: Factorize large matrices into multiple smaller components","text":"<p>05 Matrix decomposition Methods differ      </p> <ul> <li>in the type of factorization technique</li> <li>matrices being factorized</li> <li>and the property of weight matrix being exploited</li> </ul> <p>Neural Architecture Search (NAS)</p>"},{"location":"Week%2013%20-Model%20Compression/Neural%20Architecture%20Search%20%28NAS%29/","title":"Neural Architecture Search (NAS)","text":"<ul> <li>A Survey on Neural Architecture Search</li> <li>Neural Architecture Search: A Survey</li> <li>https://www.pyimagesearch.com/2019/01/07/auto-keras-and-automl-a-getting-started-guide/</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/01%20-%20Pruning%20weights/","title":"01   Pruning weights","text":""},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/01%20-%20Pruning%20weights/#Optimal-Brain-Damage-OBD","title":"Optimal Brain Damage (OBD)","text":"<ul> <li>Yann LeCun, John Denker, Sara Solla, Optimal Brain Damage (NIPS 1989)</li> <li>\u041e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u043e\u0440\u0435\u0436\u0438\u0432\u0430\u043d\u0438\u0435 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u0435\u0439</li> </ul> <p>Oldest work. How to define saliency of a weight, besides simply using its magnitude? Using change in the objective function, caused by deleting/changing the parameter.</p> <p>Drawbacks - Computationally prohibitive as second derivative computations are expensive. - Cross terms in the Hessian matrix are ignored.</p>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/01%20-%20Pruning%20weights/#Optimal-Brain-Surgeon","title":"Optimal Brain Surgeon","text":"<p>Babak Hassibi, David Stork, Second order derivatives for network pruning: Optimal Brain Surgeon (NIPS 1992)</p> <ul> <li>Use information from all second order derivatives of the error function to perform network pruning.</li> <li>Also, unlike other methods (like OBD or magnitude pruning), OBS does not demand (typically slow) retraining after the pruning of a weight.</li> <li>Computationally prohibitive as second derivative computations are expensive.</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/01%20-%20Pruning%20weights/#Deep-Compression","title":"Deep Compression","text":"<p>Song Han, Huizi Mao, William J. Dally, Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</p> <p>A more computationally feasible method for pruning connections and relearning weights based solely on the magnitude of the original weights</p> <p></p>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/01%20-%20Pruning%20weights/#Pruning-encoder-decoder-models","title":"Pruning encoder-decoder models","text":"<p>Abigail See, Minh-Thang Luong, Christopher D. Manning, Compression of Neural Machine Translation Models via Pruning</p> <p></p>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/01%20-%20Pruning%20weights/#Pruning-Schemes","title":"Pruning Schemes","text":"<ul> <li>How do we distribute the pruning over the different weight classes of our model?<ul> <li>Class-blind: Take all parameters, sort them by magnitude and prune the x% with smallest magnitude, regardless of weight class.</li> <li>Class-uniform: Within each class, sort the weights by magnitude and prune the x% with smallest magnitude.</li> <li>Class-distribution: For each class c, weights with magnitude less than \\(\\lambda\\) are pruned.</li> </ul> </li> <li>Retraining the sparse pruned network helps. Retrain with smaller learning rate (LR), simpler LR schedule (halve every 2 epochs), and train for fewer epochs.</li> <li>Class-blind pruning outperforms both other schemes.</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/01%20-%20Pruning%20weights/#Iterative-Pruning","title":"Iterative Pruning","text":"<p>Song Han, Jeff Pool, John Tran, William J. Dally Learning both Weights and Connections for Efficient Neural Networks, 2015</p> <p></p> <ul> <li>Regularization (L1/L2) while training.</li> <li>Fixed threshold is used for magnitude pruning in every iteration.</li> <li>Dropout Ratio Adjustment<ul> <li>During retraining, the dropout ratio must be adjusted to account for the change in model capacity.</li> <li>As pruning already reduced model capacity, the retraining dropout ratio should be smaller.</li> </ul> </li> </ul>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/01%20-%20Pruning%20weights/#Iterative-Magnitude-Pruning-for-Transformers","title":"Iterative Magnitude Pruning for Transformers","text":"<p>Robin Cheong, Robel Daniel, Compressing Transformers with Pruning and Quantization, 2019 ERNIE github</p> <ul> <li>For starting proportion X% and ending proportion Y%, our iterative magnitude pruning procedure pruned X% of each of the pre-trained Transformer layers, began re-training, and pruned (Y -X)/9 % of each of the layers every 1001 iterations.</li> </ul> <p></p> <ul> <li>By the 10,000th iteration, we reached Y% pruning of the model iteratively.</li> <li>Do not factor in word embeddings in compression rate.</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/01%20-%20Pruning%20weights/#Sparse-BERT-with-improved-pruning","title":"Sparse BERT with improved pruning","text":"<p>Fu-Ming Guo, Sijia Liu, Finlay S. Mungall, Xue Lin, Yanzhi Wang, Reweighted Proximal Pruning for Large-Scale Language Representation, 2019</p> <ul> <li>Two problems with pruning<ul> <li>The larger weight \\(w_i\\), is penalized more heavily than smaller weight \\(w_i\\) in \\(l_1\\) regularization, which violates the original intention of weight pruning, \u201cremoving the unimportant connections\u201d.</li> <li>Direct optimization of a regularization penalty term causes divergence from the original loss function and has negative effect on the effectiveness of gradient-based update.</li> </ul> </li> <li>Solution using reweiehted proximal pruning (which depends on proximal operators)<ul> <li>Decouples the goals of high sparsity from minimizing loss.</li> </ul> </li> <li>NIP: Progressive/gradual pruning without regularizers.</li> </ul> <p></p> <ul> <li>Even when 90% of weights are pruned, next sentence prediction accuracy keeps above 95% in RPP. 80% pruning for most GLUE tasks and 41% for SQuAD 1.1 at 0 degradation for BERT BASE.</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/01%20-%20Pruning%20weights/#Should-we-prune-large-networks-or-build-small-dense-networks","title":"Should we prune large networks or build small dense networks?","text":"<p>Michael Zhu, Suyog Gupta, To prune, or not to prune: exploring the efficacy of pruning for model compression, 2017</p> <p></p> <p></p> <ul> <li>Pruning involves extra processing plus sparse matrices need special handling - can we avoid it</li> <li>Large-sparse models consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.</li> <li>Models: stacked LSTMs for language modeling, and seq2seq models for NMT.</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/02%20-%20Pruning%20neurons/","title":"Pruning neurons","text":""},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/02%20-%20Pruning%20neurons/#Node-Importance-Functions","title":"Node Importance Functions","text":"<p>Tianxing He; Yuchen Fan; Yanmin Qian; Tian Tan; Kai Yu, Reshaping deep neural network for fast decoding by node-pruning, 2014</p> <ul> <li>After training, a score is calculated for each hidden node using one of these importance functions.<ul> <li>Let  \\(a_i\\) be # instances with node output &gt; 0.5 and \\(d_i\\) be # instances with node output \u2264 0.5</li> <li>\\(Entropy(i)=\\dfrac{d_i}{a_i+d_i}log_2\\dfrac{d_i}{a_i+d_i}+\\dfrac{a_i}{a_i+d_i}log_2\\dfrac{a_i}{a_i+d_i}\\)</li> <li>The intuition is that if one node's outputs are almost identical on all training data, these outputs do not generate variations to later layers and consequently the node may not be useful.</li> <li>Output-weights Norm (onorm): average L1-norm of the weights of its outgoing links</li> <li>Input-weights norm (inorm): average L1-norm of the weights of its incoming links</li> </ul> </li> <li>All the nodes are sorted by their scores and nodes with less importance values are removed.</li> <li>On switchboard speech recognition data, onorm was found to be the best.</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/03%20-%20Block%20Prunning/","title":"03   Block Prunning","text":"<ul> <li>Problems with weight prunning: Irregularity of sparse ,atricies limits the maximum performance and energy efficency achevable on hardware accelerators.</li> <li>Block-sparse formats store blocks continiously in memory reducing irregular memory accesses.</li> <li>If the maximum magnitude weight of the block is below the current threshold, we set all the weights to zero.</li> <li>If the maximum magnitude weight of a block is below the current threshold we set all the weights in that block to zeroz</li> <li>For block prunning we need to modify the starting slope to account for the number of elements in a block \\((N_b)\\)<ul> <li>Start slope for weight prunning \\(\\Theta_w=\\Theta\\)</li> <li>Start slope for weight prunning \\(\\Theta_b=\\Theta_w \\times  \\sqrt[4]{N_b}\\)</li> </ul> </li> </ul> <p>Bank-Ballanced Sparsity (BBS) Split tata to banks and remove 50% of the weights within each bank</p> <p>![[Pasted image 20221208131112.png]]</p>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/03%20-%20Block%20Prunning/#References","title":"References","text":"<ul> <li>Efficient and Effective Sparse LSTM on FPGA with Bank-Balanced Sparsity, pdf</li> <li>BLOCK-SPARSE RECURRENT NEURAL NETWORKS, ICLR 2018, pdf</li> <li>Exploring Sparsity in Recurrent Neural Networks, ICLR 2017, pdf</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/04%20-%20Pruning%20heads%20and%20layers/","title":"04   Pruning heads and layers","text":"<p>Paul Michel, Omer Levy, Graham Neubig, Are Sixteen Heads Really Better than One?, 2019</p> <p></p> <p>Majority of attention heads can be removed without deviating too much from the original score. Surprisingly, in some cases removing an attention head results in an increase in BLEU/accuracy.</p> <ul> <li>Only 8 (out of 96) heads in 6-layer WMT NMT Transformer (16 heads / layer) cause a statistically significant change in performance when they are removed from the model, half of which actually result in a higher BLEU score.</li> </ul> <p></p> <ul> <li>For most layers, one head is indeed sufficient at test time, even though the network was trained with 12 (BERT) or 16 (WMT Transformer) attention heads.</li> </ul> <p></p> <ul> <li> <p>What if we pruned heads across two or more different layers at the same time?</p> <ul> <li>Sort all the attention heads, and prune.</li> <li>Prune up to 20% and 40% of heads from WMT and BERT resp., without incurring any noticeable negative impact.</li> </ul> </li> <li> <p>Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov, Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned 2019</p> </li> </ul>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/04%20-%20Pruning%20heads%20and%20layers/#Layer-prunning","title":"Layer prunning","text":"<p> LayerDrop (right) randomly drops layers at training time. At test time, this allows for sub-network selection to any desired depth as the network has been trained to be robust to pruning. In contrast to standard approaches that must re-train a new model from scratch for each model size (left), our method trains only one network from which multiple shallow models can be extracted.</p> <p></p> <p>Reducing Transformer Depth on Demand with Structured Dropout, ICLR 2020,  pdf, openreview</p>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/04%20-%20Pruning%20heads%20and%20layers/#Prunning-attention-heads-and-MLP-layers","title":"Prunning attention heads and MLP layers","text":"<p>For a finetuned BERT it is possible to find a subnetwork of elements, that achieves performance, comparable with the full model. 86% heads and 57% MLPs survive in less than 7 tasks, which rises concerns about the degree to which BERT relies on task-specific heuristics rather than general linguistic knowledge</p> <p> The \u201cgood\u201d subnetworks: self-attention heads and MLPs that survive pruning. Each cell gives the average number of GLUE tasks in which a given head/MLP survived, and the standard deviation across 5 finetuning initializations.</p> <p>When BERT Plays the Lottery, All Tickets Are Winning, Anna Rumshisky, 2020  pdf</p>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/Index/","title":"Pruning: sparsifying weight matrice","text":""},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/Index/#Pruning-Motivation","title":"Pruning: Motivation","text":"<p>Trillion of synapses are generated in the human brain during the first few months of birth. Christopher A. Walsh Peter Huttenlocher (1931\u20132013)</p> <p></p> <ul> <li>1 year old, peaked at 1000 trillion</li> <li>Pruning begins to occur.</li> <li>10 years old, a child has nearly 500 trillion synapses</li> <li>This 'pruning' mechanism removes redundant connections in the brain.</li> </ul>"},{"location":"Week%2013%20-Model%20Compression/01.%20Pruning/Index/#Pruning-strategies","title":"Pruning strategies","text":"<p> - 01 - Pruning weights - 02 - Pruning neurons - 03 - Block Prunning - 04 - Pruning heads and layers</p>"}]}